{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 起步：numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "介绍 PyTorch 之前，首先使用 numpy 实现网络。\n",
    "\n",
    "numpy 提供了n维数组，许多操作数组的函数。它是科学计算的通用框架，它对计算图、深度学习或梯度一无所知，但可手动实现网络的前向传播和反向传播，轻松使用两层网络拟合随机数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 , loss: 39019361.38700613\n",
      "iteration 1 , loss: 35719082.139826335\n",
      "iteration 2 , loss: 32509352.125539895\n",
      "iteration 3 , loss: 25420309.580064043\n",
      "iteration 4 , loss: 16606637.71162427\n",
      "iteration 5 , loss: 9387762.619381998\n",
      "iteration 6 , loss: 5149326.868747448\n",
      "iteration 7 , loss: 3007308.489897264\n",
      "iteration 8 , loss: 1967574.1297512027\n",
      "iteration 9 , loss: 1423981.9169151098\n",
      "iteration 10 , loss: 1104667.3536703452\n",
      "iteration 11 , loss: 892595.5990871357\n",
      "iteration 12 , loss: 738092.2728450412\n",
      "iteration 13 , loss: 618807.9198511755\n",
      "iteration 14 , loss: 523574.3970470655\n",
      "iteration 15 , loss: 446008.78058524156\n",
      "iteration 16 , loss: 382001.6517942947\n",
      "iteration 17 , loss: 328756.84876854956\n",
      "iteration 18 , loss: 284160.2632113299\n",
      "iteration 19 , loss: 246638.5445840901\n",
      "iteration 20 , loss: 214840.1214017758\n",
      "iteration 21 , loss: 187770.00418735202\n",
      "iteration 22 , loss: 164686.3088019176\n",
      "iteration 23 , loss: 144847.86762839375\n",
      "iteration 24 , loss: 127742.92794579946\n",
      "iteration 25 , loss: 112939.54151495782\n",
      "iteration 26 , loss: 100097.23470853514\n",
      "iteration 27 , loss: 88918.46807062213\n",
      "iteration 28 , loss: 79163.61447646422\n",
      "iteration 29 , loss: 70628.79501344514\n",
      "iteration 30 , loss: 63140.22145921184\n",
      "iteration 31 , loss: 56548.710464150055\n",
      "iteration 32 , loss: 50738.13992985703\n",
      "iteration 33 , loss: 45599.31832137153\n",
      "iteration 34 , loss: 41044.83930014767\n",
      "iteration 35 , loss: 37004.02049130117\n",
      "iteration 36 , loss: 33409.756895615705\n",
      "iteration 37 , loss: 30218.18727972295\n",
      "iteration 38 , loss: 27374.402813798748\n",
      "iteration 39 , loss: 24829.74731001116\n",
      "iteration 40 , loss: 22549.931816663575\n",
      "iteration 41 , loss: 20504.73310817113\n",
      "iteration 42 , loss: 18666.81482606422\n",
      "iteration 43 , loss: 17011.794056572362\n",
      "iteration 44 , loss: 15519.942396677206\n",
      "iteration 45 , loss: 14172.490904365903\n",
      "iteration 46 , loss: 12954.22097343914\n",
      "iteration 47 , loss: 11851.471372997852\n",
      "iteration 48 , loss: 10851.600674765656\n",
      "iteration 49 , loss: 9944.350863540005\n",
      "iteration 50 , loss: 9120.755289187873\n",
      "iteration 51 , loss: 8371.219499122351\n",
      "iteration 52 , loss: 7688.876633053142\n",
      "iteration 53 , loss: 7067.61579574122\n",
      "iteration 54 , loss: 6500.57149602071\n",
      "iteration 55 , loss: 5982.939281324724\n",
      "iteration 56 , loss: 5510.143468515638\n",
      "iteration 57 , loss: 5077.668810107856\n",
      "iteration 58 , loss: 4681.7828499480665\n",
      "iteration 59 , loss: 4319.237859183899\n",
      "iteration 60 , loss: 3987.042829411417\n",
      "iteration 61 , loss: 3682.2157625477344\n",
      "iteration 62 , loss: 3402.4363192617197\n",
      "iteration 63 , loss: 3145.643575184189\n",
      "iteration 64 , loss: 2909.868359685755\n",
      "iteration 65 , loss: 2692.9534366927005\n",
      "iteration 66 , loss: 2493.331206218995\n",
      "iteration 67 , loss: 2309.584751442214\n",
      "iteration 68 , loss: 2140.244204468815\n",
      "iteration 69 , loss: 1984.1388385606194\n",
      "iteration 70 , loss: 1840.1499647830829\n",
      "iteration 71 , loss: 1707.3049180067096\n",
      "iteration 72 , loss: 1584.5759773044256\n",
      "iteration 73 , loss: 1471.2114732070047\n",
      "iteration 74 , loss: 1366.4584370718196\n",
      "iteration 75 , loss: 1269.6136998699471\n",
      "iteration 76 , loss: 1179.9456563492367\n",
      "iteration 77 , loss: 1097.003964222568\n",
      "iteration 78 , loss: 1020.2010693787626\n",
      "iteration 79 , loss: 949.0566984078446\n",
      "iteration 80 , loss: 883.1648412588765\n",
      "iteration 81 , loss: 822.0942936344625\n",
      "iteration 82 , loss: 765.4399824148456\n",
      "iteration 83 , loss: 712.9007664106125\n",
      "iteration 84 , loss: 664.130290855413\n",
      "iteration 85 , loss: 618.8617573405942\n",
      "iteration 86 , loss: 576.8321344619437\n",
      "iteration 87 , loss: 537.7985433064464\n",
      "iteration 88 , loss: 501.5275065614207\n",
      "iteration 89 , loss: 467.8156559979599\n",
      "iteration 90 , loss: 436.4773091359688\n",
      "iteration 91 , loss: 407.336780256264\n",
      "iteration 92 , loss: 380.22934895258857\n",
      "iteration 93 , loss: 355.01637847904215\n",
      "iteration 94 , loss: 331.53637738504756\n",
      "iteration 95 , loss: 309.67258519330494\n",
      "iteration 96 , loss: 289.31200995049164\n",
      "iteration 97 , loss: 270.34933219523805\n",
      "iteration 98 , loss: 252.68276885837466\n",
      "iteration 99 , loss: 236.22123682400843\n",
      "iteration 100 , loss: 220.87842692351245\n",
      "iteration 101 , loss: 206.56828505776593\n",
      "iteration 102 , loss: 193.22394900009664\n",
      "iteration 103 , loss: 180.77613404878326\n",
      "iteration 104 , loss: 169.16528644072838\n",
      "iteration 105 , loss: 158.32650868814318\n",
      "iteration 106 , loss: 148.21064797071733\n",
      "iteration 107 , loss: 138.76487985739266\n",
      "iteration 108 , loss: 129.94670687274674\n",
      "iteration 109 , loss: 121.71130011664678\n",
      "iteration 110 , loss: 114.01615228336594\n",
      "iteration 111 , loss: 106.82432847756988\n",
      "iteration 112 , loss: 100.10313010264551\n",
      "iteration 113 , loss: 93.82099931475543\n",
      "iteration 114 , loss: 87.9473514593806\n",
      "iteration 115 , loss: 82.45639420608866\n",
      "iteration 116 , loss: 77.32006970733993\n",
      "iteration 117 , loss: 72.51509852986428\n",
      "iteration 118 , loss: 68.02045384061005\n",
      "iteration 119 , loss: 63.81355057057077\n",
      "iteration 120 , loss: 59.874841352923724\n",
      "iteration 121 , loss: 56.188222380524344\n",
      "iteration 122 , loss: 52.738049703904764\n",
      "iteration 123 , loss: 49.505870202657505\n",
      "iteration 124 , loss: 46.47819593012189\n",
      "iteration 125 , loss: 43.64239064525569\n",
      "iteration 126 , loss: 40.98555757924922\n",
      "iteration 127 , loss: 38.4963075464002\n",
      "iteration 128 , loss: 36.16356387722884\n",
      "iteration 129 , loss: 33.9763911710683\n",
      "iteration 130 , loss: 31.92593426550024\n",
      "iteration 131 , loss: 30.00298330051581\n",
      "iteration 132 , loss: 28.199801087835034\n",
      "iteration 133 , loss: 26.508241819747987\n",
      "iteration 134 , loss: 24.9217942380158\n",
      "iteration 135 , loss: 23.436138497862157\n",
      "iteration 136 , loss: 22.042155594978023\n",
      "iteration 137 , loss: 20.734298395996223\n",
      "iteration 138 , loss: 19.506197534079547\n",
      "iteration 139 , loss: 18.352748323710347\n",
      "iteration 140 , loss: 17.2700078522361\n",
      "iteration 141 , loss: 16.253088127005444\n",
      "iteration 142 , loss: 15.297747325625068\n",
      "iteration 143 , loss: 14.400390905415161\n",
      "iteration 144 , loss: 13.557371904601906\n",
      "iteration 145 , loss: 12.765072276913855\n",
      "iteration 146 , loss: 12.020605198945773\n",
      "iteration 147 , loss: 11.320721119577213\n",
      "iteration 148 , loss: 10.663122506087994\n",
      "iteration 149 , loss: 10.044628539530175\n",
      "iteration 150 , loss: 9.463013598510353\n",
      "iteration 151 , loss: 8.916125821912285\n",
      "iteration 152 , loss: 8.40176642394038\n",
      "iteration 153 , loss: 7.917962115884277\n",
      "iteration 154 , loss: 7.4628255493828775\n",
      "iteration 155 , loss: 7.034548872041514\n",
      "iteration 156 , loss: 6.631552293729744\n",
      "iteration 157 , loss: 6.25230427992921\n",
      "iteration 158 , loss: 5.895324529585136\n",
      "iteration 159 , loss: 5.559471064192451\n",
      "iteration 160 , loss: 5.243167213121057\n",
      "iteration 161 , loss: 4.9453990731203845\n",
      "iteration 162 , loss: 4.6649143177390275\n",
      "iteration 163 , loss: 4.4008511864844255\n",
      "iteration 164 , loss: 4.1520964622921746\n",
      "iteration 165 , loss: 3.9177676696536787\n",
      "iteration 166 , loss: 3.6970445860702825\n",
      "iteration 167 , loss: 3.4890883073237235\n",
      "iteration 168 , loss: 3.2931393055825966\n",
      "iteration 169 , loss: 3.108479680036517\n",
      "iteration 170 , loss: 2.934492707070512\n",
      "iteration 171 , loss: 2.770549484380746\n",
      "iteration 172 , loss: 2.6159171492092597\n",
      "iteration 173 , loss: 2.4701390887534043\n",
      "iteration 174 , loss: 2.3326957737913125\n",
      "iteration 175 , loss: 2.2030950743316575\n",
      "iteration 176 , loss: 2.080884538072115\n",
      "iteration 177 , loss: 1.965617807940167\n",
      "iteration 178 , loss: 1.8569016869423072\n",
      "iteration 179 , loss: 1.7543397864252537\n",
      "iteration 180 , loss: 1.6576160935835906\n",
      "iteration 181 , loss: 1.566329616720091\n",
      "iteration 182 , loss: 1.480197438867465\n",
      "iteration 183 , loss: 1.3989445572858383\n",
      "iteration 184 , loss: 1.3222547730752048\n",
      "iteration 185 , loss: 1.2498512471025465\n",
      "iteration 186 , loss: 1.1815114281407797\n",
      "iteration 187 , loss: 1.1169952231360794\n",
      "iteration 188 , loss: 1.056088954196829\n",
      "iteration 189 , loss: 0.9985831247108861\n",
      "iteration 190 , loss: 0.9442812140789876\n",
      "iteration 191 , loss: 0.893007343124802\n",
      "iteration 192 , loss: 0.8445801384476215\n",
      "iteration 193 , loss: 0.7988368924419877\n",
      "iteration 194 , loss: 0.7556334963785226\n",
      "iteration 195 , loss: 0.7148163984097556\n",
      "iteration 196 , loss: 0.6762609405339861\n",
      "iteration 197 , loss: 0.6398397868064452\n",
      "iteration 198 , loss: 0.6054137093913223\n",
      "iteration 199 , loss: 0.5728820187654159\n",
      "iteration 200 , loss: 0.5421367170894715\n",
      "iteration 201 , loss: 0.5130792059745244\n",
      "iteration 202 , loss: 0.48561455159404293\n",
      "iteration 203 , loss: 0.4596510213529935\n",
      "iteration 204 , loss: 0.4351052373660131\n",
      "iteration 205 , loss: 0.41189903132273076\n",
      "iteration 206 , loss: 0.38995808152774003\n",
      "iteration 207 , loss: 0.36921155333835914\n",
      "iteration 208 , loss: 0.3495920450449741\n",
      "iteration 209 , loss: 0.3310377051832869\n",
      "iteration 210 , loss: 0.3134903328596936\n",
      "iteration 211 , loss: 0.2968988084981832\n",
      "iteration 212 , loss: 0.2811973670088739\n",
      "iteration 213 , loss: 0.2663431174264106\n",
      "iteration 214 , loss: 0.25229159923364075\n",
      "iteration 215 , loss: 0.23899618399010034\n",
      "iteration 216 , loss: 0.22641532943595344\n",
      "iteration 217 , loss: 0.21450970756155202\n",
      "iteration 218 , loss: 0.20324347139363422\n",
      "iteration 219 , loss: 0.19258099027600697\n",
      "iteration 220 , loss: 0.1824894126083949\n",
      "iteration 221 , loss: 0.17293701057426092\n",
      "iteration 222 , loss: 0.1638954018470541\n",
      "iteration 223 , loss: 0.15533555879940925\n",
      "iteration 224 , loss: 0.1472314007908484\n",
      "iteration 225 , loss: 0.13955902551910887\n",
      "iteration 226 , loss: 0.13229719346532443\n",
      "iteration 227 , loss: 0.12541730667564982\n",
      "iteration 228 , loss: 0.11890343895087456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 229 , loss: 0.11273381354723264\n",
      "iteration 230 , loss: 0.10688977878647675\n",
      "iteration 231 , loss: 0.10135500967126455\n",
      "iteration 232 , loss: 0.09611211989963034\n",
      "iteration 233 , loss: 0.09114553818302557\n",
      "iteration 234 , loss: 0.0864406087379351\n",
      "iteration 235 , loss: 0.08198331994283264\n",
      "iteration 236 , loss: 0.07776001519745812\n",
      "iteration 237 , loss: 0.07375798508108763\n",
      "iteration 238 , loss: 0.06996604906194312\n",
      "iteration 239 , loss: 0.06637264694039033\n",
      "iteration 240 , loss: 0.06296701063977775\n",
      "iteration 241 , loss: 0.05974011369023456\n",
      "iteration 242 , loss: 0.05668193799372451\n",
      "iteration 243 , loss: 0.05378212789565301\n",
      "iteration 244 , loss: 0.051033747469932916\n",
      "iteration 245 , loss: 0.04842810248894612\n",
      "iteration 246 , loss: 0.04595770987643143\n",
      "iteration 247 , loss: 0.04361559420791622\n",
      "iteration 248 , loss: 0.041395044060547365\n",
      "iteration 249 , loss: 0.039289669743393804\n",
      "iteration 250 , loss: 0.037293242026212356\n",
      "iteration 251 , loss: 0.035399941226231794\n",
      "iteration 252 , loss: 0.03360442983906953\n",
      "iteration 253 , loss: 0.0319015080944197\n",
      "iteration 254 , loss: 0.030286414627809382\n",
      "iteration 255 , loss: 0.028754446359611552\n",
      "iteration 256 , loss: 0.02730125051606446\n",
      "iteration 257 , loss: 0.025922863144938253\n",
      "iteration 258 , loss: 0.024615875065634313\n",
      "iteration 259 , loss: 0.02337523318995246\n",
      "iteration 260 , loss: 0.02219814363578421\n",
      "iteration 261 , loss: 0.021081329104467555\n",
      "iteration 262 , loss: 0.020021777046750094\n",
      "iteration 263 , loss: 0.019016218084402577\n",
      "iteration 264 , loss: 0.01806205070563957\n",
      "iteration 265 , loss: 0.017156530396189418\n",
      "iteration 266 , loss: 0.016297099309341766\n",
      "iteration 267 , loss: 0.015481474433220873\n",
      "iteration 268 , loss: 0.014707278255534535\n",
      "iteration 269 , loss: 0.013972418526016972\n",
      "iteration 270 , loss: 0.013274888027267447\n",
      "iteration 271 , loss: 0.012612736662509633\n",
      "iteration 272 , loss: 0.011984149315477997\n",
      "iteration 273 , loss: 0.011387395770038604\n",
      "iteration 274 , loss: 0.010820830305165636\n",
      "iteration 275 , loss: 0.01028311712875294\n",
      "iteration 276 , loss: 0.00977231287199334\n",
      "iteration 277 , loss: 0.009287251631913747\n",
      "iteration 278 , loss: 0.008826629735081136\n",
      "iteration 279 , loss: 0.008389260616764017\n",
      "iteration 280 , loss: 0.007973884825850226\n",
      "iteration 281 , loss: 0.0075793523164396125\n",
      "iteration 282 , loss: 0.007204662786767625\n",
      "iteration 283 , loss: 0.006848779869103103\n",
      "iteration 284 , loss: 0.006510742425782284\n",
      "iteration 285 , loss: 0.00618961256612581\n",
      "iteration 286 , loss: 0.005884560160077072\n",
      "iteration 287 , loss: 0.005594768373864566\n",
      "iteration 288 , loss: 0.005319462005418845\n",
      "iteration 289 , loss: 0.005057897153899116\n",
      "iteration 290 , loss: 0.004809383619799011\n",
      "iteration 291 , loss: 0.004573266925477987\n",
      "iteration 292 , loss: 0.004349000994890895\n",
      "iteration 293 , loss: 0.004135822459330323\n",
      "iteration 294 , loss: 0.0039332233796526795\n",
      "iteration 295 , loss: 0.0037406976828325664\n",
      "iteration 296 , loss: 0.0035577190591786385\n",
      "iteration 297 , loss: 0.0033838260465626456\n",
      "iteration 298 , loss: 0.003218544929418145\n",
      "iteration 299 , loss: 0.003061454135538653\n",
      "iteration 300 , loss: 0.0029121413698522335\n",
      "iteration 301 , loss: 0.0027702024247861705\n",
      "iteration 302 , loss: 0.0026352834909346527\n",
      "iteration 303 , loss: 0.0025070236534840288\n",
      "iteration 304 , loss: 0.002385091369417786\n",
      "iteration 305 , loss: 0.0022691743338970043\n",
      "iteration 306 , loss: 0.002158965318509368\n",
      "iteration 307 , loss: 0.0020541773140075616\n",
      "iteration 308 , loss: 0.0019545512263029353\n",
      "iteration 309 , loss: 0.001859862245758631\n",
      "iteration 310 , loss: 0.0017697982084228332\n",
      "iteration 311 , loss: 0.0016841420314385527\n",
      "iteration 312 , loss: 0.0016026822958451624\n",
      "iteration 313 , loss: 0.0015252142308140944\n",
      "iteration 314 , loss: 0.0014515387854441616\n",
      "iteration 315 , loss: 0.0013814672938122442\n",
      "iteration 316 , loss: 0.00131482576357212\n",
      "iteration 317 , loss: 0.0012514383786063807\n",
      "iteration 318 , loss: 0.0011911455993466585\n",
      "iteration 319 , loss: 0.0011337969898195452\n",
      "iteration 320 , loss: 0.0010792435660062479\n",
      "iteration 321 , loss: 0.0010273484440838422\n",
      "iteration 322 , loss: 0.000977981108034363\n",
      "iteration 323 , loss: 0.0009310136086439322\n",
      "iteration 324 , loss: 0.0008863327839848062\n",
      "iteration 325 , loss: 0.0008438208470876949\n",
      "iteration 326 , loss: 0.0008033854606162901\n",
      "iteration 327 , loss: 0.000764913168008297\n",
      "iteration 328 , loss: 0.0007282966307237402\n",
      "iteration 329 , loss: 0.0006934520036927982\n",
      "iteration 330 , loss: 0.000660294962206739\n",
      "iteration 331 , loss: 0.0006287416883258668\n",
      "iteration 332 , loss: 0.0005987158215883372\n",
      "iteration 333 , loss: 0.0005701400754678229\n",
      "iteration 334 , loss: 0.000542944768846266\n",
      "iteration 335 , loss: 0.0005170625419730948\n",
      "iteration 336 , loss: 0.0004924276293489218\n",
      "iteration 337 , loss: 0.0004689817297762723\n",
      "iteration 338 , loss: 0.00044666382796084846\n",
      "iteration 339 , loss: 0.0004254212734759337\n",
      "iteration 340 , loss: 0.00040520022834592115\n",
      "iteration 341 , loss: 0.0003859529895994369\n",
      "iteration 342 , loss: 0.00036762987553309233\n",
      "iteration 343 , loss: 0.00035018950096127874\n",
      "iteration 344 , loss: 0.00033358896452914005\n",
      "iteration 345 , loss: 0.0003177786781295418\n",
      "iteration 346 , loss: 0.00030272625020691716\n",
      "iteration 347 , loss: 0.0002883942963218521\n",
      "iteration 348 , loss: 0.0002747482138722851\n",
      "iteration 349 , loss: 0.00026175515867437395\n",
      "iteration 350 , loss: 0.0002493831845060017\n",
      "iteration 351 , loss: 0.0002376025220080696\n",
      "iteration 352 , loss: 0.00022638453316125881\n",
      "iteration 353 , loss: 0.00021570183528325793\n",
      "iteration 354 , loss: 0.00020552874914688205\n",
      "iteration 355 , loss: 0.000195840464814944\n",
      "iteration 356 , loss: 0.00018661385951860307\n",
      "iteration 357 , loss: 0.00017782648309628185\n",
      "iteration 358 , loss: 0.0001694573293927964\n",
      "iteration 359 , loss: 0.00016148708189042384\n",
      "iteration 360 , loss: 0.00015389689085879823\n",
      "iteration 361 , loss: 0.00014666728199256944\n",
      "iteration 362 , loss: 0.00013977864550527257\n",
      "iteration 363 , loss: 0.00013321715244108545\n",
      "iteration 364 , loss: 0.0001269670531174549\n",
      "iteration 365 , loss: 0.00012101276805897204\n",
      "iteration 366 , loss: 0.00011534081114843796\n",
      "iteration 367 , loss: 0.00010993702276181658\n",
      "iteration 368 , loss: 0.00010478929058799541\n",
      "iteration 369 , loss: 9.988475684852342e-05\n",
      "iteration 370 , loss: 9.52121025769064e-05\n",
      "iteration 371 , loss: 9.076029965178722e-05\n",
      "iteration 372 , loss: 8.651849795165634e-05\n",
      "iteration 373 , loss: 8.24770772063497e-05\n",
      "iteration 374 , loss: 7.862627997858355e-05\n",
      "iteration 375 , loss: 7.495700480776216e-05\n",
      "iteration 376 , loss: 7.146064868416075e-05\n",
      "iteration 377 , loss: 6.813056737995782e-05\n",
      "iteration 378 , loss: 6.495541314776069e-05\n",
      "iteration 379 , loss: 6.192973959238233e-05\n",
      "iteration 380 , loss: 5.9046232084626626e-05\n",
      "iteration 381 , loss: 5.6298260289730676e-05\n",
      "iteration 382 , loss: 5.3679562931790645e-05\n",
      "iteration 383 , loss: 5.1183739180265744e-05\n",
      "iteration 384 , loss: 4.880500950613116e-05\n",
      "iteration 385 , loss: 4.65376874355363e-05\n",
      "iteration 386 , loss: 4.437671883852593e-05\n",
      "iteration 387 , loss: 4.231697541257878e-05\n",
      "iteration 388 , loss: 4.035371133957767e-05\n",
      "iteration 389 , loss: 3.848237752469235e-05\n",
      "iteration 390 , loss: 3.669860682905244e-05\n",
      "iteration 391 , loss: 3.499814674025743e-05\n",
      "iteration 392 , loss: 3.3377211642565157e-05\n",
      "iteration 393 , loss: 3.183276883485464e-05\n",
      "iteration 394 , loss: 3.0359759506733457e-05\n",
      "iteration 395 , loss: 2.8955411033583556e-05\n",
      "iteration 396 , loss: 2.761660411771001e-05\n",
      "iteration 397 , loss: 2.634019472299824e-05\n",
      "iteration 398 , loss: 2.5123277310530187e-05\n",
      "iteration 399 , loss: 2.3963093690360104e-05\n",
      "iteration 400 , loss: 2.2856886161988613e-05\n",
      "iteration 401 , loss: 2.180219772775697e-05\n",
      "iteration 402 , loss: 2.0796572760839993e-05\n",
      "iteration 403 , loss: 1.9837794616623643e-05\n",
      "iteration 404 , loss: 1.892349604269292e-05\n",
      "iteration 405 , loss: 1.8051708643618155e-05\n",
      "iteration 406 , loss: 1.7220353446391825e-05\n",
      "iteration 407 , loss: 1.642762278989371e-05\n",
      "iteration 408 , loss: 1.567190212660543e-05\n",
      "iteration 409 , loss: 1.4951182312882447e-05\n",
      "iteration 410 , loss: 1.4263670391926178e-05\n",
      "iteration 411 , loss: 1.3608071496847965e-05\n",
      "iteration 412 , loss: 1.2982790406823531e-05\n",
      "iteration 413 , loss: 1.2386486373046492e-05\n",
      "iteration 414 , loss: 1.1817781921485565e-05\n",
      "iteration 415 , loss: 1.1275389518435355e-05\n",
      "iteration 416 , loss: 1.0758056628196445e-05\n",
      "iteration 417 , loss: 1.0264697870758605e-05\n",
      "iteration 418 , loss: 9.7940842035852e-06\n",
      "iteration 419 , loss: 9.345217027913107e-06\n",
      "iteration 420 , loss: 8.917066654852144e-06\n",
      "iteration 421 , loss: 8.5086903918051e-06\n",
      "iteration 422 , loss: 8.119132885299013e-06\n",
      "iteration 423 , loss: 7.747675289718633e-06\n",
      "iteration 424 , loss: 7.3933142027794e-06\n",
      "iteration 425 , loss: 7.055182671044023e-06\n",
      "iteration 426 , loss: 6.732618730783525e-06\n",
      "iteration 427 , loss: 6.424921984729624e-06\n",
      "iteration 428 , loss: 6.131374439169777e-06\n",
      "iteration 429 , loss: 5.851333149719412e-06\n",
      "iteration 430 , loss: 5.584180923516721e-06\n",
      "iteration 431 , loss: 5.329313448078332e-06\n",
      "iteration 432 , loss: 5.08614631122311e-06\n",
      "iteration 433 , loss: 4.85414931917522e-06\n",
      "iteration 434 , loss: 4.632817476839862e-06\n",
      "iteration 435 , loss: 4.421631160363417e-06\n",
      "iteration 436 , loss: 4.220151829741908e-06\n",
      "iteration 437 , loss: 4.027927336512066e-06\n",
      "iteration 438 , loss: 3.844582166458454e-06\n",
      "iteration 439 , loss: 3.6695482289328812e-06\n",
      "iteration 440 , loss: 3.5025403282740292e-06\n",
      "iteration 441 , loss: 3.343183999409187e-06\n",
      "iteration 442 , loss: 3.1911216837982504e-06\n",
      "iteration 443 , loss: 3.0460187245908486e-06\n",
      "iteration 444 , loss: 2.907564780135459e-06\n",
      "iteration 445 , loss: 2.7754430270195424e-06\n",
      "iteration 446 , loss: 2.649356215996854e-06\n",
      "iteration 447 , loss: 2.5290336804951154e-06\n",
      "iteration 448 , loss: 2.4142101442846955e-06\n",
      "iteration 449 , loss: 2.3046311382990907e-06\n",
      "iteration 450 , loss: 2.2000534559410775e-06\n",
      "iteration 451 , loss: 2.100256626112391e-06\n",
      "iteration 452 , loss: 2.005059880633119e-06\n",
      "iteration 453 , loss: 1.914155016430754e-06\n",
      "iteration 454 , loss: 1.827396477968664e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 455 , loss: 1.7445935378968276e-06\n",
      "iteration 456 , loss: 1.6655665130788384e-06\n",
      "iteration 457 , loss: 1.5901395882884578e-06\n",
      "iteration 458 , loss: 1.5181475796990957e-06\n",
      "iteration 459 , loss: 1.4494346177532335e-06\n",
      "iteration 460 , loss: 1.3838473597522563e-06\n",
      "iteration 461 , loss: 1.3212450847941512e-06\n",
      "iteration 462 , loss: 1.2614895774650485e-06\n",
      "iteration 463 , loss: 1.2044554949853435e-06\n",
      "iteration 464 , loss: 1.1500106105274685e-06\n",
      "iteration 465 , loss: 1.0980530434214043e-06\n",
      "iteration 466 , loss: 1.0484605002834046e-06\n",
      "iteration 467 , loss: 1.0011062040930382e-06\n",
      "iteration 468 , loss: 9.559031193539084e-07\n",
      "iteration 469 , loss: 9.127490726000991e-07\n",
      "iteration 470 , loss: 8.715570044647074e-07\n",
      "iteration 471 , loss: 8.322321991994698e-07\n",
      "iteration 472 , loss: 7.94690115659629e-07\n",
      "iteration 473 , loss: 7.588505356090387e-07\n",
      "iteration 474 , loss: 7.246352641455797e-07\n",
      "iteration 475 , loss: 6.919722158897731e-07\n",
      "iteration 476 , loss: 6.607873267072988e-07\n",
      "iteration 477 , loss: 6.310152912059791e-07\n",
      "iteration 478 , loss: 6.025969038951085e-07\n",
      "iteration 479 , loss: 5.754698791820832e-07\n",
      "iteration 480 , loss: 5.495605370502025e-07\n",
      "iteration 481 , loss: 5.248229100099213e-07\n",
      "iteration 482 , loss: 5.012048706514762e-07\n",
      "iteration 483 , loss: 4.786565013501861e-07\n",
      "iteration 484 , loss: 4.571253012223829e-07\n",
      "iteration 485 , loss: 4.3656777729120186e-07\n",
      "iteration 486 , loss: 4.169388494287546e-07\n",
      "iteration 487 , loss: 3.981973237491601e-07\n",
      "iteration 488 , loss: 3.8030189885313814e-07\n",
      "iteration 489 , loss: 3.632140366164805e-07\n",
      "iteration 490 , loss: 3.4689796192175674e-07\n",
      "iteration 491 , loss: 3.313247447246686e-07\n",
      "iteration 492 , loss: 3.1645010160305464e-07\n",
      "iteration 493 , loss: 3.0224392644770576e-07\n",
      "iteration 494 , loss: 2.886780177956734e-07\n",
      "iteration 495 , loss: 2.757245073797374e-07\n",
      "iteration 496 , loss: 2.6335488384891915e-07\n",
      "iteration 497 , loss: 2.515418948656852e-07\n",
      "iteration 498 , loss: 2.402613036553002e-07\n",
      "iteration 499 , loss: 2.294887715362963e-07\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N是数据样本数，D_in是输入层维度\n",
    "# H是隐藏层维度，D_out是输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 构建随机输入和输出数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6  # 学习率\n",
    "iteration_count = 500  # 迭代次数\n",
    "for t in range(iteration_count):\n",
    "    # 前向传播：计算预测值 y\n",
    "    h = x.dot(w1)  # dot矩阵乘法\n",
    "    h_relu = np.maximum(h, 0)  # h = ReLU(h)\n",
    "    y_pred = h_relu.dot(w2) # 注意，输出层没有应用 ReLU\n",
    "\n",
    "    # 计算并输出损失\n",
    "    loss = np.square(y_pred - y).sum()  # 平方损失\n",
    "    print(\"iteration\", t, \",\", \"loss:\", loss)  # t是循环变量\n",
    "\n",
    "    # 反向传播，计算w1和w2关于损失的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch: Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy 是一个很棒的框架，但它不能利用 GPU 来加速其数值计算。对于现代深度神经网络，GPU通常提供50倍或更高的加速，所以不幸的是，numpy 对于现代深度学习来说还不够。\n",
    "\n",
    "在这里，我们介绍最基本的 PyTorch 概念：Tensor。 PyTorch Tensor 在概念上与 numpy 数组相同：Tensor 是一个n维数组，PyTorch 提供了许多用于在这些 Tensor 上运算的函数。Tensor 可以跟踪计算图和梯度，但它们也可用作科学计算的通用工具。\n",
    "\n",
    "与 numpy 不同，PyTorch Tensor 可以利用 GPU 加速其数值计算。要在 GPU 上运行 PyTorch Tensor，只需将其转换为新的数据类型即可。\n",
    "\n",
    "在这里，我们使用 PyTorch Tensor 将双层网络与随机数据相匹配。像上面的 numpy 示例一样，我们需要手动实现通过网络的前向和后向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 , loss: 30581504.0\n",
      "iteration 1 , loss: 27329054.0\n",
      "iteration 2 , loss: 25768940.0\n",
      "iteration 3 , loss: 22976022.0\n",
      "iteration 4 , loss: 18152610.0\n",
      "iteration 5 , loss: 12641956.0\n",
      "iteration 6 , loss: 7968243.5\n",
      "iteration 7 , loss: 4815052.5\n",
      "iteration 8 , loss: 2941822.0\n",
      "iteration 9 , loss: 1893403.75\n",
      "iteration 10 , loss: 1305025.75\n",
      "iteration 11 , loss: 961672.8125\n",
      "iteration 12 , loss: 747942.9375\n",
      "iteration 13 , loss: 604822.4375\n",
      "iteration 14 , loss: 502382.03125\n",
      "iteration 15 , loss: 424860.53125\n",
      "iteration 16 , loss: 363749.1875\n",
      "iteration 17 , loss: 314159.625\n",
      "iteration 18 , loss: 273175.90625\n",
      "iteration 19 , loss: 238828.546875\n",
      "iteration 20 , loss: 209776.625\n",
      "iteration 21 , loss: 185002.421875\n",
      "iteration 22 , loss: 163748.921875\n",
      "iteration 23 , loss: 145438.828125\n",
      "iteration 24 , loss: 129555.3125\n",
      "iteration 25 , loss: 115736.7109375\n",
      "iteration 26 , loss: 103673.5546875\n",
      "iteration 27 , loss: 93098.7109375\n",
      "iteration 28 , loss: 83802.4375\n",
      "iteration 29 , loss: 75611.21875\n",
      "iteration 30 , loss: 68379.5703125\n",
      "iteration 31 , loss: 61965.27734375\n",
      "iteration 32 , loss: 56258.21875\n",
      "iteration 33 , loss: 51174.671875\n",
      "iteration 34 , loss: 46647.51171875\n",
      "iteration 35 , loss: 42589.29296875\n",
      "iteration 36 , loss: 38945.05859375\n",
      "iteration 37 , loss: 35665.98046875\n",
      "iteration 38 , loss: 32717.82421875\n",
      "iteration 39 , loss: 30059.345703125\n",
      "iteration 40 , loss: 27653.490234375\n",
      "iteration 41 , loss: 25473.6484375\n",
      "iteration 42 , loss: 23495.35546875\n",
      "iteration 43 , loss: 21696.037109375\n",
      "iteration 44 , loss: 20055.658203125\n",
      "iteration 45 , loss: 18559.349609375\n",
      "iteration 46 , loss: 17192.697265625\n",
      "iteration 47 , loss: 15942.90625\n",
      "iteration 48 , loss: 14798.5078125\n",
      "iteration 49 , loss: 13749.5029296875\n",
      "iteration 50 , loss: 12787.75\n",
      "iteration 51 , loss: 11903.69921875\n",
      "iteration 52 , loss: 11089.576171875\n",
      "iteration 53 , loss: 10340.271484375\n",
      "iteration 54 , loss: 9648.564453125\n",
      "iteration 55 , loss: 9010.4755859375\n",
      "iteration 56 , loss: 8420.78125\n",
      "iteration 57 , loss: 7875.15380859375\n",
      "iteration 58 , loss: 7369.74609375\n",
      "iteration 59 , loss: 6901.21484375\n",
      "iteration 60 , loss: 6466.4755859375\n",
      "iteration 61 , loss: 6062.81298828125\n",
      "iteration 62 , loss: 5687.591796875\n",
      "iteration 63 , loss: 5338.69921875\n",
      "iteration 64 , loss: 5014.00390625\n",
      "iteration 65 , loss: 4711.4423828125\n",
      "iteration 66 , loss: 4429.43701171875\n",
      "iteration 67 , loss: 4166.423828125\n",
      "iteration 68 , loss: 3920.877197265625\n",
      "iteration 69 , loss: 3691.586181640625\n",
      "iteration 70 , loss: 3477.518310546875\n",
      "iteration 71 , loss: 3277.34326171875\n",
      "iteration 72 , loss: 3090.047607421875\n",
      "iteration 73 , loss: 2914.6669921875\n",
      "iteration 74 , loss: 2750.356689453125\n",
      "iteration 75 , loss: 2596.375\n",
      "iteration 76 , loss: 2451.9931640625\n",
      "iteration 77 , loss: 2316.52783203125\n",
      "iteration 78 , loss: 2189.3466796875\n",
      "iteration 79 , loss: 2069.8798828125\n",
      "iteration 80 , loss: 1957.636962890625\n",
      "iteration 81 , loss: 1852.09130859375\n",
      "iteration 82 , loss: 1752.80419921875\n",
      "iteration 83 , loss: 1659.4017333984375\n",
      "iteration 84 , loss: 1571.443359375\n",
      "iteration 85 , loss: 1488.6146240234375\n",
      "iteration 86 , loss: 1410.5706787109375\n",
      "iteration 87 , loss: 1337.0013427734375\n",
      "iteration 88 , loss: 1267.6287841796875\n",
      "iteration 89 , loss: 1202.199462890625\n",
      "iteration 90 , loss: 1140.44580078125\n",
      "iteration 91 , loss: 1082.1461181640625\n",
      "iteration 92 , loss: 1027.1048583984375\n",
      "iteration 93 , loss: 975.1123046875\n",
      "iteration 94 , loss: 925.9846801757812\n",
      "iteration 95 , loss: 879.5408935546875\n",
      "iteration 96 , loss: 835.6304321289062\n",
      "iteration 97 , loss: 794.0872192382812\n",
      "iteration 98 , loss: 754.7835083007812\n",
      "iteration 99 , loss: 717.5816040039062\n",
      "iteration 100 , loss: 682.3577880859375\n",
      "iteration 101 , loss: 648.995361328125\n",
      "iteration 102 , loss: 617.398193359375\n",
      "iteration 103 , loss: 587.4525146484375\n",
      "iteration 104 , loss: 559.0753784179688\n",
      "iteration 105 , loss: 532.1732788085938\n",
      "iteration 106 , loss: 506.6599426269531\n",
      "iteration 107 , loss: 482.4594421386719\n",
      "iteration 108 , loss: 459.4938049316406\n",
      "iteration 109 , loss: 437.7023620605469\n",
      "iteration 110 , loss: 417.0119323730469\n",
      "iteration 111 , loss: 397.3709411621094\n",
      "iteration 112 , loss: 378.71673583984375\n",
      "iteration 113 , loss: 360.9961853027344\n",
      "iteration 114 , loss: 344.1598205566406\n",
      "iteration 115 , loss: 328.15985107421875\n",
      "iteration 116 , loss: 312.9510803222656\n",
      "iteration 117 , loss: 298.4922790527344\n",
      "iteration 118 , loss: 284.7413330078125\n",
      "iteration 119 , loss: 271.66436767578125\n",
      "iteration 120 , loss: 259.2253112792969\n",
      "iteration 121 , loss: 247.38973999023438\n",
      "iteration 122 , loss: 236.12437438964844\n",
      "iteration 123 , loss: 225.41026306152344\n",
      "iteration 124 , loss: 215.20913696289062\n",
      "iteration 125 , loss: 205.49356079101562\n",
      "iteration 126 , loss: 196.24099731445312\n",
      "iteration 127 , loss: 187.42874145507812\n",
      "iteration 128 , loss: 179.03305053710938\n",
      "iteration 129 , loss: 171.0346221923828\n",
      "iteration 130 , loss: 163.4107666015625\n",
      "iteration 131 , loss: 156.1444091796875\n",
      "iteration 132 , loss: 149.2274932861328\n",
      "iteration 133 , loss: 142.63478088378906\n",
      "iteration 134 , loss: 136.34591674804688\n",
      "iteration 135 , loss: 130.34840393066406\n",
      "iteration 136 , loss: 124.628173828125\n",
      "iteration 137 , loss: 119.17163848876953\n",
      "iteration 138 , loss: 113.96428680419922\n",
      "iteration 139 , loss: 108.99505615234375\n",
      "iteration 140 , loss: 104.25302124023438\n",
      "iteration 141 , loss: 99.7258529663086\n",
      "iteration 142 , loss: 95.40406036376953\n",
      "iteration 143 , loss: 91.27816772460938\n",
      "iteration 144 , loss: 87.3377914428711\n",
      "iteration 145 , loss: 83.57539367675781\n",
      "iteration 146 , loss: 79.98255157470703\n",
      "iteration 147 , loss: 76.54823303222656\n",
      "iteration 148 , loss: 73.26966857910156\n",
      "iteration 149 , loss: 70.13597106933594\n",
      "iteration 150 , loss: 67.14118194580078\n",
      "iteration 151 , loss: 64.27959442138672\n",
      "iteration 152 , loss: 61.545108795166016\n",
      "iteration 153 , loss: 58.93042755126953\n",
      "iteration 154 , loss: 56.43175506591797\n",
      "iteration 155 , loss: 54.041526794433594\n",
      "iteration 156 , loss: 51.75798416137695\n",
      "iteration 157 , loss: 49.57341766357422\n",
      "iteration 158 , loss: 47.4843635559082\n",
      "iteration 159 , loss: 45.486122131347656\n",
      "iteration 160 , loss: 43.57546615600586\n",
      "iteration 161 , loss: 41.747161865234375\n",
      "iteration 162 , loss: 39.99824523925781\n",
      "iteration 163 , loss: 38.32487106323242\n",
      "iteration 164 , loss: 36.72468948364258\n",
      "iteration 165 , loss: 35.19253921508789\n",
      "iteration 166 , loss: 33.726036071777344\n",
      "iteration 167 , loss: 32.32234573364258\n",
      "iteration 168 , loss: 30.979568481445312\n",
      "iteration 169 , loss: 29.693689346313477\n",
      "iteration 170 , loss: 28.463029861450195\n",
      "iteration 171 , loss: 27.285432815551758\n",
      "iteration 172 , loss: 26.156539916992188\n",
      "iteration 173 , loss: 25.076690673828125\n",
      "iteration 174 , loss: 24.042205810546875\n",
      "iteration 175 , loss: 23.05140495300293\n",
      "iteration 176 , loss: 22.102476119995117\n",
      "iteration 177 , loss: 21.194978713989258\n",
      "iteration 178 , loss: 20.324493408203125\n",
      "iteration 179 , loss: 19.490697860717773\n",
      "iteration 180 , loss: 18.69224739074707\n",
      "iteration 181 , loss: 17.927261352539062\n",
      "iteration 182 , loss: 17.194494247436523\n",
      "iteration 183 , loss: 16.491811752319336\n",
      "iteration 184 , loss: 15.818975448608398\n",
      "iteration 185 , loss: 15.174097061157227\n",
      "iteration 186 , loss: 14.556325912475586\n",
      "iteration 187 , loss: 13.963830947875977\n",
      "iteration 188 , loss: 13.39647102355957\n",
      "iteration 189 , loss: 12.852645874023438\n",
      "iteration 190 , loss: 12.330808639526367\n",
      "iteration 191 , loss: 11.83121395111084\n",
      "iteration 192 , loss: 11.352071762084961\n",
      "iteration 193 , loss: 10.892072677612305\n",
      "iteration 194 , loss: 10.452208518981934\n",
      "iteration 195 , loss: 10.029571533203125\n",
      "iteration 196 , loss: 9.624946594238281\n",
      "iteration 197 , loss: 9.237045288085938\n",
      "iteration 198 , loss: 8.864571571350098\n",
      "iteration 199 , loss: 8.507972717285156\n",
      "iteration 200 , loss: 8.165567398071289\n",
      "iteration 201 , loss: 7.837224960327148\n",
      "iteration 202 , loss: 7.522188186645508\n",
      "iteration 203 , loss: 7.220375061035156\n",
      "iteration 204 , loss: 6.930716514587402\n",
      "iteration 205 , loss: 6.652906894683838\n",
      "iteration 206 , loss: 6.386406898498535\n",
      "iteration 207 , loss: 6.130654335021973\n",
      "iteration 208 , loss: 5.885509014129639\n",
      "iteration 209 , loss: 5.650147438049316\n",
      "iteration 210 , loss: 5.42448091506958\n",
      "iteration 211 , loss: 5.207810878753662\n",
      "iteration 212 , loss: 5.000147819519043\n",
      "iteration 213 , loss: 4.800638198852539\n",
      "iteration 214 , loss: 4.609192371368408\n",
      "iteration 215 , loss: 4.425631523132324\n",
      "iteration 216 , loss: 4.24958610534668\n",
      "iteration 217 , loss: 4.0804123878479\n",
      "iteration 218 , loss: 3.918426752090454\n",
      "iteration 219 , loss: 3.7626185417175293\n",
      "iteration 220 , loss: 3.613210678100586\n",
      "iteration 221 , loss: 3.4697365760803223\n",
      "iteration 222 , loss: 3.332061290740967\n",
      "iteration 223 , loss: 3.1999356746673584\n",
      "iteration 224 , loss: 3.073402166366577\n",
      "iteration 225 , loss: 2.9515745639801025\n",
      "iteration 226 , loss: 2.834723949432373\n",
      "iteration 227 , loss: 2.7225983142852783\n",
      "iteration 228 , loss: 2.614884376525879\n",
      "iteration 229 , loss: 2.5115671157836914\n",
      "iteration 230 , loss: 2.4123148918151855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 231 , loss: 2.31713604927063\n",
      "iteration 232 , loss: 2.2256031036376953\n",
      "iteration 233 , loss: 2.137796401977539\n",
      "iteration 234 , loss: 2.0533764362335205\n",
      "iteration 235 , loss: 1.9725390672683716\n",
      "iteration 236 , loss: 1.894963026046753\n",
      "iteration 237 , loss: 1.8202667236328125\n",
      "iteration 238 , loss: 1.7485911846160889\n",
      "iteration 239 , loss: 1.6797354221343994\n",
      "iteration 240 , loss: 1.6136040687561035\n",
      "iteration 241 , loss: 1.5502310991287231\n",
      "iteration 242 , loss: 1.489249348640442\n",
      "iteration 243 , loss: 1.4307622909545898\n",
      "iteration 244 , loss: 1.3745924234390259\n",
      "iteration 245 , loss: 1.3207014799118042\n",
      "iteration 246 , loss: 1.2688047885894775\n",
      "iteration 247 , loss: 1.2189857959747314\n",
      "iteration 248 , loss: 1.1712957620620728\n",
      "iteration 249 , loss: 1.125312089920044\n",
      "iteration 250 , loss: 1.0812067985534668\n",
      "iteration 251 , loss: 1.0388457775115967\n",
      "iteration 252 , loss: 0.9981720447540283\n",
      "iteration 253 , loss: 0.9591154456138611\n",
      "iteration 254 , loss: 0.9215317368507385\n",
      "iteration 255 , loss: 0.8855146765708923\n",
      "iteration 256 , loss: 0.8509403467178345\n",
      "iteration 257 , loss: 0.8176182508468628\n",
      "iteration 258 , loss: 0.7856844663619995\n",
      "iteration 259 , loss: 0.7549080848693848\n",
      "iteration 260 , loss: 0.7254700660705566\n",
      "iteration 261 , loss: 0.6971484422683716\n",
      "iteration 262 , loss: 0.6698836088180542\n",
      "iteration 263 , loss: 0.6437623500823975\n",
      "iteration 264 , loss: 0.6186352372169495\n",
      "iteration 265 , loss: 0.5945279002189636\n",
      "iteration 266 , loss: 0.5713711380958557\n",
      "iteration 267 , loss: 0.5490924119949341\n",
      "iteration 268 , loss: 0.5277491211891174\n",
      "iteration 269 , loss: 0.5071855783462524\n",
      "iteration 270 , loss: 0.48742103576660156\n",
      "iteration 271 , loss: 0.4684995412826538\n",
      "iteration 272 , loss: 0.4502106010913849\n",
      "iteration 273 , loss: 0.43269842863082886\n",
      "iteration 274 , loss: 0.41583356261253357\n",
      "iteration 275 , loss: 0.3995899558067322\n",
      "iteration 276 , loss: 0.3840276896953583\n",
      "iteration 277 , loss: 0.36913686990737915\n",
      "iteration 278 , loss: 0.35479459166526794\n",
      "iteration 279 , loss: 0.34097930788993835\n",
      "iteration 280 , loss: 0.32773637771606445\n",
      "iteration 281 , loss: 0.3150375485420227\n",
      "iteration 282 , loss: 0.30279770493507385\n",
      "iteration 283 , loss: 0.2910659611225128\n",
      "iteration 284 , loss: 0.27974873781204224\n",
      "iteration 285 , loss: 0.26887160539627075\n",
      "iteration 286 , loss: 0.2583947777748108\n",
      "iteration 287 , loss: 0.24839551746845245\n",
      "iteration 288 , loss: 0.23877765238285065\n",
      "iteration 289 , loss: 0.22951583564281464\n",
      "iteration 290 , loss: 0.2206433266401291\n",
      "iteration 291 , loss: 0.21203330159187317\n",
      "iteration 292 , loss: 0.20382505655288696\n",
      "iteration 293 , loss: 0.19592201709747314\n",
      "iteration 294 , loss: 0.18832097947597504\n",
      "iteration 295 , loss: 0.18106508255004883\n",
      "iteration 296 , loss: 0.17401747405529022\n",
      "iteration 297 , loss: 0.1673167645931244\n",
      "iteration 298 , loss: 0.160811185836792\n",
      "iteration 299 , loss: 0.15458250045776367\n",
      "iteration 300 , loss: 0.1486065834760666\n",
      "iteration 301 , loss: 0.14284169673919678\n",
      "iteration 302 , loss: 0.13733427226543427\n",
      "iteration 303 , loss: 0.1320362091064453\n",
      "iteration 304 , loss: 0.12691469490528107\n",
      "iteration 305 , loss: 0.12200405448675156\n",
      "iteration 306 , loss: 0.11728702485561371\n",
      "iteration 307 , loss: 0.11274193227291107\n",
      "iteration 308 , loss: 0.10841202735900879\n",
      "iteration 309 , loss: 0.1041940376162529\n",
      "iteration 310 , loss: 0.10019119083881378\n",
      "iteration 311 , loss: 0.09631385654211044\n",
      "iteration 312 , loss: 0.09262067079544067\n",
      "iteration 313 , loss: 0.08904535323381424\n",
      "iteration 314 , loss: 0.08562389761209488\n",
      "iteration 315 , loss: 0.08231806010007858\n",
      "iteration 316 , loss: 0.07915239781141281\n",
      "iteration 317 , loss: 0.0760841891169548\n",
      "iteration 318 , loss: 0.07315095514059067\n",
      "iteration 319 , loss: 0.07034172117710114\n",
      "iteration 320 , loss: 0.0676247775554657\n",
      "iteration 321 , loss: 0.06501757353544235\n",
      "iteration 322 , loss: 0.06251182407140732\n",
      "iteration 323 , loss: 0.06009521335363388\n",
      "iteration 324 , loss: 0.05778637155890465\n",
      "iteration 325 , loss: 0.055568061769008636\n",
      "iteration 326 , loss: 0.05341857671737671\n",
      "iteration 327 , loss: 0.051376666873693466\n",
      "iteration 328 , loss: 0.04940631985664368\n",
      "iteration 329 , loss: 0.04749765619635582\n",
      "iteration 330 , loss: 0.0456511415541172\n",
      "iteration 331 , loss: 0.04391222447156906\n",
      "iteration 332 , loss: 0.042235586792230606\n",
      "iteration 333 , loss: 0.0405949205160141\n",
      "iteration 334 , loss: 0.03905220329761505\n",
      "iteration 335 , loss: 0.037543054670095444\n",
      "iteration 336 , loss: 0.03609742224216461\n",
      "iteration 337 , loss: 0.03471023589372635\n",
      "iteration 338 , loss: 0.03337963670492172\n",
      "iteration 339 , loss: 0.03209961950778961\n",
      "iteration 340 , loss: 0.03087335079908371\n",
      "iteration 341 , loss: 0.02969832345843315\n",
      "iteration 342 , loss: 0.028566446155309677\n",
      "iteration 343 , loss: 0.027486780658364296\n",
      "iteration 344 , loss: 0.026427675038576126\n",
      "iteration 345 , loss: 0.02541213110089302\n",
      "iteration 346 , loss: 0.02443217858672142\n",
      "iteration 347 , loss: 0.023502660915255547\n",
      "iteration 348 , loss: 0.02262228913605213\n",
      "iteration 349 , loss: 0.02175477333366871\n",
      "iteration 350 , loss: 0.020928002893924713\n",
      "iteration 351 , loss: 0.020133137702941895\n",
      "iteration 352 , loss: 0.01937621273100376\n",
      "iteration 353 , loss: 0.01863970048725605\n",
      "iteration 354 , loss: 0.017922142520546913\n",
      "iteration 355 , loss: 0.01725039631128311\n",
      "iteration 356 , loss: 0.01659269817173481\n",
      "iteration 357 , loss: 0.015970751643180847\n",
      "iteration 358 , loss: 0.015370520763099194\n",
      "iteration 359 , loss: 0.014795703813433647\n",
      "iteration 360 , loss: 0.01423365343362093\n",
      "iteration 361 , loss: 0.013701303862035275\n",
      "iteration 362 , loss: 0.013183465227484703\n",
      "iteration 363 , loss: 0.012692952528595924\n",
      "iteration 364 , loss: 0.01221837755292654\n",
      "iteration 365 , loss: 0.011756179854273796\n",
      "iteration 366 , loss: 0.011315546929836273\n",
      "iteration 367 , loss: 0.010889263823628426\n",
      "iteration 368 , loss: 0.010485513135790825\n",
      "iteration 369 , loss: 0.010096452198922634\n",
      "iteration 370 , loss: 0.009716544300317764\n",
      "iteration 371 , loss: 0.009362810291349888\n",
      "iteration 372 , loss: 0.009008567780256271\n",
      "iteration 373 , loss: 0.00867416337132454\n",
      "iteration 374 , loss: 0.008356180042028427\n",
      "iteration 375 , loss: 0.008050307631492615\n",
      "iteration 376 , loss: 0.007754966616630554\n",
      "iteration 377 , loss: 0.007464745547622442\n",
      "iteration 378 , loss: 0.0071939099580049515\n",
      "iteration 379 , loss: 0.006925284396857023\n",
      "iteration 380 , loss: 0.006675390526652336\n",
      "iteration 381 , loss: 0.006426534615457058\n",
      "iteration 382 , loss: 0.006194972898811102\n",
      "iteration 383 , loss: 0.005976133048534393\n",
      "iteration 384 , loss: 0.005757451988756657\n",
      "iteration 385 , loss: 0.005547183565795422\n",
      "iteration 386 , loss: 0.00534852733835578\n",
      "iteration 387 , loss: 0.005161002278327942\n",
      "iteration 388 , loss: 0.004973596427589655\n",
      "iteration 389 , loss: 0.004794055130332708\n",
      "iteration 390 , loss: 0.004623216111212969\n",
      "iteration 391 , loss: 0.004462220706045628\n",
      "iteration 392 , loss: 0.004303109832108021\n",
      "iteration 393 , loss: 0.004150997381657362\n",
      "iteration 394 , loss: 0.0040034637786448\n",
      "iteration 395 , loss: 0.0038628242909908295\n",
      "iteration 396 , loss: 0.003728831186890602\n",
      "iteration 397 , loss: 0.003596921218559146\n",
      "iteration 398 , loss: 0.0034757228568196297\n",
      "iteration 399 , loss: 0.003353691194206476\n",
      "iteration 400 , loss: 0.0032424451783299446\n",
      "iteration 401 , loss: 0.0031311933416873217\n",
      "iteration 402 , loss: 0.0030238558538258076\n",
      "iteration 403 , loss: 0.0029225442558526993\n",
      "iteration 404 , loss: 0.0028226026333868504\n",
      "iteration 405 , loss: 0.0027264896780252457\n",
      "iteration 406 , loss: 0.0026373539585620165\n",
      "iteration 407 , loss: 0.0025496333837509155\n",
      "iteration 408 , loss: 0.002463228302076459\n",
      "iteration 409 , loss: 0.002380887744948268\n",
      "iteration 410 , loss: 0.0023040908854454756\n",
      "iteration 411 , loss: 0.0022281508427113295\n",
      "iteration 412 , loss: 0.002156489761546254\n",
      "iteration 413 , loss: 0.002084827981889248\n",
      "iteration 414 , loss: 0.0020166097674518824\n",
      "iteration 415 , loss: 0.0019501404603943229\n",
      "iteration 416 , loss: 0.0018864801386371255\n",
      "iteration 417 , loss: 0.0018279892392456532\n",
      "iteration 418 , loss: 0.0017701261676847935\n",
      "iteration 419 , loss: 0.0017132649663835764\n",
      "iteration 420 , loss: 0.001660541514866054\n",
      "iteration 421 , loss: 0.0016111874720081687\n",
      "iteration 422 , loss: 0.001560233999043703\n",
      "iteration 423 , loss: 0.0015125370118767023\n",
      "iteration 424 , loss: 0.0014658082509413362\n",
      "iteration 425 , loss: 0.001418268191628158\n",
      "iteration 426 , loss: 0.0013739089481532574\n",
      "iteration 427 , loss: 0.0013321936130523682\n",
      "iteration 428 , loss: 0.0012920519802719355\n",
      "iteration 429 , loss: 0.001252537127584219\n",
      "iteration 430 , loss: 0.0012150898110121489\n",
      "iteration 431 , loss: 0.0011783138616010547\n",
      "iteration 432 , loss: 0.0011450148886069655\n",
      "iteration 433 , loss: 0.0011109750485047698\n",
      "iteration 434 , loss: 0.0010776773560792208\n",
      "iteration 435 , loss: 0.0010469825938344002\n",
      "iteration 436 , loss: 0.0010167781729251146\n",
      "iteration 437 , loss: 0.0009878751588985324\n",
      "iteration 438 , loss: 0.0009595423471182585\n",
      "iteration 439 , loss: 0.0009307583095505834\n",
      "iteration 440 , loss: 0.000904698099475354\n",
      "iteration 441 , loss: 0.0008790497668087482\n",
      "iteration 442 , loss: 0.0008542057476006448\n",
      "iteration 443 , loss: 0.0008308444521389902\n",
      "iteration 444 , loss: 0.0008076323429122567\n",
      "iteration 445 , loss: 0.0007862680358812213\n",
      "iteration 446 , loss: 0.0007650530315004289\n",
      "iteration 447 , loss: 0.0007437174208462238\n",
      "iteration 448 , loss: 0.0007228422909975052\n",
      "iteration 449 , loss: 0.0007021260098554194\n",
      "iteration 450 , loss: 0.0006837371620349586\n",
      "iteration 451 , loss: 0.000665789470076561\n",
      "iteration 452 , loss: 0.0006499000592157245\n",
      "iteration 453 , loss: 0.0006319735548458993\n",
      "iteration 454 , loss: 0.0006165396771393716\n",
      "iteration 455 , loss: 0.0006003365851938725\n",
      "iteration 456 , loss: 0.0005846740677952766\n",
      "iteration 457 , loss: 0.0005709023680537939\n",
      "iteration 458 , loss: 0.0005552966031245887\n",
      "iteration 459 , loss: 0.0005415729829110205\n",
      "iteration 460 , loss: 0.0005279989563859999\n",
      "iteration 461 , loss: 0.0005141747533343732\n",
      "iteration 462 , loss: 0.0005030013853684068\n",
      "iteration 463 , loss: 0.0004900152562186122\n",
      "iteration 464 , loss: 0.0004792175896000117\n",
      "iteration 465 , loss: 0.0004672675859183073\n",
      "iteration 466 , loss: 0.0004556394706014544\n",
      "iteration 467 , loss: 0.0004437473835423589\n",
      "iteration 468 , loss: 0.0004334450059104711\n",
      "iteration 469 , loss: 0.0004241737478878349\n",
      "iteration 470 , loss: 0.00041437725303694606\n",
      "iteration 471 , loss: 0.0004040451312903315\n",
      "iteration 472 , loss: 0.00039442404522560537\n",
      "iteration 473 , loss: 0.0003864314057864249\n",
      "iteration 474 , loss: 0.0003764638095162809\n",
      "iteration 475 , loss: 0.00036889981129206717\n",
      "iteration 476 , loss: 0.00035995952202938497\n",
      "iteration 477 , loss: 0.0003521016624290496\n",
      "iteration 478 , loss: 0.00034367581247352064\n",
      "iteration 479 , loss: 0.0003361493581905961\n",
      "iteration 480 , loss: 0.00032860596547834575\n",
      "iteration 481 , loss: 0.0003215812030248344\n",
      "iteration 482 , loss: 0.00031390239018946886\n",
      "iteration 483 , loss: 0.00030789736774750054\n",
      "iteration 484 , loss: 0.00030186554067768157\n",
      "iteration 485 , loss: 0.000295269419439137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 486 , loss: 0.00028901596670039\n",
      "iteration 487 , loss: 0.00028307060711085796\n",
      "iteration 488 , loss: 0.0002761351934168488\n",
      "iteration 489 , loss: 0.0002704347134567797\n",
      "iteration 490 , loss: 0.0002648920635692775\n",
      "iteration 491 , loss: 0.0002589305804576725\n",
      "iteration 492 , loss: 0.0002539852575864643\n",
      "iteration 493 , loss: 0.00024899072013795376\n",
      "iteration 494 , loss: 0.0002441420219838619\n",
      "iteration 495 , loss: 0.00023855161271058023\n",
      "iteration 496 , loss: 0.0002345659158891067\n",
      "iteration 497 , loss: 0.00022961907961871475\n",
      "iteration 498 , loss: 0.00022488643298856914\n",
      "iteration 499 , loss: 0.00022019920288585126\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "dtype = torch.float  # data type\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # 撤销这句注释以在 GPU 上运行\n",
    "\n",
    "# N是数据样本数，D_in是输入层维度\n",
    "# H是隐藏层维度，D_out是输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机输入和输出数据\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6  # 学习率\n",
    "iteration_count = 500  # 迭代次数\n",
    "for t in range(iteration_count):\n",
    "    # 前向传播：计算预测值y\n",
    "    h = x.mm(w1)  # 矩阵乘法 matrix multiply\n",
    "    h_relu = h.clamp(min=0)  # h_relu = ReLU(h)\n",
    "    # 函数 clamp\n",
    "    # if x < min: x = min\n",
    "    # else if x > max: x = max\n",
    "    # else x = x\n",
    "    y_pred = h_relu.mm(w2)  # 矩阵乘法\n",
    "\n",
    "    # 计算并输出损失\n",
    "    loss = (y_pred - y).pow(2).sum().item()  # 平方损失\n",
    "    print(\"iteration\", t, \",\", \"loss:\", loss)  # t是循环变量\n",
    "\n",
    "    # 反向传播，计算w1和w2关于损失的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Tensor 和自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，我们手动实现了神经网络的前向和后向传播。手动实现反向传播对于小型双层网络来说并非难题，但对于大型复杂网络来说，很快就会变得非常繁琐。\n",
    "\n",
    "值得庆幸的是，我们可以使用自动微分来自动计算神经网络中的后向传播。PyTorch 中的 autograd 包提供了这个功能。使用 autograd 时，网络的前向传播将定义计算图；图中的节点是 Tensor，边是从输入 Tensor 产生输出 Tensor 的函数。通过此图反向传播，您可以轻松计算梯度。\n",
    "\n",
    "这听起来很复杂，在实践中使用起来非常简单。每个 Tensor 代表计算图中的节点。如果 x 是具有 x.requires_grad = True 的 Tensor，则 x.grad 是另一个Tensor，相对于某个标量值保持 x 的梯度。\n",
    "\n",
    "在这里，我们使用 PyTorch Tensor 和 autograd 来实现我们的双层网络;现在我们不再需要手动实现通过网络的反向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 , loss: 28531214.0\n",
      "iteration 1 , loss: 27689062.0\n",
      "iteration 2 , loss: 30838962.0\n",
      "iteration 3 , loss: 33524800.0\n",
      "iteration 4 , loss: 31596386.0\n",
      "iteration 5 , loss: 24105248.0\n",
      "iteration 6 , loss: 14803715.0\n",
      "iteration 7 , loss: 7824204.5\n",
      "iteration 8 , loss: 3994001.25\n",
      "iteration 9 , loss: 2196424.25\n",
      "iteration 10 , loss: 1377377.375\n",
      "iteration 11 , loss: 980553.3125\n",
      "iteration 12 , loss: 763403.25\n",
      "iteration 13 , loss: 626416.5\n",
      "iteration 14 , loss: 529183.0625\n",
      "iteration 15 , loss: 454337.65625\n",
      "iteration 16 , loss: 393925.46875\n",
      "iteration 17 , loss: 343872.4375\n",
      "iteration 18 , loss: 301695.5\n",
      "iteration 19 , loss: 265823.03125\n",
      "iteration 20 , loss: 235087.296875\n",
      "iteration 21 , loss: 208601.359375\n",
      "iteration 22 , loss: 185668.6875\n",
      "iteration 23 , loss: 165714.65625\n",
      "iteration 24 , loss: 148287.453125\n",
      "iteration 25 , loss: 133008.921875\n",
      "iteration 26 , loss: 119610.984375\n",
      "iteration 27 , loss: 107816.296875\n",
      "iteration 28 , loss: 97388.1953125\n",
      "iteration 29 , loss: 88142.0703125\n",
      "iteration 30 , loss: 79916.046875\n",
      "iteration 31 , loss: 72577.90625\n",
      "iteration 32 , loss: 66020.53125\n",
      "iteration 33 , loss: 60151.140625\n",
      "iteration 34 , loss: 54879.1953125\n",
      "iteration 35 , loss: 50138.15625\n",
      "iteration 36 , loss: 45870.63671875\n",
      "iteration 37 , loss: 42019.6640625\n",
      "iteration 38 , loss: 38536.515625\n",
      "iteration 39 , loss: 35380.4921875\n",
      "iteration 40 , loss: 32517.99609375\n",
      "iteration 41 , loss: 29917.130859375\n",
      "iteration 42 , loss: 27550.5\n",
      "iteration 43 , loss: 25394.708984375\n",
      "iteration 44 , loss: 23428.478515625\n",
      "iteration 45 , loss: 21633.4375\n",
      "iteration 46 , loss: 19991.8984375\n",
      "iteration 47 , loss: 18488.76953125\n",
      "iteration 48 , loss: 17111.599609375\n",
      "iteration 49 , loss: 15848.833984375\n",
      "iteration 50 , loss: 14689.68359375\n",
      "iteration 51 , loss: 13624.427734375\n",
      "iteration 52 , loss: 12644.712890625\n",
      "iteration 53 , loss: 11743.69140625\n",
      "iteration 54 , loss: 10913.5224609375\n",
      "iteration 55 , loss: 10147.71875\n",
      "iteration 56 , loss: 9441.3154296875\n",
      "iteration 57 , loss: 8788.7197265625\n",
      "iteration 58 , loss: 8186.10986328125\n",
      "iteration 59 , loss: 7628.93505859375\n",
      "iteration 60 , loss: 7112.9423828125\n",
      "iteration 61 , loss: 6635.044921875\n",
      "iteration 62 , loss: 6192.29296875\n",
      "iteration 63 , loss: 5782.00830078125\n",
      "iteration 64 , loss: 5401.0556640625\n",
      "iteration 65 , loss: 5047.4150390625\n",
      "iteration 66 , loss: 4718.86083984375\n",
      "iteration 67 , loss: 4413.62255859375\n",
      "iteration 68 , loss: 4129.75341796875\n",
      "iteration 69 , loss: 3865.642333984375\n",
      "iteration 70 , loss: 3619.802734375\n",
      "iteration 71 , loss: 3390.927490234375\n",
      "iteration 72 , loss: 3177.680419921875\n",
      "iteration 73 , loss: 2978.82275390625\n",
      "iteration 74 , loss: 2793.402099609375\n",
      "iteration 75 , loss: 2620.45654296875\n",
      "iteration 76 , loss: 2458.978271484375\n",
      "iteration 77 , loss: 2308.189453125\n",
      "iteration 78 , loss: 2167.373046875\n",
      "iteration 79 , loss: 2035.771240234375\n",
      "iteration 80 , loss: 1912.810546875\n",
      "iteration 81 , loss: 1798.02197265625\n",
      "iteration 82 , loss: 1690.5770263671875\n",
      "iteration 83 , loss: 1590.025634765625\n",
      "iteration 84 , loss: 1495.94775390625\n",
      "iteration 85 , loss: 1407.838134765625\n",
      "iteration 86 , loss: 1325.287841796875\n",
      "iteration 87 , loss: 1247.8885498046875\n",
      "iteration 88 , loss: 1175.3330078125\n",
      "iteration 89 , loss: 1107.291015625\n",
      "iteration 90 , loss: 1043.465087890625\n",
      "iteration 91 , loss: 983.578857421875\n",
      "iteration 92 , loss: 927.4705810546875\n",
      "iteration 93 , loss: 874.8168334960938\n",
      "iteration 94 , loss: 825.3807373046875\n",
      "iteration 95 , loss: 778.9490966796875\n",
      "iteration 96 , loss: 735.28955078125\n",
      "iteration 97 , loss: 694.2350463867188\n",
      "iteration 98 , loss: 655.6141967773438\n",
      "iteration 99 , loss: 619.2841186523438\n",
      "iteration 100 , loss: 585.1007690429688\n",
      "iteration 101 , loss: 552.9187622070312\n",
      "iteration 102 , loss: 522.6267700195312\n",
      "iteration 103 , loss: 494.0892028808594\n",
      "iteration 104 , loss: 467.1953430175781\n",
      "iteration 105 , loss: 441.86395263671875\n",
      "iteration 106 , loss: 417.9826965332031\n",
      "iteration 107 , loss: 395.47119140625\n",
      "iteration 108 , loss: 374.2366027832031\n",
      "iteration 109 , loss: 354.22698974609375\n",
      "iteration 110 , loss: 335.3457946777344\n",
      "iteration 111 , loss: 317.52325439453125\n",
      "iteration 112 , loss: 300.7001647949219\n",
      "iteration 113 , loss: 284.8155822753906\n",
      "iteration 114 , loss: 269.8210754394531\n",
      "iteration 115 , loss: 255.6576385498047\n",
      "iteration 116 , loss: 242.28341674804688\n",
      "iteration 117 , loss: 229.642578125\n",
      "iteration 118 , loss: 217.69308471679688\n",
      "iteration 119 , loss: 206.3963165283203\n",
      "iteration 120 , loss: 195.7216796875\n",
      "iteration 121 , loss: 185.62596130371094\n",
      "iteration 122 , loss: 176.07774353027344\n",
      "iteration 123 , loss: 167.04563903808594\n",
      "iteration 124 , loss: 158.4983673095703\n",
      "iteration 125 , loss: 150.40997314453125\n",
      "iteration 126 , loss: 142.757568359375\n",
      "iteration 127 , loss: 135.5149688720703\n",
      "iteration 128 , loss: 128.6551971435547\n",
      "iteration 129 , loss: 122.15711975097656\n",
      "iteration 130 , loss: 116.00856018066406\n",
      "iteration 131 , loss: 110.1844711303711\n",
      "iteration 132 , loss: 104.6661148071289\n",
      "iteration 133 , loss: 99.43521881103516\n",
      "iteration 134 , loss: 94.47940826416016\n",
      "iteration 135 , loss: 89.78060150146484\n",
      "iteration 136 , loss: 85.32431030273438\n",
      "iteration 137 , loss: 81.1010971069336\n",
      "iteration 138 , loss: 77.09606170654297\n",
      "iteration 139 , loss: 73.29717254638672\n",
      "iteration 140 , loss: 69.6932601928711\n",
      "iteration 141 , loss: 66.27289581298828\n",
      "iteration 142 , loss: 63.0302619934082\n",
      "iteration 143 , loss: 59.95320129394531\n",
      "iteration 144 , loss: 57.031150817871094\n",
      "iteration 145 , loss: 54.25847625732422\n",
      "iteration 146 , loss: 51.62483215332031\n",
      "iteration 147 , loss: 49.123531341552734\n",
      "iteration 148 , loss: 46.74861145019531\n",
      "iteration 149 , loss: 44.494468688964844\n",
      "iteration 150 , loss: 42.35371398925781\n",
      "iteration 151 , loss: 40.31863021850586\n",
      "iteration 152 , loss: 38.384613037109375\n",
      "iteration 153 , loss: 36.547794342041016\n",
      "iteration 154 , loss: 34.801517486572266\n",
      "iteration 155 , loss: 33.14258575439453\n",
      "iteration 156 , loss: 31.56488800048828\n",
      "iteration 157 , loss: 30.066116333007812\n",
      "iteration 158 , loss: 28.64006805419922\n",
      "iteration 159 , loss: 27.28369903564453\n",
      "iteration 160 , loss: 25.993743896484375\n",
      "iteration 161 , loss: 24.76841926574707\n",
      "iteration 162 , loss: 23.60108184814453\n",
      "iteration 163 , loss: 22.491052627563477\n",
      "iteration 164 , loss: 21.43572235107422\n",
      "iteration 165 , loss: 20.43059539794922\n",
      "iteration 166 , loss: 19.474199295043945\n",
      "iteration 167 , loss: 18.563886642456055\n",
      "iteration 168 , loss: 17.698436737060547\n",
      "iteration 169 , loss: 16.874574661254883\n",
      "iteration 170 , loss: 16.089509963989258\n",
      "iteration 171 , loss: 15.342432975769043\n",
      "iteration 172 , loss: 14.630828857421875\n",
      "iteration 173 , loss: 13.953399658203125\n",
      "iteration 174 , loss: 13.308304786682129\n",
      "iteration 175 , loss: 12.693666458129883\n",
      "iteration 176 , loss: 12.1085844039917\n",
      "iteration 177 , loss: 11.550771713256836\n",
      "iteration 178 , loss: 11.019312858581543\n",
      "iteration 179 , loss: 10.513276100158691\n",
      "iteration 180 , loss: 10.03134822845459\n",
      "iteration 181 , loss: 9.571533203125\n",
      "iteration 182 , loss: 9.133650779724121\n",
      "iteration 183 , loss: 8.716787338256836\n",
      "iteration 184 , loss: 8.31904411315918\n",
      "iteration 185 , loss: 7.9395623207092285\n",
      "iteration 186 , loss: 7.577993392944336\n",
      "iteration 187 , loss: 7.233572959899902\n",
      "iteration 188 , loss: 6.905392169952393\n",
      "iteration 189 , loss: 6.591896057128906\n",
      "iteration 190 , loss: 6.293533802032471\n",
      "iteration 191 , loss: 6.008559703826904\n",
      "iteration 192 , loss: 5.736822605133057\n",
      "iteration 193 , loss: 5.477746486663818\n",
      "iteration 194 , loss: 5.230498313903809\n",
      "iteration 195 , loss: 4.994915008544922\n",
      "iteration 196 , loss: 4.770195960998535\n",
      "iteration 197 , loss: 4.555769920349121\n",
      "iteration 198 , loss: 4.350784778594971\n",
      "iteration 199 , loss: 4.155577182769775\n",
      "iteration 200 , loss: 3.969275712966919\n",
      "iteration 201 , loss: 3.7915780544281006\n",
      "iteration 202 , loss: 3.6219871044158936\n",
      "iteration 203 , loss: 3.4599456787109375\n",
      "iteration 204 , loss: 3.305337905883789\n",
      "iteration 205 , loss: 3.1580371856689453\n",
      "iteration 206 , loss: 3.0172665119171143\n",
      "iteration 207 , loss: 2.8829193115234375\n",
      "iteration 208 , loss: 2.75454044342041\n",
      "iteration 209 , loss: 2.6322214603424072\n",
      "iteration 210 , loss: 2.5152599811553955\n",
      "iteration 211 , loss: 2.403567314147949\n",
      "iteration 212 , loss: 2.296921730041504\n",
      "iteration 213 , loss: 2.195089817047119\n",
      "iteration 214 , loss: 2.0979084968566895\n",
      "iteration 215 , loss: 2.0049922466278076\n",
      "iteration 216 , loss: 1.9164141416549683\n",
      "iteration 217 , loss: 1.8319153785705566\n",
      "iteration 218 , loss: 1.751004934310913\n",
      "iteration 219 , loss: 1.673704981803894\n",
      "iteration 220 , loss: 1.600005865097046\n",
      "iteration 221 , loss: 1.5295140743255615\n",
      "iteration 222 , loss: 1.4622597694396973\n",
      "iteration 223 , loss: 1.3980050086975098\n",
      "iteration 224 , loss: 1.336607813835144\n",
      "iteration 225 , loss: 1.2778856754302979\n",
      "iteration 226 , loss: 1.221886157989502\n",
      "iteration 227 , loss: 1.1681838035583496\n",
      "iteration 228 , loss: 1.1171090602874756\n",
      "iteration 229 , loss: 1.0681666135787964\n",
      "iteration 230 , loss: 1.021381139755249\n",
      "iteration 231 , loss: 0.9767537117004395\n",
      "iteration 232 , loss: 0.9339795112609863\n",
      "iteration 233 , loss: 0.8932145833969116\n",
      "iteration 234 , loss: 0.854276716709137\n",
      "iteration 235 , loss: 0.8170152306556702\n",
      "iteration 236 , loss: 0.7813735008239746\n",
      "iteration 237 , loss: 0.7473434209823608\n",
      "iteration 238 , loss: 0.7148012518882751\n",
      "iteration 239 , loss: 0.6836816668510437\n",
      "iteration 240 , loss: 0.6539707779884338\n",
      "iteration 241 , loss: 0.6256136894226074\n",
      "iteration 242 , loss: 0.5983343124389648\n",
      "iteration 243 , loss: 0.5724667906761169\n",
      "iteration 244 , loss: 0.5476539134979248\n",
      "iteration 245 , loss: 0.5238771438598633\n",
      "iteration 246 , loss: 0.5012606382369995\n",
      "iteration 247 , loss: 0.47951540350914\n",
      "iteration 248 , loss: 0.45869794487953186\n",
      "iteration 249 , loss: 0.4388018548488617\n",
      "iteration 250 , loss: 0.41988351941108704\n",
      "iteration 251 , loss: 0.4016951620578766\n",
      "iteration 252 , loss: 0.38433146476745605\n",
      "iteration 253 , loss: 0.3677600622177124\n",
      "iteration 254 , loss: 0.3518739938735962\n",
      "iteration 255 , loss: 0.3366680443286896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 256 , loss: 0.32217103242874146\n",
      "iteration 257 , loss: 0.30832892656326294\n",
      "iteration 258 , loss: 0.2950286567211151\n",
      "iteration 259 , loss: 0.2823435962200165\n",
      "iteration 260 , loss: 0.27019017934799194\n",
      "iteration 261 , loss: 0.2585216462612152\n",
      "iteration 262 , loss: 0.2474524825811386\n",
      "iteration 263 , loss: 0.23679931461811066\n",
      "iteration 264 , loss: 0.22663022577762604\n",
      "iteration 265 , loss: 0.21690930426120758\n",
      "iteration 266 , loss: 0.20761583745479584\n",
      "iteration 267 , loss: 0.19870463013648987\n",
      "iteration 268 , loss: 0.190194770693779\n",
      "iteration 269 , loss: 0.18205435574054718\n",
      "iteration 270 , loss: 0.17425155639648438\n",
      "iteration 271 , loss: 0.16680128872394562\n",
      "iteration 272 , loss: 0.15961702167987823\n",
      "iteration 273 , loss: 0.1527835875749588\n",
      "iteration 274 , loss: 0.14627139270305634\n",
      "iteration 275 , loss: 0.1400095522403717\n",
      "iteration 276 , loss: 0.13401833176612854\n",
      "iteration 277 , loss: 0.1282569020986557\n",
      "iteration 278 , loss: 0.12282818555831909\n",
      "iteration 279 , loss: 0.11758533120155334\n",
      "iteration 280 , loss: 0.11254975199699402\n",
      "iteration 281 , loss: 0.10777553915977478\n",
      "iteration 282 , loss: 0.10316116362810135\n",
      "iteration 283 , loss: 0.09876209497451782\n",
      "iteration 284 , loss: 0.09454625099897385\n",
      "iteration 285 , loss: 0.09052872657775879\n",
      "iteration 286 , loss: 0.08666663616895676\n",
      "iteration 287 , loss: 0.08297659456729889\n",
      "iteration 288 , loss: 0.07945774495601654\n",
      "iteration 289 , loss: 0.07610726356506348\n",
      "iteration 290 , loss: 0.0728616937994957\n",
      "iteration 291 , loss: 0.06977219879627228\n",
      "iteration 292 , loss: 0.06680009514093399\n",
      "iteration 293 , loss: 0.06396008282899857\n",
      "iteration 294 , loss: 0.06125403195619583\n",
      "iteration 295 , loss: 0.058642107993364334\n",
      "iteration 296 , loss: 0.05617857351899147\n",
      "iteration 297 , loss: 0.053791530430316925\n",
      "iteration 298 , loss: 0.05150652676820755\n",
      "iteration 299 , loss: 0.04933770000934601\n",
      "iteration 300 , loss: 0.047255225479602814\n",
      "iteration 301 , loss: 0.04524866119027138\n",
      "iteration 302 , loss: 0.04333633929491043\n",
      "iteration 303 , loss: 0.04150673374533653\n",
      "iteration 304 , loss: 0.03974013775587082\n",
      "iteration 305 , loss: 0.03807123750448227\n",
      "iteration 306 , loss: 0.03647361323237419\n",
      "iteration 307 , loss: 0.034942880272865295\n",
      "iteration 308 , loss: 0.033479753881692886\n",
      "iteration 309 , loss: 0.03206445649266243\n",
      "iteration 310 , loss: 0.030706563964486122\n",
      "iteration 311 , loss: 0.02941378392279148\n",
      "iteration 312 , loss: 0.02818669192492962\n",
      "iteration 313 , loss: 0.02700268104672432\n",
      "iteration 314 , loss: 0.025870032608509064\n",
      "iteration 315 , loss: 0.024785079061985016\n",
      "iteration 316 , loss: 0.023757971823215485\n",
      "iteration 317 , loss: 0.022765392437577248\n",
      "iteration 318 , loss: 0.021809374913573265\n",
      "iteration 319 , loss: 0.02089880406856537\n",
      "iteration 320 , loss: 0.020029114559292793\n",
      "iteration 321 , loss: 0.019186563789844513\n",
      "iteration 322 , loss: 0.01838909089565277\n",
      "iteration 323 , loss: 0.01763022691011429\n",
      "iteration 324 , loss: 0.01690901443362236\n",
      "iteration 325 , loss: 0.016207154840230942\n",
      "iteration 326 , loss: 0.015523636713624\n",
      "iteration 327 , loss: 0.014888012781739235\n",
      "iteration 328 , loss: 0.014272541739046574\n",
      "iteration 329 , loss: 0.013681857846677303\n",
      "iteration 330 , loss: 0.013112087734043598\n",
      "iteration 331 , loss: 0.012579486705362797\n",
      "iteration 332 , loss: 0.012058427557349205\n",
      "iteration 333 , loss: 0.011568361893296242\n",
      "iteration 334 , loss: 0.011094370856881142\n",
      "iteration 335 , loss: 0.010633187368512154\n",
      "iteration 336 , loss: 0.010206297971308231\n",
      "iteration 337 , loss: 0.00978466123342514\n",
      "iteration 338 , loss: 0.009385193698108196\n",
      "iteration 339 , loss: 0.009009498171508312\n",
      "iteration 340 , loss: 0.00864432379603386\n",
      "iteration 341 , loss: 0.008289615623652935\n",
      "iteration 342 , loss: 0.007959275506436825\n",
      "iteration 343 , loss: 0.007637688424438238\n",
      "iteration 344 , loss: 0.007330181542783976\n",
      "iteration 345 , loss: 0.007040149532258511\n",
      "iteration 346 , loss: 0.006751114968210459\n",
      "iteration 347 , loss: 0.0064816344529390335\n",
      "iteration 348 , loss: 0.006223685573786497\n",
      "iteration 349 , loss: 0.005978570785373449\n",
      "iteration 350 , loss: 0.005745903588831425\n",
      "iteration 351 , loss: 0.0055176871828734875\n",
      "iteration 352 , loss: 0.00529731810092926\n",
      "iteration 353 , loss: 0.005090394522994757\n",
      "iteration 354 , loss: 0.004890521056950092\n",
      "iteration 355 , loss: 0.004706551320850849\n",
      "iteration 356 , loss: 0.004521445836871862\n",
      "iteration 357 , loss: 0.004351707175374031\n",
      "iteration 358 , loss: 0.0041780853644013405\n",
      "iteration 359 , loss: 0.004019305109977722\n",
      "iteration 360 , loss: 0.0038649323396384716\n",
      "iteration 361 , loss: 0.0037187773268669844\n",
      "iteration 362 , loss: 0.0035808642860502005\n",
      "iteration 363 , loss: 0.0034450720995664597\n",
      "iteration 364 , loss: 0.0033200313337147236\n",
      "iteration 365 , loss: 0.0031957344617694616\n",
      "iteration 366 , loss: 0.0030771158635616302\n",
      "iteration 367 , loss: 0.0029653089586645365\n",
      "iteration 368 , loss: 0.002855587052181363\n",
      "iteration 369 , loss: 0.0027489985805004835\n",
      "iteration 370 , loss: 0.002649869304150343\n",
      "iteration 371 , loss: 0.002554351929575205\n",
      "iteration 372 , loss: 0.002465966157615185\n",
      "iteration 373 , loss: 0.002376961288973689\n",
      "iteration 374 , loss: 0.002292213262990117\n",
      "iteration 375 , loss: 0.002211572602391243\n",
      "iteration 376 , loss: 0.00213336362503469\n",
      "iteration 377 , loss: 0.002057569334283471\n",
      "iteration 378 , loss: 0.0019847762305289507\n",
      "iteration 379 , loss: 0.0019155126065015793\n",
      "iteration 380 , loss: 0.001848354353569448\n",
      "iteration 381 , loss: 0.0017861331580206752\n",
      "iteration 382 , loss: 0.0017253581900149584\n",
      "iteration 383 , loss: 0.0016678099054843187\n",
      "iteration 384 , loss: 0.0016093081794679165\n",
      "iteration 385 , loss: 0.001554760616272688\n",
      "iteration 386 , loss: 0.0015016006072983146\n",
      "iteration 387 , loss: 0.0014519599499180913\n",
      "iteration 388 , loss: 0.0014057650696486235\n",
      "iteration 389 , loss: 0.001357860746793449\n",
      "iteration 390 , loss: 0.0013141782255843282\n",
      "iteration 391 , loss: 0.001271516433916986\n",
      "iteration 392 , loss: 0.0012311531463637948\n",
      "iteration 393 , loss: 0.0011916977819055319\n",
      "iteration 394 , loss: 0.0011533641954883933\n",
      "iteration 395 , loss: 0.001118772430345416\n",
      "iteration 396 , loss: 0.001083353185094893\n",
      "iteration 397 , loss: 0.001049427897669375\n",
      "iteration 398 , loss: 0.0010171420872211456\n",
      "iteration 399 , loss: 0.000984984915703535\n",
      "iteration 400 , loss: 0.0009563128696754575\n",
      "iteration 401 , loss: 0.0009275602642446756\n",
      "iteration 402 , loss: 0.0009004314779303968\n",
      "iteration 403 , loss: 0.0008731277193874121\n",
      "iteration 404 , loss: 0.0008472923072986305\n",
      "iteration 405 , loss: 0.0008226968348026276\n",
      "iteration 406 , loss: 0.0007981373928487301\n",
      "iteration 407 , loss: 0.0007751542725600302\n",
      "iteration 408 , loss: 0.0007536813500337303\n",
      "iteration 409 , loss: 0.0007324505131691694\n",
      "iteration 410 , loss: 0.0007109089638106525\n",
      "iteration 411 , loss: 0.0006915527628734708\n",
      "iteration 412 , loss: 0.0006711892783641815\n",
      "iteration 413 , loss: 0.0006534636486321688\n",
      "iteration 414 , loss: 0.0006355689838528633\n",
      "iteration 415 , loss: 0.0006185503443703055\n",
      "iteration 416 , loss: 0.0006002653972245753\n",
      "iteration 417 , loss: 0.0005843770341016352\n",
      "iteration 418 , loss: 0.0005690680118277669\n",
      "iteration 419 , loss: 0.0005536245880648494\n",
      "iteration 420 , loss: 0.0005392340826801956\n",
      "iteration 421 , loss: 0.0005250368849374354\n",
      "iteration 422 , loss: 0.0005106858443468809\n",
      "iteration 423 , loss: 0.0004979756777174771\n",
      "iteration 424 , loss: 0.0004847442323807627\n",
      "iteration 425 , loss: 0.0004728632920887321\n",
      "iteration 426 , loss: 0.00045981042785570025\n",
      "iteration 427 , loss: 0.0004487850237637758\n",
      "iteration 428 , loss: 0.00043827391345985234\n",
      "iteration 429 , loss: 0.00042723986553028226\n",
      "iteration 430 , loss: 0.00041674915701150894\n",
      "iteration 431 , loss: 0.00040550431003794074\n",
      "iteration 432 , loss: 0.0003968039236497134\n",
      "iteration 433 , loss: 0.00038779014721512794\n",
      "iteration 434 , loss: 0.00037780997809022665\n",
      "iteration 435 , loss: 0.0003692666650749743\n",
      "iteration 436 , loss: 0.00036075443495064974\n",
      "iteration 437 , loss: 0.0003517836157698184\n",
      "iteration 438 , loss: 0.00034307673922739923\n",
      "iteration 439 , loss: 0.00033497248659841716\n",
      "iteration 440 , loss: 0.0003264373226556927\n",
      "iteration 441 , loss: 0.0003193204174749553\n",
      "iteration 442 , loss: 0.00031181989470496774\n",
      "iteration 443 , loss: 0.0003049210936296731\n",
      "iteration 444 , loss: 0.00029821315547451377\n",
      "iteration 445 , loss: 0.0002915088552981615\n",
      "iteration 446 , loss: 0.0002858619554899633\n",
      "iteration 447 , loss: 0.00027888669865205884\n",
      "iteration 448 , loss: 0.00027332696481607854\n",
      "iteration 449 , loss: 0.0002671842521522194\n",
      "iteration 450 , loss: 0.00026058152434416115\n",
      "iteration 451 , loss: 0.0002556004619691521\n",
      "iteration 452 , loss: 0.00024994011619128287\n",
      "iteration 453 , loss: 0.0002442551776766777\n",
      "iteration 454 , loss: 0.00023926129506435245\n",
      "iteration 455 , loss: 0.00023448214051313698\n",
      "iteration 456 , loss: 0.00022918908507563174\n",
      "iteration 457 , loss: 0.00022491517302114516\n",
      "iteration 458 , loss: 0.0002195190027123317\n",
      "iteration 459 , loss: 0.0002153394598281011\n",
      "iteration 460 , loss: 0.00021009940246585757\n",
      "iteration 461 , loss: 0.00020688721269834787\n",
      "iteration 462 , loss: 0.0002030178002314642\n",
      "iteration 463 , loss: 0.0001990489399759099\n",
      "iteration 464 , loss: 0.00019455133588053286\n",
      "iteration 465 , loss: 0.00019083631923422217\n",
      "iteration 466 , loss: 0.00018755380006041378\n",
      "iteration 467 , loss: 0.00018387445015832782\n",
      "iteration 468 , loss: 0.00018058977730106562\n",
      "iteration 469 , loss: 0.00017732923151925206\n",
      "iteration 470 , loss: 0.0001743404136504978\n",
      "iteration 471 , loss: 0.00017108187603298575\n",
      "iteration 472 , loss: 0.00016759417485445738\n",
      "iteration 473 , loss: 0.00016455622971989214\n",
      "iteration 474 , loss: 0.00016087392577901483\n",
      "iteration 475 , loss: 0.0001582805416546762\n",
      "iteration 476 , loss: 0.00015551831165794283\n",
      "iteration 477 , loss: 0.00015295030607376248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 478 , loss: 0.0001502787199569866\n",
      "iteration 479 , loss: 0.00014739845937583596\n",
      "iteration 480 , loss: 0.0001449470000807196\n",
      "iteration 481 , loss: 0.00014216073032002896\n",
      "iteration 482 , loss: 0.0001400971959810704\n",
      "iteration 483 , loss: 0.00013714835222344846\n",
      "iteration 484 , loss: 0.00013471576676238328\n",
      "iteration 485 , loss: 0.00013280218990985304\n",
      "iteration 486 , loss: 0.00013051132555119693\n",
      "iteration 487 , loss: 0.0001284302561543882\n",
      "iteration 488 , loss: 0.0001263170561287552\n",
      "iteration 489 , loss: 0.00012421308201737702\n",
      "iteration 490 , loss: 0.00012221717042848468\n",
      "iteration 491 , loss: 0.00012026918557239696\n",
      "iteration 492 , loss: 0.00011832278687506914\n",
      "iteration 493 , loss: 0.00011648976942524314\n",
      "iteration 494 , loss: 0.00011444505071267486\n",
      "iteration 495 , loss: 0.00011253806587774307\n",
      "iteration 496 , loss: 0.00011076083319494501\n",
      "iteration 497 , loss: 0.00010893803846556693\n",
      "iteration 498 , loss: 0.0001069352074409835\n",
      "iteration 499 , loss: 0.00010499829659238458\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # 取消注释以在 GPU 上运行\n",
    "\n",
    "# N是数据样本数，D_in是输入层维度\n",
    "# H是隐藏层维度，D_out是输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机输入和输出数据\n",
    "# 设置 requires_grad = False 表示在反向传播中不需要计算关于该 Tensor 的梯度\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 随机初始化权重\n",
    "# 设置 requires_grad = True 表示在反向传播中想要计算关于该 Tensor 的梯度\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6  # 学习率\n",
    "iteration_count = 500  # 迭代次数\n",
    "for t in range(iteration_count):\n",
    "    # 前向传播：计算预测值y\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    # 因为反向传播不再需要手动实现，不需要定义中间变量\n",
    "\n",
    "    # 使用 Tensor 运算，计算并输出损失\n",
    "    # 损失是一个 Tensor (1,)\n",
    "    # loss.item() 获得 loss 中的标量\n",
    "    loss = (y_pred - y).pow(2).sum()  # 平方损失\n",
    "    print(\"iteration\", t, \",\", \"loss:\", loss.item())  # t是循环变量\n",
    "\n",
    "    # 使用 autograd 计算反向传播\n",
    "    # 这条语句将计算损失函数关于所有 requires_grad = True 的 Tensor 的梯度\n",
    "    # 之后，得到 w1.grad 和 w2.grad\n",
    "    loss.backward()\n",
    "\n",
    "    # 使用梯度下降手动更新权重\n",
    "    # 包在 torch.no_grad() 是因为权重的 requires_grad = True，但此时不需要追踪梯度\n",
    "    # 另一种方式是操作 weight.data 和 weight.grad.data\n",
    "    # 回想一下，tensor.data给出了一个 Tensor ，它与 Tensor 共享存储，但不跟踪历史记录，即没有梯度的产生\n",
    "    # 也可以使用 torch.optim.SGD 实现\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 更新权重后手动清零梯度\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义新的自动微分函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这之后，每个原始 autograd 运算符实际上是两个在 Tensor 上运行的函数。前向函数从输入 Tensor 计算输出 Tensor. 反向函数接收输出 Tensor 相对于某个标量值的梯度，并根据相同的标量值计算输入 Tensor 的梯度。\n",
    "\n",
    "在 PyTorch 中，我们可以通过定义 torch.autograd.Function 的子类并实现前向和后向函数来轻松定义我们自己的 autograd 运算符。然后我们可以使用我们的新 autograd 运算符，通过构造一个实例并像函数一样调用它，传递包含输入数据的 Tensor。\n",
    "\n",
    "在这个例子中，我们定义了自己的自定义 autograd 函数来执行 ReLU 非线性，并使用它来实现我们的双层网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 , loss: 30721902.0\n",
      "iteration 1 , loss: 28507020.0\n",
      "iteration 2 , loss: 33064666.0\n",
      "iteration 3 , loss: 38874488.0\n",
      "iteration 4 , loss: 39144912.0\n",
      "iteration 5 , loss: 30448758.0\n",
      "iteration 6 , loss: 17675790.0\n",
      "iteration 7 , loss: 8324366.5\n",
      "iteration 8 , loss: 3812623.25\n",
      "iteration 9 , loss: 2011797.625\n",
      "iteration 10 , loss: 1294538.25\n",
      "iteration 11 , loss: 967610.0625\n",
      "iteration 12 , loss: 784708.5625\n",
      "iteration 13 , loss: 661807.875\n",
      "iteration 14 , loss: 568817.9375\n",
      "iteration 15 , loss: 494109.34375\n",
      "iteration 16 , loss: 432066.53125\n",
      "iteration 17 , loss: 379756.96875\n",
      "iteration 18 , loss: 335191.5625\n",
      "iteration 19 , loss: 296913.84375\n",
      "iteration 20 , loss: 263870.71875\n",
      "iteration 21 , loss: 235224.5625\n",
      "iteration 22 , loss: 210266.609375\n",
      "iteration 23 , loss: 188481.25\n",
      "iteration 24 , loss: 169364.546875\n",
      "iteration 25 , loss: 152546.765625\n",
      "iteration 26 , loss: 137704.984375\n",
      "iteration 27 , loss: 124556.578125\n",
      "iteration 28 , loss: 112884.0859375\n",
      "iteration 29 , loss: 102512.140625\n",
      "iteration 30 , loss: 93256.6875\n",
      "iteration 31 , loss: 84972.921875\n",
      "iteration 32 , loss: 77550.8359375\n",
      "iteration 33 , loss: 70874.1796875\n",
      "iteration 34 , loss: 64861.30078125\n",
      "iteration 35 , loss: 59435.28125\n",
      "iteration 36 , loss: 54532.953125\n",
      "iteration 37 , loss: 50092.65625\n",
      "iteration 38 , loss: 46065.48828125\n",
      "iteration 39 , loss: 42411.484375\n",
      "iteration 40 , loss: 39091.890625\n",
      "iteration 41 , loss: 36068.01953125\n",
      "iteration 42 , loss: 33309.83203125\n",
      "iteration 43 , loss: 30802.140625\n",
      "iteration 44 , loss: 28514.8125\n",
      "iteration 45 , loss: 26420.296875\n",
      "iteration 46 , loss: 24499.220703125\n",
      "iteration 47 , loss: 22738.693359375\n",
      "iteration 48 , loss: 21120.443359375\n",
      "iteration 49 , loss: 19632.283203125\n",
      "iteration 50 , loss: 18262.109375\n",
      "iteration 51 , loss: 16999.029296875\n",
      "iteration 52 , loss: 15834.326171875\n",
      "iteration 53 , loss: 14760.0556640625\n",
      "iteration 54 , loss: 13767.380859375\n",
      "iteration 55 , loss: 12849.2587890625\n",
      "iteration 56 , loss: 11999.29296875\n",
      "iteration 57 , loss: 11212.31640625\n",
      "iteration 58 , loss: 10482.6318359375\n",
      "iteration 59 , loss: 9805.689453125\n",
      "iteration 60 , loss: 9176.955078125\n",
      "iteration 61 , loss: 8593.3916015625\n",
      "iteration 62 , loss: 8050.50830078125\n",
      "iteration 63 , loss: 7545.59619140625\n",
      "iteration 64 , loss: 7075.2158203125\n",
      "iteration 65 , loss: 6637.46826171875\n",
      "iteration 66 , loss: 6229.576171875\n",
      "iteration 67 , loss: 5849.001953125\n",
      "iteration 68 , loss: 5493.82177734375\n",
      "iteration 69 , loss: 5162.25146484375\n",
      "iteration 70 , loss: 4852.48779296875\n",
      "iteration 71 , loss: 4563.056640625\n",
      "iteration 72 , loss: 4292.4716796875\n",
      "iteration 73 , loss: 4039.138671875\n",
      "iteration 74 , loss: 3802.15283203125\n",
      "iteration 75 , loss: 3580.260009765625\n",
      "iteration 76 , loss: 3372.35595703125\n",
      "iteration 77 , loss: 3177.573486328125\n",
      "iteration 78 , loss: 2995.023193359375\n",
      "iteration 79 , loss: 2823.836181640625\n",
      "iteration 80 , loss: 2663.227783203125\n",
      "iteration 81 , loss: 2512.427734375\n",
      "iteration 82 , loss: 2370.87890625\n",
      "iteration 83 , loss: 2237.833251953125\n",
      "iteration 84 , loss: 2112.822265625\n",
      "iteration 85 , loss: 1995.3648681640625\n",
      "iteration 86 , loss: 1884.867919921875\n",
      "iteration 87 , loss: 1780.93212890625\n",
      "iteration 88 , loss: 1683.07861328125\n",
      "iteration 89 , loss: 1590.9940185546875\n",
      "iteration 90 , loss: 1504.247314453125\n",
      "iteration 91 , loss: 1422.5316162109375\n",
      "iteration 92 , loss: 1345.5576171875\n",
      "iteration 93 , loss: 1273.01611328125\n",
      "iteration 94 , loss: 1204.642822265625\n",
      "iteration 95 , loss: 1140.1844482421875\n",
      "iteration 96 , loss: 1079.369873046875\n",
      "iteration 97 , loss: 1022.0154418945312\n",
      "iteration 98 , loss: 967.8934936523438\n",
      "iteration 99 , loss: 916.811767578125\n",
      "iteration 100 , loss: 868.5634155273438\n",
      "iteration 101 , loss: 822.9930419921875\n",
      "iteration 102 , loss: 779.9597778320312\n",
      "iteration 103 , loss: 739.2867431640625\n",
      "iteration 104 , loss: 700.8449096679688\n",
      "iteration 105 , loss: 664.5223388671875\n",
      "iteration 106 , loss: 630.1702270507812\n",
      "iteration 107 , loss: 597.6936645507812\n",
      "iteration 108 , loss: 566.9816284179688\n",
      "iteration 109 , loss: 537.9520874023438\n",
      "iteration 110 , loss: 510.44879150390625\n",
      "iteration 111 , loss: 484.43182373046875\n",
      "iteration 112 , loss: 459.7890625\n",
      "iteration 113 , loss: 436.4613342285156\n",
      "iteration 114 , loss: 414.3766174316406\n",
      "iteration 115 , loss: 393.4520568847656\n",
      "iteration 116 , loss: 373.6370849609375\n",
      "iteration 117 , loss: 354.875244140625\n",
      "iteration 118 , loss: 337.09527587890625\n",
      "iteration 119 , loss: 320.23919677734375\n",
      "iteration 120 , loss: 304.267333984375\n",
      "iteration 121 , loss: 289.126953125\n",
      "iteration 122 , loss: 274.7655334472656\n",
      "iteration 123 , loss: 261.17413330078125\n",
      "iteration 124 , loss: 248.2931365966797\n",
      "iteration 125 , loss: 236.07159423828125\n",
      "iteration 126 , loss: 224.4781494140625\n",
      "iteration 127 , loss: 213.47850036621094\n",
      "iteration 128 , loss: 203.03997802734375\n",
      "iteration 129 , loss: 193.1321258544922\n",
      "iteration 130 , loss: 183.7325897216797\n",
      "iteration 131 , loss: 174.80239868164062\n",
      "iteration 132 , loss: 166.3319091796875\n",
      "iteration 133 , loss: 158.28623962402344\n",
      "iteration 134 , loss: 150.6434326171875\n",
      "iteration 135 , loss: 143.38070678710938\n",
      "iteration 136 , loss: 136.48521423339844\n",
      "iteration 137 , loss: 129.927734375\n",
      "iteration 138 , loss: 123.70065307617188\n",
      "iteration 139 , loss: 117.78797149658203\n",
      "iteration 140 , loss: 112.16587829589844\n",
      "iteration 141 , loss: 106.81744384765625\n",
      "iteration 142 , loss: 101.73536682128906\n",
      "iteration 143 , loss: 96.90176391601562\n",
      "iteration 144 , loss: 92.30489349365234\n",
      "iteration 145 , loss: 87.93409729003906\n",
      "iteration 146 , loss: 83.77751922607422\n",
      "iteration 147 , loss: 79.82367706298828\n",
      "iteration 148 , loss: 76.06570434570312\n",
      "iteration 149 , loss: 72.48759460449219\n",
      "iteration 150 , loss: 69.08336639404297\n",
      "iteration 151 , loss: 65.84597778320312\n",
      "iteration 152 , loss: 62.763301849365234\n",
      "iteration 153 , loss: 59.82929229736328\n",
      "iteration 154 , loss: 57.03731918334961\n",
      "iteration 155 , loss: 54.381126403808594\n",
      "iteration 156 , loss: 51.851043701171875\n",
      "iteration 157 , loss: 49.444461822509766\n",
      "iteration 158 , loss: 47.150760650634766\n",
      "iteration 159 , loss: 44.964683532714844\n",
      "iteration 160 , loss: 42.88655471801758\n",
      "iteration 161 , loss: 40.90488052368164\n",
      "iteration 162 , loss: 39.01667785644531\n",
      "iteration 163 , loss: 37.21957015991211\n",
      "iteration 164 , loss: 35.506500244140625\n",
      "iteration 165 , loss: 33.876861572265625\n",
      "iteration 166 , loss: 32.322425842285156\n",
      "iteration 167 , loss: 30.840404510498047\n",
      "iteration 168 , loss: 29.429508209228516\n",
      "iteration 169 , loss: 28.086109161376953\n",
      "iteration 170 , loss: 26.804250717163086\n",
      "iteration 171 , loss: 25.5830078125\n",
      "iteration 172 , loss: 24.41803741455078\n",
      "iteration 173 , loss: 23.308897018432617\n",
      "iteration 174 , loss: 22.249862670898438\n",
      "iteration 175 , loss: 21.241863250732422\n",
      "iteration 176 , loss: 20.279870986938477\n",
      "iteration 177 , loss: 19.36227035522461\n",
      "iteration 178 , loss: 18.487491607666016\n",
      "iteration 179 , loss: 17.652469635009766\n",
      "iteration 180 , loss: 16.858003616333008\n",
      "iteration 181 , loss: 16.100194931030273\n",
      "iteration 182 , loss: 15.376043319702148\n",
      "iteration 183 , loss: 14.685101509094238\n",
      "iteration 184 , loss: 14.025890350341797\n",
      "iteration 185 , loss: 13.397366523742676\n",
      "iteration 186 , loss: 12.797332763671875\n",
      "iteration 187 , loss: 12.225074768066406\n",
      "iteration 188 , loss: 11.67914867401123\n",
      "iteration 189 , loss: 11.157787322998047\n",
      "iteration 190 , loss: 10.660408973693848\n",
      "iteration 191 , loss: 10.185552597045898\n",
      "iteration 192 , loss: 9.732725143432617\n",
      "iteration 193 , loss: 9.299819946289062\n",
      "iteration 194 , loss: 8.887155532836914\n",
      "iteration 195 , loss: 8.49302864074707\n",
      "iteration 196 , loss: 8.11684513092041\n",
      "iteration 197 , loss: 7.757309913635254\n",
      "iteration 198 , loss: 7.41444730758667\n",
      "iteration 199 , loss: 7.086859703063965\n",
      "iteration 200 , loss: 6.77426290512085\n",
      "iteration 201 , loss: 6.475288391113281\n",
      "iteration 202 , loss: 6.190223693847656\n",
      "iteration 203 , loss: 5.91789436340332\n",
      "iteration 204 , loss: 5.657452583312988\n",
      "iteration 205 , loss: 5.4091291427612305\n",
      "iteration 206 , loss: 5.171689987182617\n",
      "iteration 207 , loss: 4.944700717926025\n",
      "iteration 208 , loss: 4.728596210479736\n",
      "iteration 209 , loss: 4.521725177764893\n",
      "iteration 210 , loss: 4.324002742767334\n",
      "iteration 211 , loss: 4.135071277618408\n",
      "iteration 212 , loss: 3.9544260501861572\n",
      "iteration 213 , loss: 3.781978130340576\n",
      "iteration 214 , loss: 3.617041826248169\n",
      "iteration 215 , loss: 3.4596030712127686\n",
      "iteration 216 , loss: 3.308976888656616\n",
      "iteration 217 , loss: 3.165329933166504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 218 , loss: 3.0276153087615967\n",
      "iteration 219 , loss: 2.896275281906128\n",
      "iteration 220 , loss: 2.7707016468048096\n",
      "iteration 221 , loss: 2.6505415439605713\n",
      "iteration 222 , loss: 2.535844326019287\n",
      "iteration 223 , loss: 2.4260377883911133\n",
      "iteration 224 , loss: 2.3211584091186523\n",
      "iteration 225 , loss: 2.2208757400512695\n",
      "iteration 226 , loss: 2.125004529953003\n",
      "iteration 227 , loss: 2.033344030380249\n",
      "iteration 228 , loss: 1.9456048011779785\n",
      "iteration 229 , loss: 1.861838936805725\n",
      "iteration 230 , loss: 1.7819498777389526\n",
      "iteration 231 , loss: 1.705227017402649\n",
      "iteration 232 , loss: 1.6319055557250977\n",
      "iteration 233 , loss: 1.5616639852523804\n",
      "iteration 234 , loss: 1.4946904182434082\n",
      "iteration 235 , loss: 1.4305580854415894\n",
      "iteration 236 , loss: 1.3692626953125\n",
      "iteration 237 , loss: 1.3106790781021118\n",
      "iteration 238 , loss: 1.254668951034546\n",
      "iteration 239 , loss: 1.2009577751159668\n",
      "iteration 240 , loss: 1.1496418714523315\n",
      "iteration 241 , loss: 1.1005030870437622\n",
      "iteration 242 , loss: 1.053478717803955\n",
      "iteration 243 , loss: 1.0085560083389282\n",
      "iteration 244 , loss: 0.9655780792236328\n",
      "iteration 245 , loss: 0.9245429039001465\n",
      "iteration 246 , loss: 0.8851790428161621\n",
      "iteration 247 , loss: 0.8473890423774719\n",
      "iteration 248 , loss: 0.8113535046577454\n",
      "iteration 249 , loss: 0.7768730521202087\n",
      "iteration 250 , loss: 0.7438965439796448\n",
      "iteration 251 , loss: 0.712319552898407\n",
      "iteration 252 , loss: 0.6821936964988708\n",
      "iteration 253 , loss: 0.6531481146812439\n",
      "iteration 254 , loss: 0.6254113912582397\n",
      "iteration 255 , loss: 0.5989657640457153\n",
      "iteration 256 , loss: 0.5736451148986816\n",
      "iteration 257 , loss: 0.5494151711463928\n",
      "iteration 258 , loss: 0.5261560678482056\n",
      "iteration 259 , loss: 0.5039294958114624\n",
      "iteration 260 , loss: 0.4826526939868927\n",
      "iteration 261 , loss: 0.46227845549583435\n",
      "iteration 262 , loss: 0.44273561239242554\n",
      "iteration 263 , loss: 0.42415186762809753\n",
      "iteration 264 , loss: 0.40629807114601135\n",
      "iteration 265 , loss: 0.38913479447364807\n",
      "iteration 266 , loss: 0.3727452754974365\n",
      "iteration 267 , loss: 0.35704028606414795\n",
      "iteration 268 , loss: 0.34205010533332825\n",
      "iteration 269 , loss: 0.327658087015152\n",
      "iteration 270 , loss: 0.3138968348503113\n",
      "iteration 271 , loss: 0.3006812632083893\n",
      "iteration 272 , loss: 0.28807953000068665\n",
      "iteration 273 , loss: 0.27594172954559326\n",
      "iteration 274 , loss: 0.26438185572624207\n",
      "iteration 275 , loss: 0.25336986780166626\n",
      "iteration 276 , loss: 0.24275508522987366\n",
      "iteration 277 , loss: 0.23255376517772675\n",
      "iteration 278 , loss: 0.22279474139213562\n",
      "iteration 279 , loss: 0.213511660695076\n",
      "iteration 280 , loss: 0.20459489524364471\n",
      "iteration 281 , loss: 0.19604338705539703\n",
      "iteration 282 , loss: 0.18786326050758362\n",
      "iteration 283 , loss: 0.17999358475208282\n",
      "iteration 284 , loss: 0.1724683791399002\n",
      "iteration 285 , loss: 0.16527126729488373\n",
      "iteration 286 , loss: 0.15837717056274414\n",
      "iteration 287 , loss: 0.1518266797065735\n",
      "iteration 288 , loss: 0.14548230171203613\n",
      "iteration 289 , loss: 0.13942532241344452\n",
      "iteration 290 , loss: 0.13364623486995697\n",
      "iteration 291 , loss: 0.12804260849952698\n",
      "iteration 292 , loss: 0.1227259486913681\n",
      "iteration 293 , loss: 0.11761946976184845\n",
      "iteration 294 , loss: 0.11276045441627502\n",
      "iteration 295 , loss: 0.10805942863225937\n",
      "iteration 296 , loss: 0.10357607156038284\n",
      "iteration 297 , loss: 0.0992729663848877\n",
      "iteration 298 , loss: 0.09514369070529938\n",
      "iteration 299 , loss: 0.09117242693901062\n",
      "iteration 300 , loss: 0.08740827441215515\n",
      "iteration 301 , loss: 0.08379964530467987\n",
      "iteration 302 , loss: 0.08032017201185226\n",
      "iteration 303 , loss: 0.07701046019792557\n",
      "iteration 304 , loss: 0.07382301986217499\n",
      "iteration 305 , loss: 0.07076798379421234\n",
      "iteration 306 , loss: 0.06784466654062271\n",
      "iteration 307 , loss: 0.06504364311695099\n",
      "iteration 308 , loss: 0.06236148998141289\n",
      "iteration 309 , loss: 0.05978456884622574\n",
      "iteration 310 , loss: 0.0573207251727581\n",
      "iteration 311 , loss: 0.05495995283126831\n",
      "iteration 312 , loss: 0.05269820988178253\n",
      "iteration 313 , loss: 0.05052715167403221\n",
      "iteration 314 , loss: 0.04845254123210907\n",
      "iteration 315 , loss: 0.046454306691884995\n",
      "iteration 316 , loss: 0.044533465057611465\n",
      "iteration 317 , loss: 0.04270832613110542\n",
      "iteration 318 , loss: 0.04095311835408211\n",
      "iteration 319 , loss: 0.03926926478743553\n",
      "iteration 320 , loss: 0.037664901465177536\n",
      "iteration 321 , loss: 0.03611934185028076\n",
      "iteration 322 , loss: 0.03463059291243553\n",
      "iteration 323 , loss: 0.03322442248463631\n",
      "iteration 324 , loss: 0.031865548342466354\n",
      "iteration 325 , loss: 0.030561208724975586\n",
      "iteration 326 , loss: 0.029316406697034836\n",
      "iteration 327 , loss: 0.028119966387748718\n",
      "iteration 328 , loss: 0.026987873017787933\n",
      "iteration 329 , loss: 0.025874055922031403\n",
      "iteration 330 , loss: 0.02482551336288452\n",
      "iteration 331 , loss: 0.02381003648042679\n",
      "iteration 332 , loss: 0.02284468337893486\n",
      "iteration 333 , loss: 0.021918542683124542\n",
      "iteration 334 , loss: 0.02102530188858509\n",
      "iteration 335 , loss: 0.020166153088212013\n",
      "iteration 336 , loss: 0.019353941082954407\n",
      "iteration 337 , loss: 0.018569163978099823\n",
      "iteration 338 , loss: 0.017819857224822044\n",
      "iteration 339 , loss: 0.01709662936627865\n",
      "iteration 340 , loss: 0.01641211472451687\n",
      "iteration 341 , loss: 0.015746621415019035\n",
      "iteration 342 , loss: 0.01510913111269474\n",
      "iteration 343 , loss: 0.014502846635878086\n",
      "iteration 344 , loss: 0.013920634053647518\n",
      "iteration 345 , loss: 0.013355170376598835\n",
      "iteration 346 , loss: 0.012829501181840897\n",
      "iteration 347 , loss: 0.012311192229390144\n",
      "iteration 348 , loss: 0.01182305347174406\n",
      "iteration 349 , loss: 0.011353169567883015\n",
      "iteration 350 , loss: 0.010896175168454647\n",
      "iteration 351 , loss: 0.010469886474311352\n",
      "iteration 352 , loss: 0.01005590334534645\n",
      "iteration 353 , loss: 0.009655962698161602\n",
      "iteration 354 , loss: 0.009268583729863167\n",
      "iteration 355 , loss: 0.00890639703720808\n",
      "iteration 356 , loss: 0.008555497042834759\n",
      "iteration 357 , loss: 0.008224204182624817\n",
      "iteration 358 , loss: 0.007896446622908115\n",
      "iteration 359 , loss: 0.0075896624475717545\n",
      "iteration 360 , loss: 0.007293432950973511\n",
      "iteration 361 , loss: 0.007006996776908636\n",
      "iteration 362 , loss: 0.006742600351572037\n",
      "iteration 363 , loss: 0.00647685956209898\n",
      "iteration 364 , loss: 0.006227683275938034\n",
      "iteration 365 , loss: 0.005990838631987572\n",
      "iteration 366 , loss: 0.005757171660661697\n",
      "iteration 367 , loss: 0.005534202326089144\n",
      "iteration 368 , loss: 0.005324521102011204\n",
      "iteration 369 , loss: 0.005120818968862295\n",
      "iteration 370 , loss: 0.0049271914176642895\n",
      "iteration 371 , loss: 0.00474380049854517\n",
      "iteration 372 , loss: 0.004560988396406174\n",
      "iteration 373 , loss: 0.004389127250760794\n",
      "iteration 374 , loss: 0.004224169068038464\n",
      "iteration 375 , loss: 0.0040682656690478325\n",
      "iteration 376 , loss: 0.003919799812138081\n",
      "iteration 377 , loss: 0.003774665528908372\n",
      "iteration 378 , loss: 0.0036382137332111597\n",
      "iteration 379 , loss: 0.0035031698644161224\n",
      "iteration 380 , loss: 0.003374435706064105\n",
      "iteration 381 , loss: 0.003252588678151369\n",
      "iteration 382 , loss: 0.003135266713798046\n",
      "iteration 383 , loss: 0.0030243550427258015\n",
      "iteration 384 , loss: 0.0029131569899618626\n",
      "iteration 385 , loss: 0.002812139457091689\n",
      "iteration 386 , loss: 0.0027119938749819994\n",
      "iteration 387 , loss: 0.002616169163957238\n",
      "iteration 388 , loss: 0.0025242778938263655\n",
      "iteration 389 , loss: 0.0024360327515751123\n",
      "iteration 390 , loss: 0.0023518726229667664\n",
      "iteration 391 , loss: 0.002271722536534071\n",
      "iteration 392 , loss: 0.002193755004554987\n",
      "iteration 393 , loss: 0.0021152030676603317\n",
      "iteration 394 , loss: 0.0020445643458515406\n",
      "iteration 395 , loss: 0.0019729826599359512\n",
      "iteration 396 , loss: 0.0019041771301999688\n",
      "iteration 397 , loss: 0.0018421290442347527\n",
      "iteration 398 , loss: 0.0017818280030041933\n",
      "iteration 399 , loss: 0.001723272493109107\n",
      "iteration 400 , loss: 0.0016648996388539672\n",
      "iteration 401 , loss: 0.0016116172773763537\n",
      "iteration 402 , loss: 0.0015596795128658414\n",
      "iteration 403 , loss: 0.0015099986921995878\n",
      "iteration 404 , loss: 0.001460178755223751\n",
      "iteration 405 , loss: 0.0014140100684016943\n",
      "iteration 406 , loss: 0.0013692752690985799\n",
      "iteration 407 , loss: 0.0013261624844744802\n",
      "iteration 408 , loss: 0.0012864405289292336\n",
      "iteration 409 , loss: 0.0012452432420104742\n",
      "iteration 410 , loss: 0.0012075355043634772\n",
      "iteration 411 , loss: 0.0011689483653753996\n",
      "iteration 412 , loss: 0.0011356299510225654\n",
      "iteration 413 , loss: 0.0010995682096108794\n",
      "iteration 414 , loss: 0.0010667812312021852\n",
      "iteration 415 , loss: 0.0010341437300667167\n",
      "iteration 416 , loss: 0.0010037170723080635\n",
      "iteration 417 , loss: 0.0009745542192831635\n",
      "iteration 418 , loss: 0.000946183514315635\n",
      "iteration 419 , loss: 0.000916810822673142\n",
      "iteration 420 , loss: 0.0008914383361116052\n",
      "iteration 421 , loss: 0.0008658815640956163\n",
      "iteration 422 , loss: 0.0008421281818300486\n",
      "iteration 423 , loss: 0.0008165999315679073\n",
      "iteration 424 , loss: 0.0007923998055048287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 425 , loss: 0.0007699169800616801\n",
      "iteration 426 , loss: 0.0007504752138629556\n",
      "iteration 427 , loss: 0.000730007654055953\n",
      "iteration 428 , loss: 0.0007089725695550442\n",
      "iteration 429 , loss: 0.0006897827843204141\n",
      "iteration 430 , loss: 0.0006708703003823757\n",
      "iteration 431 , loss: 0.0006533346604555845\n",
      "iteration 432 , loss: 0.000634866941254586\n",
      "iteration 433 , loss: 0.0006191569264046848\n",
      "iteration 434 , loss: 0.0006030915537849069\n",
      "iteration 435 , loss: 0.0005871151806786656\n",
      "iteration 436 , loss: 0.0005725636729039252\n",
      "iteration 437 , loss: 0.000557442253921181\n",
      "iteration 438 , loss: 0.0005421680398285389\n",
      "iteration 439 , loss: 0.0005287661915645003\n",
      "iteration 440 , loss: 0.0005143327871337533\n",
      "iteration 441 , loss: 0.0005019673844799399\n",
      "iteration 442 , loss: 0.000488889345433563\n",
      "iteration 443 , loss: 0.00047647891915403306\n",
      "iteration 444 , loss: 0.000464641023427248\n",
      "iteration 445 , loss: 0.00045404996490105987\n",
      "iteration 446 , loss: 0.0004423176287673414\n",
      "iteration 447 , loss: 0.00043155139428563416\n",
      "iteration 448 , loss: 0.0004207512829452753\n",
      "iteration 449 , loss: 0.0004119574441574514\n",
      "iteration 450 , loss: 0.00040226549026556313\n",
      "iteration 451 , loss: 0.00039178936276584864\n",
      "iteration 452 , loss: 0.00038270867662504315\n",
      "iteration 453 , loss: 0.00037407310446724296\n",
      "iteration 454 , loss: 0.000365410785889253\n",
      "iteration 455 , loss: 0.00035666822805069387\n",
      "iteration 456 , loss: 0.00034856999991461635\n",
      "iteration 457 , loss: 0.0003407043404877186\n",
      "iteration 458 , loss: 0.00033326903940178454\n",
      "iteration 459 , loss: 0.00032653758535161614\n",
      "iteration 460 , loss: 0.0003182713990099728\n",
      "iteration 461 , loss: 0.0003108831588178873\n",
      "iteration 462 , loss: 0.000304177199723199\n",
      "iteration 463 , loss: 0.00029854863532818854\n",
      "iteration 464 , loss: 0.0002924371510744095\n",
      "iteration 465 , loss: 0.00028545671375468373\n",
      "iteration 466 , loss: 0.00027978868456557393\n",
      "iteration 467 , loss: 0.00027360583771951497\n",
      "iteration 468 , loss: 0.00026809528935700655\n",
      "iteration 469 , loss: 0.00026224073371849954\n",
      "iteration 470 , loss: 0.000256694940617308\n",
      "iteration 471 , loss: 0.0002513853833079338\n",
      "iteration 472 , loss: 0.00024574727285653353\n",
      "iteration 473 , loss: 0.00024179370666388422\n",
      "iteration 474 , loss: 0.00023663570755161345\n",
      "iteration 475 , loss: 0.00023154160589911044\n",
      "iteration 476 , loss: 0.0002269367250846699\n",
      "iteration 477 , loss: 0.00022264679137151688\n",
      "iteration 478 , loss: 0.00021732000459451228\n",
      "iteration 479 , loss: 0.00021341865067370236\n",
      "iteration 480 , loss: 0.00020921614486724138\n",
      "iteration 481 , loss: 0.00020502212282735854\n",
      "iteration 482 , loss: 0.00020168929768260568\n",
      "iteration 483 , loss: 0.0001980927336262539\n",
      "iteration 484 , loss: 0.00019406649516895413\n",
      "iteration 485 , loss: 0.00019032301497645676\n",
      "iteration 486 , loss: 0.00018642250506673008\n",
      "iteration 487 , loss: 0.00018332213221583515\n",
      "iteration 488 , loss: 0.0001802626356948167\n",
      "iteration 489 , loss: 0.00017633478273637593\n",
      "iteration 490 , loss: 0.0001732586679281667\n",
      "iteration 491 , loss: 0.00017022588872350752\n",
      "iteration 492 , loss: 0.0001673062506597489\n",
      "iteration 493 , loss: 0.00016477085591759533\n",
      "iteration 494 , loss: 0.00016134348697960377\n",
      "iteration 495 , loss: 0.00015828234609216452\n",
      "iteration 496 , loss: 0.0001559038646519184\n",
      "iteration 497 , loss: 0.00015320148668251932\n",
      "iteration 498 , loss: 0.00015040407015476376\n",
      "iteration 499 , loss: 0.000147461163578555\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    我们可以通过继承 torch.autograd.Function 并实现前向和后向传播函数来实现自定义的 autograd 函数\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        在前向传播中，我们接收包含输入的 Tensor 并返回包含输出的 Tensor \n",
    "        ctx是一个上下文对象，可用于存储信息以进行反向计算\n",
    "        您可以使用 ctx.save_for_backward 方法缓存任意对象以在反向传播中使用\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)  # if x < 0 : x = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        在反向传播中，我们接收含有损失关于输出的梯度的 Tensor，\n",
    "        并且我们需要计算损失关于输入的梯度。\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # 取消注释以在 GPU 上运行\n",
    "\n",
    "# N是数据样本数，D_in是输入层维度\n",
    "# H是隐藏层维度，D_out是输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机输入和输出数据\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6  # 学习率\n",
    "iteration_count = 500  # 迭代次数\n",
    "for t in range(iteration_count):\n",
    "    # 为应用自定义的函数，取假名 relu\n",
    "    relu = MyReLU.apply\n",
    "    \n",
    "    # 前向传播：计算预测值y，使用自定义函数\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    # 计算并打印损失\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(\"iteration\", t, \",\", \"loss:\", loss.item())  # t是循环变量\n",
    "    \n",
    "    # 使用 autograd 计算反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 使用梯度下降更新权重\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # 更新权重后手动清零梯度\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow: 静态图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch autograd 看起来很像 TensorFlow：在两个框架中我们定义了一个计算图，并使用自动微分来计算梯度。两者之间最大的区别是TensorFlow 的计算图是静态的，PyTorch 使用动态计算图。\n",
    "\n",
    "在TensorFlow中，我们定义计算图一次，然后一遍又一遍地执行相同的图，可能将不同的输入数据提供给图。在 PyTorch 中，每个前向传播定义了一个新的计算图。\n",
    "\n",
    "静态图很好，因为你可以预先优化图；例如，框架可能决定融合某些图操作以提高效率，或者提出一种策略，用于在多个 GPU 或许多机器上分布图。如果您反复使用相同的图表，那么这个代价可能高昂的前期优化可以分摊，因为相同的图表会反复重新运行。\n",
    "\n",
    "静态和动态图的一个不同之处在于控制流程。对于某些模型，我们可能希望对每个数据点执行不同的计算；例如，可以针对每个数据点针对不同数量的时间步长展开循环网络；这种展开可以作为循环实现。使用静态图形，循环结构需要是图的一部分；因此，TensorFlow 提供了诸如 tf.scan 之类的运算符，用于将循环嵌入到图中。使用动态图情况更简单：因为我们为每个示例动态构建图，我们可以使用常规命令流程控制来执行每个输入不同的计算。\n",
    "\n",
    "与上面的 PyTorch autograd 示例相比，这里我们使用 TensorFlow 来拟合一个简单的双层网："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32411678.0\n",
      "1 31821254.0\n",
      "2 36366470.0\n",
      "3 39483470.0\n",
      "4 35045570.0\n",
      "5 23726128.0\n",
      "6 12366000.0\n",
      "7 5703575.0\n",
      "8 2783157.0\n",
      "9 1613963.5\n",
      "10 1110176.9\n",
      "11 852431.56\n",
      "12 693252.25\n",
      "13 579938.25\n",
      "14 492466.06\n",
      "15 422042.84\n",
      "16 364013.9\n",
      "17 315578.78\n",
      "18 274822.3\n",
      "19 240324.23\n",
      "20 210928.53\n",
      "21 185753.53\n",
      "22 164100.47\n",
      "23 145461.8\n",
      "24 129312.5\n",
      "25 115258.86\n",
      "26 102981.18\n",
      "27 92223.266\n",
      "28 82776.18\n",
      "29 74470.74\n",
      "30 67121.17\n",
      "31 60605.125\n",
      "32 54817.645\n",
      "33 49662.43\n",
      "34 45060.53\n",
      "35 40944.977\n",
      "36 37255.773\n",
      "37 33943.062\n",
      "38 30963.555\n",
      "39 28278.371\n",
      "40 25854.418\n",
      "41 23662.754\n",
      "42 21678.625\n",
      "43 19880.09\n",
      "44 18249.746\n",
      "45 16766.582\n",
      "46 15417.279\n",
      "47 14189.137\n",
      "48 13068.486\n",
      "49 12044.978\n",
      "50 11108.893\n",
      "51 10252.022\n",
      "52 9467.367\n",
      "53 8747.801\n",
      "54 8087.2427\n",
      "55 7480.502\n",
      "56 6923.086\n",
      "57 6410.367\n",
      "58 5938.4224\n",
      "59 5503.5776\n",
      "60 5102.9863\n",
      "61 4733.6255\n",
      "62 4392.949\n",
      "63 4078.127\n",
      "64 3787.3838\n",
      "65 3518.812\n",
      "66 3270.28\n",
      "67 3040.4014\n",
      "68 2827.639\n",
      "69 2630.7683\n",
      "70 2448.345\n",
      "71 2279.3318\n",
      "72 2122.7354\n",
      "73 1977.3933\n",
      "74 1842.5115\n",
      "75 1717.3005\n",
      "76 1600.9949\n",
      "77 1492.9688\n",
      "78 1392.6289\n",
      "79 1299.3259\n",
      "80 1212.5847\n",
      "81 1131.9048\n",
      "82 1056.9021\n",
      "83 987.0773\n",
      "84 922.0475\n",
      "85 861.5944\n",
      "86 805.3194\n",
      "87 752.84265\n",
      "88 703.9086\n",
      "89 658.2787\n",
      "90 615.74695\n",
      "91 576.0624\n",
      "92 539.02\n",
      "93 504.458\n",
      "94 472.2257\n",
      "95 442.1042\n",
      "96 413.9624\n",
      "97 387.6734\n",
      "98 363.11823\n",
      "99 340.16632\n",
      "100 318.70737\n",
      "101 298.65192\n",
      "102 279.9049\n",
      "103 262.3644\n",
      "104 245.95697\n",
      "105 230.60933\n",
      "106 216.2637\n",
      "107 202.82101\n",
      "108 190.24294\n",
      "109 178.46628\n",
      "110 167.43884\n",
      "111 157.11041\n",
      "112 147.43951\n",
      "113 138.38278\n",
      "114 129.8924\n",
      "115 121.93503\n",
      "116 114.478874\n",
      "117 107.49565\n",
      "118 100.946945\n",
      "119 94.80635\n",
      "120 89.04825\n",
      "121 83.64732\n",
      "122 78.5835\n",
      "123 73.832504\n",
      "124 69.374756\n",
      "125 65.19413\n",
      "126 61.271626\n",
      "127 57.588642\n",
      "128 54.13411\n",
      "129 50.889263\n",
      "130 47.84371\n",
      "131 44.98346\n",
      "132 42.29966\n",
      "133 39.77864\n",
      "134 37.410778\n",
      "135 35.18639\n",
      "136 33.097637\n",
      "137 31.135275\n",
      "138 29.290749\n",
      "139 27.557528\n",
      "140 25.929867\n",
      "141 24.399754\n",
      "142 22.960493\n",
      "143 21.608948\n",
      "144 20.338223\n",
      "145 19.143269\n",
      "146 18.019117\n",
      "147 16.962486\n",
      "148 15.968713\n",
      "149 15.034357\n",
      "150 14.155959\n",
      "151 13.329309\n",
      "152 12.551966\n",
      "153 11.820059\n",
      "154 11.131878\n",
      "155 10.484196\n",
      "156 9.87552\n",
      "157 9.301624\n",
      "158 8.761914\n",
      "159 8.25457\n",
      "160 7.776636\n",
      "161 7.326466\n",
      "162 6.9030538\n",
      "163 6.5042443\n",
      "164 6.129032\n",
      "165 5.7755833\n",
      "166 5.4427395\n",
      "167 5.1295943\n",
      "168 4.8346567\n",
      "169 4.5571046\n",
      "170 4.2951565\n",
      "171 4.048965\n",
      "172 3.816522\n",
      "173 3.597876\n",
      "174 3.391901\n",
      "175 3.1975741\n",
      "176 3.0147963\n",
      "177 2.8428137\n",
      "178 2.68034\n",
      "179 2.5275605\n",
      "180 2.3834043\n",
      "181 2.2475219\n",
      "182 2.1194768\n",
      "183 1.9990392\n",
      "184 1.8852733\n",
      "185 1.7781284\n",
      "186 1.6771301\n",
      "187 1.5818636\n",
      "188 1.4919796\n",
      "189 1.4074106\n",
      "190 1.3277191\n",
      "191 1.2525771\n",
      "192 1.18167\n",
      "193 1.114784\n",
      "194 1.0518317\n",
      "195 0.9924525\n",
      "196 0.9363988\n",
      "197 0.8834938\n",
      "198 0.8336667\n",
      "199 0.7866651\n",
      "200 0.74233145\n",
      "201 0.7004877\n",
      "202 0.66105497\n",
      "203 0.6238005\n",
      "204 0.58885837\n",
      "205 0.5556495\n",
      "206 0.5244354\n",
      "207 0.49502102\n",
      "208 0.46723393\n",
      "209 0.4409688\n",
      "210 0.41628745\n",
      "211 0.39298284\n",
      "212 0.37092426\n",
      "213 0.35018533\n",
      "214 0.33058494\n",
      "215 0.31210527\n",
      "216 0.2946753\n",
      "217 0.27817798\n",
      "218 0.26260045\n",
      "219 0.24797909\n",
      "220 0.23417526\n",
      "221 0.2211108\n",
      "222 0.2087347\n",
      "223 0.19710603\n",
      "224 0.18612553\n",
      "225 0.17572795\n",
      "226 0.1659604\n",
      "227 0.15669598\n",
      "228 0.14797658\n",
      "229 0.13975702\n",
      "230 0.13194005\n",
      "231 0.12463072\n",
      "232 0.11767056\n",
      "233 0.11118135\n",
      "234 0.104980275\n",
      "235 0.09913859\n",
      "236 0.09365004\n",
      "237 0.0884909\n",
      "238 0.083583154\n",
      "239 0.07895233\n",
      "240 0.074575216\n",
      "241 0.0704333\n",
      "242 0.06653875\n",
      "243 0.062860705\n",
      "244 0.0593673\n",
      "245 0.056063898\n",
      "246 0.052977916\n",
      "247 0.0500744\n",
      "248 0.04728043\n",
      "249 0.04469499\n",
      "250 0.042233933\n",
      "251 0.03989884\n",
      "252 0.037711255\n",
      "253 0.035643302\n",
      "254 0.033665795\n",
      "255 0.03181907\n",
      "256 0.030061333\n",
      "257 0.028416596\n",
      "258 0.026867416\n",
      "259 0.02538061\n",
      "260 0.023973314\n",
      "261 0.022673372\n",
      "262 0.021423122\n",
      "263 0.020247491\n",
      "264 0.019160723\n",
      "265 0.018110165\n",
      "266 0.017133337\n",
      "267 0.016189972\n",
      "268 0.015309034\n",
      "269 0.014470149\n",
      "270 0.013684435\n",
      "271 0.0129455915\n",
      "272 0.012244088\n",
      "273 0.011575846\n",
      "274 0.0109579265\n",
      "275 0.010367088\n",
      "276 0.009815203\n",
      "277 0.009288533\n",
      "278 0.008790142\n",
      "279 0.008321768\n",
      "280 0.007874586\n",
      "281 0.0074558128\n",
      "282 0.0070642093\n",
      "283 0.0066881105\n",
      "284 0.0063392604\n",
      "285 0.005995458\n",
      "286 0.005683141\n",
      "287 0.0053807925\n",
      "288 0.0051018083\n",
      "289 0.0048399447\n",
      "290 0.0045888713\n",
      "291 0.004354422\n",
      "292 0.004132039\n",
      "293 0.003915292\n",
      "294 0.0037175957\n",
      "295 0.003529951\n",
      "296 0.0033550584\n",
      "297 0.0031868462\n",
      "298 0.0030263742\n",
      "299 0.0028748002\n",
      "300 0.0027380232\n",
      "301 0.0026055092\n",
      "302 0.0024776352\n",
      "303 0.0023596494\n",
      "304 0.0022454297\n",
      "305 0.0021346707\n",
      "306 0.0020297584\n",
      "307 0.0019378618\n",
      "308 0.0018449033\n",
      "309 0.0017575294\n",
      "310 0.0016762788\n",
      "311 0.0015995635\n",
      "312 0.0015237825\n",
      "313 0.0014568804\n",
      "314 0.0013879907\n",
      "315 0.0013271888\n",
      "316 0.0012683128\n",
      "317 0.0012136311\n",
      "318 0.0011574787\n",
      "319 0.0011080536\n",
      "320 0.0010591709\n",
      "321 0.0010145796\n",
      "322 0.00097106537\n",
      "323 0.0009298682\n",
      "324 0.0008896176\n",
      "325 0.00085221755\n",
      "326 0.0008174714\n",
      "327 0.0007831696\n",
      "328 0.00075312715\n",
      "329 0.00072486047\n",
      "330 0.00069389417\n",
      "331 0.00066537526\n",
      "332 0.00064109056\n",
      "333 0.0006169103\n",
      "334 0.00059199444\n",
      "335 0.000569731\n",
      "336 0.0005493504\n",
      "337 0.00052932464\n",
      "338 0.00050957955\n",
      "339 0.00049044174\n",
      "340 0.00047378617\n",
      "341 0.0004563915\n",
      "342 0.0004397879\n",
      "343 0.00042437357\n",
      "344 0.00040956275\n",
      "345 0.00039525423\n",
      "346 0.00038093154\n",
      "347 0.0003673528\n",
      "348 0.00035596275\n",
      "349 0.00034368952\n",
      "350 0.0003320285\n",
      "351 0.00032103085\n",
      "352 0.0003101752\n",
      "353 0.00030011998\n",
      "354 0.0002901902\n",
      "355 0.00028160462\n",
      "356 0.00027359533\n",
      "357 0.0002646306\n",
      "358 0.00025697434\n",
      "359 0.00024853548\n",
      "360 0.00024159461\n",
      "361 0.00023324293\n",
      "362 0.00022624803\n",
      "363 0.00021992213\n",
      "364 0.000213544\n",
      "365 0.0002067418\n",
      "366 0.00020080012\n",
      "367 0.00019591217\n",
      "368 0.00018965997\n",
      "369 0.00018406\n",
      "370 0.00017827723\n",
      "371 0.00017370103\n",
      "372 0.00016853295\n",
      "373 0.00016407642\n",
      "374 0.00015954484\n",
      "375 0.00015544714\n",
      "376 0.00015104\n",
      "377 0.00014688898\n",
      "378 0.0001433178\n",
      "379 0.00013942829\n",
      "380 0.00013607246\n",
      "381 0.00013233433\n",
      "382 0.00012917534\n",
      "383 0.00012568559\n",
      "384 0.00012288649\n",
      "385 0.00012009477\n",
      "386 0.00011647916\n",
      "387 0.00011375455\n",
      "388 0.000110855544\n",
      "389 0.0001083023\n",
      "390 0.00010601696\n",
      "391 0.000103695726\n",
      "392 0.000101227306\n",
      "393 9.859708e-05\n",
      "394 9.667936e-05\n",
      "395 9.423686e-05\n",
      "396 9.25425e-05\n",
      "397 9.013282e-05\n",
      "398 8.828774e-05\n",
      "399 8.624816e-05\n",
      "400 8.444323e-05\n",
      "401 8.255856e-05\n",
      "402 8.067067e-05\n",
      "403 7.8885816e-05\n",
      "404 7.7611956e-05\n",
      "405 7.549628e-05\n",
      "406 7.4185446e-05\n",
      "407 7.273458e-05\n",
      "408 7.124136e-05\n",
      "409 6.995049e-05\n",
      "410 6.836249e-05\n",
      "411 6.720032e-05\n",
      "412 6.573117e-05\n",
      "413 6.438958e-05\n",
      "414 6.33031e-05\n",
      "415 6.216437e-05\n",
      "416 6.051997e-05\n",
      "417 5.9737104e-05\n",
      "418 5.8504353e-05\n",
      "419 5.7483296e-05\n",
      "420 5.659096e-05\n",
      "421 5.552519e-05\n",
      "422 5.4457967e-05\n",
      "423 5.34575e-05\n",
      "424 5.259767e-05\n",
      "425 5.1227536e-05\n",
      "426 5.0411287e-05\n",
      "427 4.946954e-05\n",
      "428 4.8786886e-05\n",
      "429 4.7813508e-05\n",
      "430 4.6850666e-05\n",
      "431 4.5998328e-05\n",
      "432 4.533559e-05\n",
      "433 4.4246608e-05\n",
      "434 4.3734115e-05\n",
      "435 4.2957392e-05\n",
      "436 4.2273605e-05\n",
      "437 4.164737e-05\n",
      "438 4.103539e-05\n",
      "439 4.024439e-05\n",
      "440 3.9687024e-05\n",
      "441 3.909941e-05\n",
      "442 3.8394483e-05\n",
      "443 3.778089e-05\n",
      "444 3.7150523e-05\n",
      "445 3.6468613e-05\n",
      "446 3.6047444e-05\n",
      "447 3.5462006e-05\n",
      "448 3.5045756e-05\n",
      "449 3.4451532e-05\n",
      "450 3.392857e-05\n",
      "451 3.348723e-05\n",
      "452 3.3079355e-05\n",
      "453 3.2642416e-05\n",
      "454 3.2147844e-05\n",
      "455 3.1623083e-05\n",
      "456 3.1230113e-05\n",
      "457 3.0650328e-05\n",
      "458 3.013448e-05\n",
      "459 2.9809458e-05\n",
      "460 2.9672325e-05\n",
      "461 2.9182414e-05\n",
      "462 2.8589662e-05\n",
      "463 2.8262948e-05\n",
      "464 2.7888029e-05\n",
      "465 2.7599279e-05\n",
      "466 2.7109509e-05\n",
      "467 2.6632604e-05\n",
      "468 2.640675e-05\n",
      "469 2.6213711e-05\n",
      "470 2.6002304e-05\n",
      "471 2.5704936e-05\n",
      "472 2.5446057e-05\n",
      "473 2.5141006e-05\n",
      "474 2.4822613e-05\n",
      "475 2.4544674e-05\n",
      "476 2.4198267e-05\n",
      "477 2.4034056e-05\n",
      "478 2.364726e-05\n",
      "479 2.3457706e-05\n",
      "480 2.3034445e-05\n",
      "481 2.280344e-05\n",
      "482 2.257125e-05\n",
      "483 2.2364218e-05\n",
      "484 2.2032988e-05\n",
      "485 2.1913638e-05\n",
      "486 2.1660455e-05\n",
      "487 2.147864e-05\n",
      "488 2.1326628e-05\n",
      "489 2.1052441e-05\n",
      "490 2.081907e-05\n",
      "491 2.0638698e-05\n",
      "492 2.0430785e-05\n",
      "493 2.0177937e-05\n",
      "494 1.9875373e-05\n",
      "495 1.974287e-05\n",
      "496 1.9406241e-05\n",
      "497 1.9314703e-05\n",
      "498 1.9120012e-05\n",
      "499 1.8914645e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 首先设置计算图：\n",
    "\n",
    "# N是数据样本数，D_in是输入层维度\n",
    "# H是隐藏层维度，D_out是输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 为输入和目标数据创建占位符，在执行图时占位符会被填充数据\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "# 占位符 shape 没有包含数据样本数\n",
    "\n",
    "# 为权重创建变量并使用随机数据初始化\n",
    "# 一个 TensorFlow 变量在计算图执行的过程中保持其值。\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# 前向传播：使用 TensorFlow Tensor 计算预测值y\n",
    "# 注意这些代码并没有实际执行任何数值操作，它仅设置稍后将执行的计算图\n",
    "\n",
    "h = tf.matmul(x, w1)  # 矩阵乘法\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# 使用 TensorFlow Tensor 的运算计算损失\n",
    "loss = tf.reduce_sum((y - y_pred)**2.0)\n",
    "\n",
    "# 损失函数关于 w1 和 w2 的梯度\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# 使用梯度下降更新权重\n",
    "# 要实际更新权重，我们需要在执行计算图时评估 new_w1 和 new_w2\n",
    "# 请注意，在 TensorFlow 中，更新权重值的行为是计算图的一部分\n",
    "# 在 PyTorch 中，这发生在计算图之外\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# 现在已经构建了计算图，进入 TensorFlow session 以执行计算图\n",
    "with tf.Session() as sess:\n",
    "    # 运行图，初始化权重 w1 和 w2\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 创建包含输入 x 和目标 y 的实际数据的 numpy 数组\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    \n",
    "    iteration_count = 500  # 迭代次数\n",
    "    \n",
    "    # 每次循环将 x_value 绑定到 x，y_value 绑定到 y\n",
    "    # 每次循环计算损失与新权重\n",
    "    for t in range(iteration_count):\n",
    "        loss_value, _, _ = sess.run(\n",
    "            [loss, new_w1, new_w2], feed_dict={\n",
    "                x: x_value,\n",
    "                y: y_value\n",
    "            })\n",
    "        print(t,loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn 模块（neural network）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算图和 autograd 是一个非常强大的用于定义复杂的运算符并自动微分的范例；然而，对于大型神经网络，原始 autograd 可能有点太低级。\n",
    "\n",
    "在构建神经网络时，我们经常考虑将计算安排到层中，其中一些层具有可学习的参数，这些参数将在学习期间进行优化。\n",
    "\n",
    "在 TensorFlow 中，像 Keras，TensorFlow-Slim 和 TFLearn 这样的软件包提供了对构建神经网络有用的原始计算图的更高级别的抽象。\n",
    "\n",
    "在 PyTorch 中，nn 包服务于同样的目的。nn 包定义了一组模块，它们大致相当于神经网络层。模块接收输入 Tensor 并计算输出 Tensor，但也可以保持内部状态，例如包含可学习参数的 Tensor。nn 包还定义了一组在训练神经网络时常用的有用损失函数。\n",
    "\n",
    "在这个例子中，我们使用nn包来实现我们的双层网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 , loss: 649.6320190429688\n",
      "iteration 1 , loss: 602.88037109375\n",
      "iteration 2 , loss: 562.5580444335938\n",
      "iteration 3 , loss: 526.9771728515625\n",
      "iteration 4 , loss: 495.0279541015625\n",
      "iteration 5 , loss: 466.4129333496094\n",
      "iteration 6 , loss: 440.5115661621094\n",
      "iteration 7 , loss: 416.557861328125\n",
      "iteration 8 , loss: 394.315673828125\n",
      "iteration 9 , loss: 373.5847473144531\n",
      "iteration 10 , loss: 354.14697265625\n",
      "iteration 11 , loss: 335.86767578125\n",
      "iteration 12 , loss: 318.6815185546875\n",
      "iteration 13 , loss: 302.4425354003906\n",
      "iteration 14 , loss: 286.96038818359375\n",
      "iteration 15 , loss: 272.11285400390625\n",
      "iteration 16 , loss: 257.91082763671875\n",
      "iteration 17 , loss: 244.36141967773438\n",
      "iteration 18 , loss: 231.4207763671875\n",
      "iteration 19 , loss: 219.1044464111328\n",
      "iteration 20 , loss: 207.30130004882812\n",
      "iteration 21 , loss: 196.0321807861328\n",
      "iteration 22 , loss: 185.2052459716797\n",
      "iteration 23 , loss: 174.88754272460938\n",
      "iteration 24 , loss: 165.01783752441406\n",
      "iteration 25 , loss: 155.6190643310547\n",
      "iteration 26 , loss: 146.6458282470703\n",
      "iteration 27 , loss: 138.1441650390625\n",
      "iteration 28 , loss: 130.08436584472656\n",
      "iteration 29 , loss: 122.45802307128906\n",
      "iteration 30 , loss: 115.24796295166016\n",
      "iteration 31 , loss: 108.4255599975586\n",
      "iteration 32 , loss: 101.96631622314453\n",
      "iteration 33 , loss: 95.86844635009766\n",
      "iteration 34 , loss: 90.12474060058594\n",
      "iteration 35 , loss: 84.72140502929688\n",
      "iteration 36 , loss: 79.63262176513672\n",
      "iteration 37 , loss: 74.8070297241211\n",
      "iteration 38 , loss: 70.27640533447266\n",
      "iteration 39 , loss: 66.02023315429688\n",
      "iteration 40 , loss: 62.021095275878906\n",
      "iteration 41 , loss: 58.26713943481445\n",
      "iteration 42 , loss: 54.74323272705078\n",
      "iteration 43 , loss: 51.4234733581543\n",
      "iteration 44 , loss: 48.3073844909668\n",
      "iteration 45 , loss: 45.3836784362793\n",
      "iteration 46 , loss: 42.646392822265625\n",
      "iteration 47 , loss: 40.08295440673828\n",
      "iteration 48 , loss: 37.67913818359375\n",
      "iteration 49 , loss: 35.41404342651367\n",
      "iteration 50 , loss: 33.28663635253906\n",
      "iteration 51 , loss: 31.289392471313477\n",
      "iteration 52 , loss: 29.415876388549805\n",
      "iteration 53 , loss: 27.660551071166992\n",
      "iteration 54 , loss: 26.01093864440918\n",
      "iteration 55 , loss: 24.463777542114258\n",
      "iteration 56 , loss: 23.014925003051758\n",
      "iteration 57 , loss: 21.656702041625977\n",
      "iteration 58 , loss: 20.3780574798584\n",
      "iteration 59 , loss: 19.181110382080078\n",
      "iteration 60 , loss: 18.059019088745117\n",
      "iteration 61 , loss: 17.006786346435547\n",
      "iteration 62 , loss: 16.01825714111328\n",
      "iteration 63 , loss: 15.091500282287598\n",
      "iteration 64 , loss: 14.223381042480469\n",
      "iteration 65 , loss: 13.408987045288086\n",
      "iteration 66 , loss: 12.641165733337402\n",
      "iteration 67 , loss: 11.919682502746582\n",
      "iteration 68 , loss: 11.242319107055664\n",
      "iteration 69 , loss: 10.605626106262207\n",
      "iteration 70 , loss: 10.006904602050781\n",
      "iteration 71 , loss: 9.443828582763672\n",
      "iteration 72 , loss: 8.914645195007324\n",
      "iteration 73 , loss: 8.416518211364746\n",
      "iteration 74 , loss: 7.94883918762207\n",
      "iteration 75 , loss: 7.508540630340576\n",
      "iteration 76 , loss: 7.093409538269043\n",
      "iteration 77 , loss: 6.703507423400879\n",
      "iteration 78 , loss: 6.336175918579102\n",
      "iteration 79 , loss: 5.990071773529053\n",
      "iteration 80 , loss: 5.664380073547363\n",
      "iteration 81 , loss: 5.357723712921143\n",
      "iteration 82 , loss: 5.068705081939697\n",
      "iteration 83 , loss: 4.796433448791504\n",
      "iteration 84 , loss: 4.539559841156006\n",
      "iteration 85 , loss: 4.297518253326416\n",
      "iteration 86 , loss: 4.068931579589844\n",
      "iteration 87 , loss: 3.8533713817596436\n",
      "iteration 88 , loss: 3.6501150131225586\n",
      "iteration 89 , loss: 3.4582622051239014\n",
      "iteration 90 , loss: 3.277034044265747\n",
      "iteration 91 , loss: 3.1058263778686523\n",
      "iteration 92 , loss: 2.944260597229004\n",
      "iteration 93 , loss: 2.791470527648926\n",
      "iteration 94 , loss: 2.6469528675079346\n",
      "iteration 95 , loss: 2.5103721618652344\n",
      "iteration 96 , loss: 2.3813397884368896\n",
      "iteration 97 , loss: 2.2593071460723877\n",
      "iteration 98 , loss: 2.143911600112915\n",
      "iteration 99 , loss: 2.0349371433258057\n",
      "iteration 100 , loss: 1.9318678379058838\n",
      "iteration 101 , loss: 1.8342822790145874\n",
      "iteration 102 , loss: 1.7419185638427734\n",
      "iteration 103 , loss: 1.654455542564392\n",
      "iteration 104 , loss: 1.5716469287872314\n",
      "iteration 105 , loss: 1.4931895732879639\n",
      "iteration 106 , loss: 1.4189399480819702\n",
      "iteration 107 , loss: 1.3485764265060425\n",
      "iteration 108 , loss: 1.281803011894226\n",
      "iteration 109 , loss: 1.218479871749878\n",
      "iteration 110 , loss: 1.1584312915802002\n",
      "iteration 111 , loss: 1.1015264987945557\n",
      "iteration 112 , loss: 1.047523021697998\n",
      "iteration 113 , loss: 0.9963061809539795\n",
      "iteration 114 , loss: 0.947737455368042\n",
      "iteration 115 , loss: 0.9016340374946594\n",
      "iteration 116 , loss: 0.8579130172729492\n",
      "iteration 117 , loss: 0.8164204955101013\n",
      "iteration 118 , loss: 0.7770629525184631\n",
      "iteration 119 , loss: 0.7396833300590515\n",
      "iteration 120 , loss: 0.7041812539100647\n",
      "iteration 121 , loss: 0.6704913973808289\n",
      "iteration 122 , loss: 0.6384939551353455\n",
      "iteration 123 , loss: 0.6080971360206604\n",
      "iteration 124 , loss: 0.5792254209518433\n",
      "iteration 125 , loss: 0.5517792105674744\n",
      "iteration 126 , loss: 0.5256977677345276\n",
      "iteration 127 , loss: 0.5009050369262695\n",
      "iteration 128 , loss: 0.4773399829864502\n",
      "iteration 129 , loss: 0.4549420177936554\n",
      "iteration 130 , loss: 0.4336385428905487\n",
      "iteration 131 , loss: 0.4132128953933716\n",
      "iteration 132 , loss: 0.3937986195087433\n",
      "iteration 133 , loss: 0.37533342838287354\n",
      "iteration 134 , loss: 0.35778743028640747\n",
      "iteration 135 , loss: 0.3411129117012024\n",
      "iteration 136 , loss: 0.32525649666786194\n",
      "iteration 137 , loss: 0.3101705014705658\n",
      "iteration 138 , loss: 0.2958345115184784\n",
      "iteration 139 , loss: 0.2822069823741913\n",
      "iteration 140 , loss: 0.26923105120658875\n",
      "iteration 141 , loss: 0.25688403844833374\n",
      "iteration 142 , loss: 0.2451356202363968\n",
      "iteration 143 , loss: 0.23395858705043793\n",
      "iteration 144 , loss: 0.22331994771957397\n",
      "iteration 145 , loss: 0.2131856083869934\n",
      "iteration 146 , loss: 0.20353727042675018\n",
      "iteration 147 , loss: 0.1943373680114746\n",
      "iteration 148 , loss: 0.18558269739151\n",
      "iteration 149 , loss: 0.17724144458770752\n",
      "iteration 150 , loss: 0.16929467022418976\n",
      "iteration 151 , loss: 0.16171132028102875\n",
      "iteration 152 , loss: 0.15448319911956787\n",
      "iteration 153 , loss: 0.14759895205497742\n",
      "iteration 154 , loss: 0.14104034006595612\n",
      "iteration 155 , loss: 0.1347857415676117\n",
      "iteration 156 , loss: 0.12881693243980408\n",
      "iteration 157 , loss: 0.12312430888414383\n",
      "iteration 158 , loss: 0.11769703775644302\n",
      "iteration 159 , loss: 0.11251601576805115\n",
      "iteration 160 , loss: 0.10757189989089966\n",
      "iteration 161 , loss: 0.1028524786233902\n",
      "iteration 162 , loss: 0.09835364669561386\n",
      "iteration 163 , loss: 0.0940607413649559\n",
      "iteration 164 , loss: 0.08996330201625824\n",
      "iteration 165 , loss: 0.08605357259511948\n",
      "iteration 166 , loss: 0.08232142776250839\n",
      "iteration 167 , loss: 0.07875659316778183\n",
      "iteration 168 , loss: 0.07535643875598907\n",
      "iteration 169 , loss: 0.07210987061262131\n",
      "iteration 170 , loss: 0.0690109133720398\n",
      "iteration 171 , loss: 0.06605011969804764\n",
      "iteration 172 , loss: 0.0632236897945404\n",
      "iteration 173 , loss: 0.060523178428411484\n",
      "iteration 174 , loss: 0.05794245004653931\n",
      "iteration 175 , loss: 0.055477116256952286\n",
      "iteration 176 , loss: 0.05312061682343483\n",
      "iteration 177 , loss: 0.05086899548768997\n",
      "iteration 178 , loss: 0.048716697841882706\n",
      "iteration 179 , loss: 0.04666053131222725\n",
      "iteration 180 , loss: 0.044694237411022186\n",
      "iteration 181 , loss: 0.04281429573893547\n",
      "iteration 182 , loss: 0.04101736098527908\n",
      "iteration 183 , loss: 0.03930044546723366\n",
      "iteration 184 , loss: 0.03765726834535599\n",
      "iteration 185 , loss: 0.036086294800043106\n",
      "iteration 186 , loss: 0.03458401933312416\n",
      "iteration 187 , loss: 0.033144135028123856\n",
      "iteration 188 , loss: 0.031767670065164566\n",
      "iteration 189 , loss: 0.030450673773884773\n",
      "iteration 190 , loss: 0.029189852997660637\n",
      "iteration 191 , loss: 0.027984173968434334\n",
      "iteration 192 , loss: 0.026830067858099937\n",
      "iteration 193 , loss: 0.025725288316607475\n",
      "iteration 194 , loss: 0.024667862802743912\n",
      "iteration 195 , loss: 0.023655787110328674\n",
      "iteration 196 , loss: 0.022686421871185303\n",
      "iteration 197 , loss: 0.021758610382676125\n",
      "iteration 198 , loss: 0.020870424807071686\n",
      "iteration 199 , loss: 0.02001938968896866\n",
      "iteration 200 , loss: 0.019204692915081978\n",
      "iteration 201 , loss: 0.018425634130835533\n",
      "iteration 202 , loss: 0.01767907105386257\n",
      "iteration 203 , loss: 0.01696370169520378\n",
      "iteration 204 , loss: 0.01627807319164276\n",
      "iteration 205 , loss: 0.015620979480445385\n",
      "iteration 206 , loss: 0.014991981908679008\n",
      "iteration 207 , loss: 0.014388885349035263\n",
      "iteration 208 , loss: 0.013811065815389156\n",
      "iteration 209 , loss: 0.013257506303489208\n",
      "iteration 210 , loss: 0.012727100402116776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 211 , loss: 0.01221898477524519\n",
      "iteration 212 , loss: 0.01173127070069313\n",
      "iteration 213 , loss: 0.011263889260590076\n",
      "iteration 214 , loss: 0.010815859772264957\n",
      "iteration 215 , loss: 0.010386279784142971\n",
      "iteration 216 , loss: 0.009974314831197262\n",
      "iteration 217 , loss: 0.00957938376814127\n",
      "iteration 218 , loss: 0.009200620464980602\n",
      "iteration 219 , loss: 0.008837270550429821\n",
      "iteration 220 , loss: 0.008489087224006653\n",
      "iteration 221 , loss: 0.008155083283782005\n",
      "iteration 222 , loss: 0.007834739051759243\n",
      "iteration 223 , loss: 0.007527402136474848\n",
      "iteration 224 , loss: 0.007232581730931997\n",
      "iteration 225 , loss: 0.00694950670003891\n",
      "iteration 226 , loss: 0.0066780513152480125\n",
      "iteration 227 , loss: 0.006417642813175917\n",
      "iteration 228 , loss: 0.006167740561068058\n",
      "iteration 229 , loss: 0.005927667021751404\n",
      "iteration 230 , loss: 0.005697357002645731\n",
      "iteration 231 , loss: 0.005476396530866623\n",
      "iteration 232 , loss: 0.0052645099349319935\n",
      "iteration 233 , loss: 0.00506083806976676\n",
      "iteration 234 , loss: 0.004865285940468311\n",
      "iteration 235 , loss: 0.0046776048839092255\n",
      "iteration 236 , loss: 0.00449747871607542\n",
      "iteration 237 , loss: 0.004324446432292461\n",
      "iteration 238 , loss: 0.004158293828368187\n",
      "iteration 239 , loss: 0.003998741507530212\n",
      "iteration 240 , loss: 0.003845666768029332\n",
      "iteration 241 , loss: 0.003698549931868911\n",
      "iteration 242 , loss: 0.003557314397767186\n",
      "iteration 243 , loss: 0.003421691246330738\n",
      "iteration 244 , loss: 0.0032913910690695047\n",
      "iteration 245 , loss: 0.0031662776600569487\n",
      "iteration 246 , loss: 0.003046011086553335\n",
      "iteration 247 , loss: 0.002930512884631753\n",
      "iteration 248 , loss: 0.002819508546963334\n",
      "iteration 249 , loss: 0.0027128132060170174\n",
      "iteration 250 , loss: 0.0026103861164301634\n",
      "iteration 251 , loss: 0.0025118945632129908\n",
      "iteration 252 , loss: 0.002417225856333971\n",
      "iteration 253 , loss: 0.002326249610632658\n",
      "iteration 254 , loss: 0.0022387865465134382\n",
      "iteration 255 , loss: 0.0021547297947108746\n",
      "iteration 256 , loss: 0.002073931274935603\n",
      "iteration 257 , loss: 0.0019962384831160307\n",
      "iteration 258 , loss: 0.0019216109067201614\n",
      "iteration 259 , loss: 0.0018498330609872937\n",
      "iteration 260 , loss: 0.0017808370757848024\n",
      "iteration 261 , loss: 0.0017145265592262149\n",
      "iteration 262 , loss: 0.0016507302643731236\n",
      "iteration 263 , loss: 0.001589379389770329\n",
      "iteration 264 , loss: 0.0015303604304790497\n",
      "iteration 265 , loss: 0.0014736003940925002\n",
      "iteration 266 , loss: 0.0014189768116921186\n",
      "iteration 267 , loss: 0.00136648362968117\n",
      "iteration 268 , loss: 0.0013160101370885968\n",
      "iteration 269 , loss: 0.0012674418976530433\n",
      "iteration 270 , loss: 0.0012207203544676304\n",
      "iteration 271 , loss: 0.0011757706524804235\n",
      "iteration 272 , loss: 0.001132531906478107\n",
      "iteration 273 , loss: 0.0010909049306064844\n",
      "iteration 274 , loss: 0.0010508524719625711\n",
      "iteration 275 , loss: 0.0010123321553692222\n",
      "iteration 276 , loss: 0.0009752706973813474\n",
      "iteration 277 , loss: 0.00093962496612221\n",
      "iteration 278 , loss: 0.0009053031681105494\n",
      "iteration 279 , loss: 0.0008722755592316389\n",
      "iteration 280 , loss: 0.0008404728141613305\n",
      "iteration 281 , loss: 0.0008098644320853055\n",
      "iteration 282 , loss: 0.0007804115302860737\n",
      "iteration 283 , loss: 0.0007520543877035379\n",
      "iteration 284 , loss: 0.0007247742614708841\n",
      "iteration 285 , loss: 0.0006984893116168678\n",
      "iteration 286 , loss: 0.0006731958710588515\n",
      "iteration 287 , loss: 0.0006488583167083561\n",
      "iteration 288 , loss: 0.0006254010368138552\n",
      "iteration 289 , loss: 0.000602837186306715\n",
      "iteration 290 , loss: 0.0005810923757962883\n",
      "iteration 291 , loss: 0.0005601703887805343\n",
      "iteration 292 , loss: 0.000540021515917033\n",
      "iteration 293 , loss: 0.0005206172354519367\n",
      "iteration 294 , loss: 0.0005019089439883828\n",
      "iteration 295 , loss: 0.00048391547170467675\n",
      "iteration 296 , loss: 0.00046656440827064216\n",
      "iteration 297 , loss: 0.00044988052104599774\n",
      "iteration 298 , loss: 0.0004338092403486371\n",
      "iteration 299 , loss: 0.0004183145647402853\n",
      "iteration 300 , loss: 0.000403391633881256\n",
      "iteration 301 , loss: 0.0003889986255671829\n",
      "iteration 302 , loss: 0.00037512689596042037\n",
      "iteration 303 , loss: 0.000361779413651675\n",
      "iteration 304 , loss: 0.0003489101363811642\n",
      "iteration 305 , loss: 0.0003365127195138484\n",
      "iteration 306 , loss: 0.00032458940404467285\n",
      "iteration 307 , loss: 0.00031308585312217474\n",
      "iteration 308 , loss: 0.0003019889409188181\n",
      "iteration 309 , loss: 0.0002913160133175552\n",
      "iteration 310 , loss: 0.00028101634234189987\n",
      "iteration 311 , loss: 0.00027109880466014147\n",
      "iteration 312 , loss: 0.00026155082741752267\n",
      "iteration 313 , loss: 0.00025233515771105886\n",
      "iteration 314 , loss: 0.00024344734265469015\n",
      "iteration 315 , loss: 0.0002348900743527338\n",
      "iteration 316 , loss: 0.00022663523850496858\n",
      "iteration 317 , loss: 0.00021867983741685748\n",
      "iteration 318 , loss: 0.0002110158820869401\n",
      "iteration 319 , loss: 0.00020363024668768048\n",
      "iteration 320 , loss: 0.0001965094852494076\n",
      "iteration 321 , loss: 0.00018963938055094332\n",
      "iteration 322 , loss: 0.00018301057571079582\n",
      "iteration 323 , loss: 0.0001766342029441148\n",
      "iteration 324 , loss: 0.0001704809837974608\n",
      "iteration 325 , loss: 0.00016454099386464804\n",
      "iteration 326 , loss: 0.0001588141021784395\n",
      "iteration 327 , loss: 0.0001532988389953971\n",
      "iteration 328 , loss: 0.000147970873513259\n",
      "iteration 329 , loss: 0.0001428403047611937\n",
      "iteration 330 , loss: 0.00013788917567580938\n",
      "iteration 331 , loss: 0.00013311266957316548\n",
      "iteration 332 , loss: 0.00012851135397795588\n",
      "iteration 333 , loss: 0.0001240639976458624\n",
      "iteration 334 , loss: 0.0001197838137159124\n",
      "iteration 335 , loss: 0.00011565499880816787\n",
      "iteration 336 , loss: 0.00011167772026965395\n",
      "iteration 337 , loss: 0.00010782605386339128\n",
      "iteration 338 , loss: 0.00010411635594209656\n",
      "iteration 339 , loss: 0.0001005345766316168\n",
      "iteration 340 , loss: 9.708550351206213e-05\n",
      "iteration 341 , loss: 9.375774970976636e-05\n",
      "iteration 342 , loss: 9.054216207005084e-05\n",
      "iteration 343 , loss: 8.744467049837112e-05\n",
      "iteration 344 , loss: 8.444944251095876e-05\n",
      "iteration 345 , loss: 8.156044350471348e-05\n",
      "iteration 346 , loss: 7.878162432461977e-05\n",
      "iteration 347 , loss: 7.609082967974246e-05\n",
      "iteration 348 , loss: 7.349981024162844e-05\n",
      "iteration 349 , loss: 7.099909271346405e-05\n",
      "iteration 350 , loss: 6.858258711872622e-05\n",
      "iteration 351 , loss: 6.624882371397689e-05\n",
      "iteration 352 , loss: 6.399876292562112e-05\n",
      "iteration 353 , loss: 6.182723882375285e-05\n",
      "iteration 354 , loss: 5.972904909867793e-05\n",
      "iteration 355 , loss: 5.770724965259433e-05\n",
      "iteration 356 , loss: 5.575697650783695e-05\n",
      "iteration 357 , loss: 5.3868738177698106e-05\n",
      "iteration 358 , loss: 5.2050752856303006e-05\n",
      "iteration 359 , loss: 5.029325984651223e-05\n",
      "iteration 360 , loss: 4.859648470301181e-05\n",
      "iteration 361 , loss: 4.696008181781508e-05\n",
      "iteration 362 , loss: 4.538036955636926e-05\n",
      "iteration 363 , loss: 4.385339343571104e-05\n",
      "iteration 364 , loss: 4.2378513171570376e-05\n",
      "iteration 365 , loss: 4.0956401790026575e-05\n",
      "iteration 366 , loss: 3.9580623706569895e-05\n",
      "iteration 367 , loss: 3.8253667298704386e-05\n",
      "iteration 368 , loss: 3.6974273825762793e-05\n",
      "iteration 369 , loss: 3.573712092475034e-05\n",
      "iteration 370 , loss: 3.454336183494888e-05\n",
      "iteration 371 , loss: 3.338935857755132e-05\n",
      "iteration 372 , loss: 3.2274638215312734e-05\n",
      "iteration 373 , loss: 3.119864049949683e-05\n",
      "iteration 374 , loss: 3.0158709705574438e-05\n",
      "iteration 375 , loss: 2.9154793082852848e-05\n",
      "iteration 376 , loss: 2.818466782628093e-05\n",
      "iteration 377 , loss: 2.7246649551671e-05\n",
      "iteration 378 , loss: 2.6342184355598874e-05\n",
      "iteration 379 , loss: 2.5469660613453016e-05\n",
      "iteration 380 , loss: 2.4626018785056658e-05\n",
      "iteration 381 , loss: 2.380920159339439e-05\n",
      "iteration 382 , loss: 2.302116627106443e-05\n",
      "iteration 383 , loss: 2.2260470359469764e-05\n",
      "iteration 384 , loss: 2.15247073356295e-05\n",
      "iteration 385 , loss: 2.0815228708670475e-05\n",
      "iteration 386 , loss: 2.0127314201090485e-05\n",
      "iteration 387 , loss: 1.9465109289740212e-05\n",
      "iteration 388 , loss: 1.8823975551640615e-05\n",
      "iteration 389 , loss: 1.820434226829093e-05\n",
      "iteration 390 , loss: 1.760640589054674e-05\n",
      "iteration 391 , loss: 1.7027694411808625e-05\n",
      "iteration 392 , loss: 1.64666762429988e-05\n",
      "iteration 393 , loss: 1.592834451003e-05\n",
      "iteration 394 , loss: 1.5408111721626483e-05\n",
      "iteration 395 , loss: 1.4902118891768623e-05\n",
      "iteration 396 , loss: 1.4413999451790005e-05\n",
      "iteration 397 , loss: 1.3942411897005513e-05\n",
      "iteration 398 , loss: 1.3488351214618888e-05\n",
      "iteration 399 , loss: 1.3048400433035567e-05\n",
      "iteration 400 , loss: 1.262196292373119e-05\n",
      "iteration 401 , loss: 1.2211647117510438e-05\n",
      "iteration 402 , loss: 1.1812939192168415e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 403 , loss: 1.1428570360294543e-05\n",
      "iteration 404 , loss: 1.1057282790716272e-05\n",
      "iteration 405 , loss: 1.0699145605030935e-05\n",
      "iteration 406 , loss: 1.035140212479746e-05\n",
      "iteration 407 , loss: 1.001528562483145e-05\n",
      "iteration 408 , loss: 9.689575563243125e-06\n",
      "iteration 409 , loss: 9.376405614602845e-06\n",
      "iteration 410 , loss: 9.073151886695996e-06\n",
      "iteration 411 , loss: 8.780773896432947e-06\n",
      "iteration 412 , loss: 8.496646842104383e-06\n",
      "iteration 413 , loss: 8.2215392467333e-06\n",
      "iteration 414 , loss: 7.957321940921247e-06\n",
      "iteration 415 , loss: 7.701451067987364e-06\n",
      "iteration 416 , loss: 7.452852059941506e-06\n",
      "iteration 417 , loss: 7.214126071630744e-06\n",
      "iteration 418 , loss: 6.9803186306671705e-06\n",
      "iteration 419 , loss: 6.757031314919004e-06\n",
      "iteration 420 , loss: 6.5402305153838824e-06\n",
      "iteration 421 , loss: 6.329547431960236e-06\n",
      "iteration 422 , loss: 6.127862434368581e-06\n",
      "iteration 423 , loss: 5.931303803663468e-06\n",
      "iteration 424 , loss: 5.742279427067842e-06\n",
      "iteration 425 , loss: 5.5584587244084105e-06\n",
      "iteration 426 , loss: 5.380393304221798e-06\n",
      "iteration 427 , loss: 5.208949005464092e-06\n",
      "iteration 428 , loss: 5.042918473918689e-06\n",
      "iteration 429 , loss: 4.8820684241945855e-06\n",
      "iteration 430 , loss: 4.727012310468126e-06\n",
      "iteration 431 , loss: 4.575407729134895e-06\n",
      "iteration 432 , loss: 4.430187345860759e-06\n",
      "iteration 433 , loss: 4.289795469958335e-06\n",
      "iteration 434 , loss: 4.153443114773836e-06\n",
      "iteration 435 , loss: 4.021753738925327e-06\n",
      "iteration 436 , loss: 3.894285327987745e-06\n",
      "iteration 437 , loss: 3.771161800614209e-06\n",
      "iteration 438 , loss: 3.651495717349462e-06\n",
      "iteration 439 , loss: 3.5358664263185346e-06\n",
      "iteration 440 , loss: 3.4239715205330867e-06\n",
      "iteration 441 , loss: 3.3163664738822263e-06\n",
      "iteration 442 , loss: 3.2120797186507843e-06\n",
      "iteration 443 , loss: 3.1106380902201636e-06\n",
      "iteration 444 , loss: 3.0123489977995632e-06\n",
      "iteration 445 , loss: 2.917201982199913e-06\n",
      "iteration 446 , loss: 2.8258971269679023e-06\n",
      "iteration 447 , loss: 2.7364799279894214e-06\n",
      "iteration 448 , loss: 2.6508469090913422e-06\n",
      "iteration 449 , loss: 2.5673184609331656e-06\n",
      "iteration 450 , loss: 2.4869334538379917e-06\n",
      "iteration 451 , loss: 2.409390617685858e-06\n",
      "iteration 452 , loss: 2.3340833195106825e-06\n",
      "iteration 453 , loss: 2.260910605400568e-06\n",
      "iteration 454 , loss: 2.1904234017711133e-06\n",
      "iteration 455 , loss: 2.1214145817793906e-06\n",
      "iteration 456 , loss: 2.055203594863997e-06\n",
      "iteration 457 , loss: 1.99152509594569e-06\n",
      "iteration 458 , loss: 1.929462086991407e-06\n",
      "iteration 459 , loss: 1.8689886474021478e-06\n",
      "iteration 460 , loss: 1.8107130017597228e-06\n",
      "iteration 461 , loss: 1.7547223478686647e-06\n",
      "iteration 462 , loss: 1.6999126728478586e-06\n",
      "iteration 463 , loss: 1.6467580508106039e-06\n",
      "iteration 464 , loss: 1.5955814660628675e-06\n",
      "iteration 465 , loss: 1.5461149587281398e-06\n",
      "iteration 466 , loss: 1.4984966583142523e-06\n",
      "iteration 467 , loss: 1.452360265830066e-06\n",
      "iteration 468 , loss: 1.4071086980038672e-06\n",
      "iteration 469 , loss: 1.3637520623888122e-06\n",
      "iteration 470 , loss: 1.3212369367465726e-06\n",
      "iteration 471 , loss: 1.2806361837647273e-06\n",
      "iteration 472 , loss: 1.2412431260599988e-06\n",
      "iteration 473 , loss: 1.2032159020236577e-06\n",
      "iteration 474 , loss: 1.1660957852654974e-06\n",
      "iteration 475 , loss: 1.1298481013000128e-06\n",
      "iteration 476 , loss: 1.095325501410116e-06\n",
      "iteration 477 , loss: 1.0615068504193914e-06\n",
      "iteration 478 , loss: 1.0289134024787927e-06\n",
      "iteration 479 , loss: 9.973457508749561e-07\n",
      "iteration 480 , loss: 9.667751328379381e-07\n",
      "iteration 481 , loss: 9.371631790600077e-07\n",
      "iteration 482 , loss: 9.083587997338327e-07\n",
      "iteration 483 , loss: 8.807029985291592e-07\n",
      "iteration 484 , loss: 8.535778874829703e-07\n",
      "iteration 485 , loss: 8.275754339592822e-07\n",
      "iteration 486 , loss: 8.022543624974787e-07\n",
      "iteration 487 , loss: 7.777568384881306e-07\n",
      "iteration 488 , loss: 7.538958470831858e-07\n",
      "iteration 489 , loss: 7.30725446373981e-07\n",
      "iteration 490 , loss: 7.086447908477567e-07\n",
      "iteration 491 , loss: 6.871363211757853e-07\n",
      "iteration 492 , loss: 6.66106814151135e-07\n",
      "iteration 493 , loss: 6.460032295763085e-07\n",
      "iteration 494 , loss: 6.262339979912213e-07\n",
      "iteration 495 , loss: 6.071969664844801e-07\n",
      "iteration 496 , loss: 5.890175884815108e-07\n",
      "iteration 497 , loss: 5.710029427064001e-07\n",
      "iteration 498 , loss: 5.54178484435397e-07\n",
      "iteration 499 , loss: 5.370276312532951e-07\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N是数据样本数，D_in是输入层维度\n",
    "# H是隐藏层维度，D_out是输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机输入和输出数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 使用 nn 将模型定义为神经网络层的序列\n",
    "# nn.Sequential 是一个包含其他模块的模块，并按序应用模块以产生输出\n",
    "# 每个线性模块使用线性函数由输入计算输出，并为其权重和偏置保留内部 Tensor\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# nn 还包含了常用损失函数的定义\n",
    "# 使用均方误差 MSE 作为我们的损失函数\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "iteration_count = 500  # 迭代次数\n",
    "for t in range(iteration_count):\n",
    "    # 前向传播：将 x 传递给 model 以计算预测值 y\n",
    "    # 模块对象重载 __call__ 运算符，所以能直接像函数一样调用\n",
    "    # 传递一个 Tensor 给模型，模型将输出一个含有数据的 Tensor\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 计算并打印损失\n",
    "    # 将预测值与真实值传递给 loss_fn，返回损失\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(\"iteration\", t, \",\", \"loss:\", loss.item())  # t是循环变量\n",
    "\n",
    "    # 在计算反向传播前清零梯度\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 反向传播：计算损失关于所有可学习参数的梯度\n",
    "    # 在内部，每个模块的参数都存储在具有 requires_grad = True 的 Tensor 中\n",
    "    # 因此该调用将计算模型中所有可学习参数的梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 使用梯度下降更新权重，每个参数是一个 Tensor\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前为止，我们通过手动改变持有可学习参数的 Tensor 来更新模型的权重（使用 torch.no_grad() 或 .data 以避免在 autograd 中跟踪历史记录）。 对于像随机梯度下降这样的简单优化算法来说，这不是一个巨大的负担，但在实践中，我们经常使用更复杂的优化器如 AdaGrad，RMSProp，Adam 等来训练神经网络。\n",
    "\n",
    "PyTorch 中的 optim 包抽象出优化算法的思想，并提供常用优化算法的实现。\n",
    "\n",
    "在这个例子中，我们将使用 nn 包像以前一样定义我们的模型，但我们将使用 optim 包提供的 Adam 算法优化模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 , loss: 694.8057861328125\n",
      "iteration 1 , loss: 677.6531372070312\n",
      "iteration 2 , loss: 661.04931640625\n",
      "iteration 3 , loss: 644.896484375\n",
      "iteration 4 , loss: 629.1853637695312\n",
      "iteration 5 , loss: 613.8985595703125\n",
      "iteration 6 , loss: 599.0946044921875\n",
      "iteration 7 , loss: 584.7273559570312\n",
      "iteration 8 , loss: 570.8052368164062\n",
      "iteration 9 , loss: 557.2130126953125\n",
      "iteration 10 , loss: 543.9581298828125\n",
      "iteration 11 , loss: 531.0118408203125\n",
      "iteration 12 , loss: 518.3655395507812\n",
      "iteration 13 , loss: 506.0689697265625\n",
      "iteration 14 , loss: 494.1007995605469\n",
      "iteration 15 , loss: 482.4564514160156\n",
      "iteration 16 , loss: 471.1747741699219\n",
      "iteration 17 , loss: 460.2269287109375\n",
      "iteration 18 , loss: 449.57568359375\n",
      "iteration 19 , loss: 439.17877197265625\n",
      "iteration 20 , loss: 429.0790100097656\n",
      "iteration 21 , loss: 419.29608154296875\n",
      "iteration 22 , loss: 409.78472900390625\n",
      "iteration 23 , loss: 400.469482421875\n",
      "iteration 24 , loss: 391.3703918457031\n",
      "iteration 25 , loss: 382.5168151855469\n",
      "iteration 26 , loss: 373.8690185546875\n",
      "iteration 27 , loss: 365.4083251953125\n",
      "iteration 28 , loss: 357.0874938964844\n",
      "iteration 29 , loss: 348.9424743652344\n",
      "iteration 30 , loss: 340.97369384765625\n",
      "iteration 31 , loss: 333.1853332519531\n",
      "iteration 32 , loss: 325.5506896972656\n",
      "iteration 33 , loss: 318.1153869628906\n",
      "iteration 34 , loss: 310.85540771484375\n",
      "iteration 35 , loss: 303.8310241699219\n",
      "iteration 36 , loss: 296.94097900390625\n",
      "iteration 37 , loss: 290.2109069824219\n",
      "iteration 38 , loss: 283.62518310546875\n",
      "iteration 39 , loss: 277.1865539550781\n",
      "iteration 40 , loss: 270.8914794921875\n",
      "iteration 41 , loss: 264.7288818359375\n",
      "iteration 42 , loss: 258.6892395019531\n",
      "iteration 43 , loss: 252.77517700195312\n",
      "iteration 44 , loss: 246.9733428955078\n",
      "iteration 45 , loss: 241.28411865234375\n",
      "iteration 46 , loss: 235.70480346679688\n",
      "iteration 47 , loss: 230.25477600097656\n",
      "iteration 48 , loss: 224.92918395996094\n",
      "iteration 49 , loss: 219.7184295654297\n",
      "iteration 50 , loss: 214.62489318847656\n",
      "iteration 51 , loss: 209.60960388183594\n",
      "iteration 52 , loss: 204.71334838867188\n",
      "iteration 53 , loss: 199.9144287109375\n",
      "iteration 54 , loss: 195.20323181152344\n",
      "iteration 55 , loss: 190.60043334960938\n",
      "iteration 56 , loss: 186.08807373046875\n",
      "iteration 57 , loss: 181.65867614746094\n",
      "iteration 58 , loss: 177.3188018798828\n",
      "iteration 59 , loss: 173.06723022460938\n",
      "iteration 60 , loss: 168.90171813964844\n",
      "iteration 61 , loss: 164.82244873046875\n",
      "iteration 62 , loss: 160.82965087890625\n",
      "iteration 63 , loss: 156.91432189941406\n",
      "iteration 64 , loss: 153.07118225097656\n",
      "iteration 65 , loss: 149.31179809570312\n",
      "iteration 66 , loss: 145.6262969970703\n",
      "iteration 67 , loss: 142.0115966796875\n",
      "iteration 68 , loss: 138.46209716796875\n",
      "iteration 69 , loss: 134.99180603027344\n",
      "iteration 70 , loss: 131.5952911376953\n",
      "iteration 71 , loss: 128.2623748779297\n",
      "iteration 72 , loss: 124.99408721923828\n",
      "iteration 73 , loss: 121.78276824951172\n",
      "iteration 74 , loss: 118.64098358154297\n",
      "iteration 75 , loss: 115.56417846679688\n",
      "iteration 76 , loss: 112.54339599609375\n",
      "iteration 77 , loss: 109.5789794921875\n",
      "iteration 78 , loss: 106.67536163330078\n",
      "iteration 79 , loss: 103.83531951904297\n",
      "iteration 80 , loss: 101.05221557617188\n",
      "iteration 81 , loss: 98.326416015625\n",
      "iteration 82 , loss: 95.65924072265625\n",
      "iteration 83 , loss: 93.04682159423828\n",
      "iteration 84 , loss: 90.48517608642578\n",
      "iteration 85 , loss: 87.9721908569336\n",
      "iteration 86 , loss: 85.516357421875\n",
      "iteration 87 , loss: 83.11603546142578\n",
      "iteration 88 , loss: 80.76256561279297\n",
      "iteration 89 , loss: 78.46115112304688\n",
      "iteration 90 , loss: 76.20630645751953\n",
      "iteration 91 , loss: 73.99864959716797\n",
      "iteration 92 , loss: 71.84188079833984\n",
      "iteration 93 , loss: 69.73236083984375\n",
      "iteration 94 , loss: 67.67046356201172\n",
      "iteration 95 , loss: 65.65544128417969\n",
      "iteration 96 , loss: 63.688045501708984\n",
      "iteration 97 , loss: 61.76488494873047\n",
      "iteration 98 , loss: 59.88910675048828\n",
      "iteration 99 , loss: 58.061092376708984\n",
      "iteration 100 , loss: 56.27910232543945\n",
      "iteration 101 , loss: 54.53876495361328\n",
      "iteration 102 , loss: 52.84247970581055\n",
      "iteration 103 , loss: 51.187965393066406\n",
      "iteration 104 , loss: 49.57188034057617\n",
      "iteration 105 , loss: 47.996707916259766\n",
      "iteration 106 , loss: 46.461177825927734\n",
      "iteration 107 , loss: 44.96296310424805\n",
      "iteration 108 , loss: 43.50558853149414\n",
      "iteration 109 , loss: 42.08599090576172\n",
      "iteration 110 , loss: 40.703338623046875\n",
      "iteration 111 , loss: 39.35577392578125\n",
      "iteration 112 , loss: 38.04875183105469\n",
      "iteration 113 , loss: 36.778770446777344\n",
      "iteration 114 , loss: 35.543514251708984\n",
      "iteration 115 , loss: 34.343223571777344\n",
      "iteration 116 , loss: 33.17457580566406\n",
      "iteration 117 , loss: 32.0400505065918\n",
      "iteration 118 , loss: 30.93891716003418\n",
      "iteration 119 , loss: 29.86963653564453\n",
      "iteration 120 , loss: 28.831626892089844\n",
      "iteration 121 , loss: 27.824325561523438\n",
      "iteration 122 , loss: 26.846071243286133\n",
      "iteration 123 , loss: 25.89687156677246\n",
      "iteration 124 , loss: 24.97347068786621\n",
      "iteration 125 , loss: 24.07877540588379\n",
      "iteration 126 , loss: 23.210453033447266\n",
      "iteration 127 , loss: 22.368749618530273\n",
      "iteration 128 , loss: 21.55429458618164\n",
      "iteration 129 , loss: 20.765005111694336\n",
      "iteration 130 , loss: 20.000944137573242\n",
      "iteration 131 , loss: 19.261648178100586\n",
      "iteration 132 , loss: 18.545761108398438\n",
      "iteration 133 , loss: 17.853723526000977\n",
      "iteration 134 , loss: 17.183374404907227\n",
      "iteration 135 , loss: 16.535377502441406\n",
      "iteration 136 , loss: 15.90955638885498\n",
      "iteration 137 , loss: 15.303787231445312\n",
      "iteration 138 , loss: 14.719103813171387\n",
      "iteration 139 , loss: 14.154359817504883\n",
      "iteration 140 , loss: 13.608591079711914\n",
      "iteration 141 , loss: 13.082813262939453\n",
      "iteration 142 , loss: 12.574972152709961\n",
      "iteration 143 , loss: 12.085434913635254\n",
      "iteration 144 , loss: 11.61253547668457\n",
      "iteration 145 , loss: 11.156291961669922\n",
      "iteration 146 , loss: 10.7160005569458\n",
      "iteration 147 , loss: 10.29195785522461\n",
      "iteration 148 , loss: 9.883013725280762\n",
      "iteration 149 , loss: 9.488487243652344\n",
      "iteration 150 , loss: 9.108660697937012\n",
      "iteration 151 , loss: 8.742778778076172\n",
      "iteration 152 , loss: 8.39081859588623\n",
      "iteration 153 , loss: 8.051684379577637\n",
      "iteration 154 , loss: 7.724987983703613\n",
      "iteration 155 , loss: 7.410731315612793\n",
      "iteration 156 , loss: 7.1083083152771\n",
      "iteration 157 , loss: 6.81705379486084\n",
      "iteration 158 , loss: 6.537437915802002\n",
      "iteration 159 , loss: 6.268154621124268\n",
      "iteration 160 , loss: 6.009212493896484\n",
      "iteration 161 , loss: 5.760294437408447\n",
      "iteration 162 , loss: 5.521408557891846\n",
      "iteration 163 , loss: 5.2916436195373535\n",
      "iteration 164 , loss: 5.0710673332214355\n",
      "iteration 165 , loss: 4.8592658042907715\n",
      "iteration 166 , loss: 4.655776023864746\n",
      "iteration 167 , loss: 4.4606828689575195\n",
      "iteration 168 , loss: 4.273200988769531\n",
      "iteration 169 , loss: 4.0932841300964355\n",
      "iteration 170 , loss: 3.92063307762146\n",
      "iteration 171 , loss: 3.754621744155884\n",
      "iteration 172 , loss: 3.595604419708252\n",
      "iteration 173 , loss: 3.4428956508636475\n",
      "iteration 174 , loss: 3.296330451965332\n",
      "iteration 175 , loss: 3.1558284759521484\n",
      "iteration 176 , loss: 3.0210163593292236\n",
      "iteration 177 , loss: 2.8916890621185303\n",
      "iteration 178 , loss: 2.7675747871398926\n",
      "iteration 179 , loss: 2.648538827896118\n",
      "iteration 180 , loss: 2.53442120552063\n",
      "iteration 181 , loss: 2.425142765045166\n",
      "iteration 182 , loss: 2.3203022480010986\n",
      "iteration 183 , loss: 2.2204270362854004\n",
      "iteration 184 , loss: 2.124725341796875\n",
      "iteration 185 , loss: 2.0330281257629395\n",
      "iteration 186 , loss: 1.9451699256896973\n",
      "iteration 187 , loss: 1.8609908819198608\n",
      "iteration 188 , loss: 1.7803601026535034\n",
      "iteration 189 , loss: 1.7030699253082275\n",
      "iteration 190 , loss: 1.6290453672409058\n",
      "iteration 191 , loss: 1.5583009719848633\n",
      "iteration 192 , loss: 1.4905660152435303\n",
      "iteration 193 , loss: 1.4256517887115479\n",
      "iteration 194 , loss: 1.3635324239730835\n",
      "iteration 195 , loss: 1.3040906190872192\n",
      "iteration 196 , loss: 1.2471513748168945\n",
      "iteration 197 , loss: 1.1926931142807007\n",
      "iteration 198 , loss: 1.1406563520431519\n",
      "iteration 199 , loss: 1.090935230255127\n",
      "iteration 200 , loss: 1.043332815170288\n",
      "iteration 201 , loss: 0.9977890253067017\n",
      "iteration 202 , loss: 0.9542078971862793\n",
      "iteration 203 , loss: 0.9124987125396729\n",
      "iteration 204 , loss: 0.8726276159286499\n",
      "iteration 205 , loss: 0.8344813585281372\n",
      "iteration 206 , loss: 0.7979720234870911\n",
      "iteration 207 , loss: 0.7630361318588257\n",
      "iteration 208 , loss: 0.7296406030654907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 209 , loss: 0.697719156742096\n",
      "iteration 210 , loss: 0.6672675609588623\n",
      "iteration 211 , loss: 0.6381561160087585\n",
      "iteration 212 , loss: 0.6102754473686218\n",
      "iteration 213 , loss: 0.5836135745048523\n",
      "iteration 214 , loss: 0.5581120848655701\n",
      "iteration 215 , loss: 0.5337255597114563\n",
      "iteration 216 , loss: 0.5104163289070129\n",
      "iteration 217 , loss: 0.4880940318107605\n",
      "iteration 218 , loss: 0.46675166487693787\n",
      "iteration 219 , loss: 0.4463338255882263\n",
      "iteration 220 , loss: 0.4268069267272949\n",
      "iteration 221 , loss: 0.4081316888332367\n",
      "iteration 222 , loss: 0.3902675211429596\n",
      "iteration 223 , loss: 0.37318116426467896\n",
      "iteration 224 , loss: 0.3568435609340668\n",
      "iteration 225 , loss: 0.3412090539932251\n",
      "iteration 226 , loss: 0.32625705003738403\n",
      "iteration 227 , loss: 0.3119499087333679\n",
      "iteration 228 , loss: 0.29826873540878296\n",
      "iteration 229 , loss: 0.2851804494857788\n",
      "iteration 230 , loss: 0.2727362811565399\n",
      "iteration 231 , loss: 0.2608889937400818\n",
      "iteration 232 , loss: 0.24956701695919037\n",
      "iteration 233 , loss: 0.23871557414531708\n",
      "iteration 234 , loss: 0.22834117710590363\n",
      "iteration 235 , loss: 0.218434140086174\n",
      "iteration 236 , loss: 0.20895470678806305\n",
      "iteration 237 , loss: 0.1998860239982605\n",
      "iteration 238 , loss: 0.1912178099155426\n",
      "iteration 239 , loss: 0.18292219936847687\n",
      "iteration 240 , loss: 0.17498908936977386\n",
      "iteration 241 , loss: 0.16740065813064575\n",
      "iteration 242 , loss: 0.16014832258224487\n",
      "iteration 243 , loss: 0.15319715440273285\n",
      "iteration 244 , loss: 0.14655709266662598\n",
      "iteration 245 , loss: 0.14020255208015442\n",
      "iteration 246 , loss: 0.13412386178970337\n",
      "iteration 247 , loss: 0.12830857932567596\n",
      "iteration 248 , loss: 0.12273764610290527\n",
      "iteration 249 , loss: 0.11741798371076584\n",
      "iteration 250 , loss: 0.11232078075408936\n",
      "iteration 251 , loss: 0.10745203495025635\n",
      "iteration 252 , loss: 0.10279116779565811\n",
      "iteration 253 , loss: 0.09833186119794846\n",
      "iteration 254 , loss: 0.0940667912364006\n",
      "iteration 255 , loss: 0.08998490124940872\n",
      "iteration 256 , loss: 0.08607669174671173\n",
      "iteration 257 , loss: 0.08233718574047089\n",
      "iteration 258 , loss: 0.07875937223434448\n",
      "iteration 259 , loss: 0.07533583790063858\n",
      "iteration 260 , loss: 0.07205861061811447\n",
      "iteration 261 , loss: 0.06892429292201996\n",
      "iteration 262 , loss: 0.06592126190662384\n",
      "iteration 263 , loss: 0.06304927915334702\n",
      "iteration 264 , loss: 0.060301411896944046\n",
      "iteration 265 , loss: 0.057672418653964996\n",
      "iteration 266 , loss: 0.0551573783159256\n",
      "iteration 267 , loss: 0.052749693393707275\n",
      "iteration 268 , loss: 0.0504460446536541\n",
      "iteration 269 , loss: 0.04824274405837059\n",
      "iteration 270 , loss: 0.04613179340958595\n",
      "iteration 271 , loss: 0.044111523777246475\n",
      "iteration 272 , loss: 0.04217902943491936\n",
      "iteration 273 , loss: 0.04032965376973152\n",
      "iteration 274 , loss: 0.03856099396944046\n",
      "iteration 275 , loss: 0.036867573857307434\n",
      "iteration 276 , loss: 0.0352475605905056\n",
      "iteration 277 , loss: 0.03369688615202904\n",
      "iteration 278 , loss: 0.03221359848976135\n",
      "iteration 279 , loss: 0.030793946236371994\n",
      "iteration 280 , loss: 0.029436878859996796\n",
      "iteration 281 , loss: 0.0281367264688015\n",
      "iteration 282 , loss: 0.026894455775618553\n",
      "iteration 283 , loss: 0.025704680010676384\n",
      "iteration 284 , loss: 0.024567170068621635\n",
      "iteration 285 , loss: 0.023479342460632324\n",
      "iteration 286 , loss: 0.02243853360414505\n",
      "iteration 287 , loss: 0.02144252136349678\n",
      "iteration 288 , loss: 0.020490262657403946\n",
      "iteration 289 , loss: 0.019579293206334114\n",
      "iteration 290 , loss: 0.01870817318558693\n",
      "iteration 291 , loss: 0.017874881625175476\n",
      "iteration 292 , loss: 0.017078550532460213\n",
      "iteration 293 , loss: 0.016317525878548622\n",
      "iteration 294 , loss: 0.015588046051561832\n",
      "iteration 295 , loss: 0.014891505241394043\n",
      "iteration 296 , loss: 0.014225286431610584\n",
      "iteration 297 , loss: 0.01358907762914896\n",
      "iteration 298 , loss: 0.012980317696928978\n",
      "iteration 299 , loss: 0.012397978454828262\n",
      "iteration 300 , loss: 0.011841680854558945\n",
      "iteration 301 , loss: 0.011310066096484661\n",
      "iteration 302 , loss: 0.010801596567034721\n",
      "iteration 303 , loss: 0.010315677151083946\n",
      "iteration 304 , loss: 0.009851326234638691\n",
      "iteration 305 , loss: 0.00940782018005848\n",
      "iteration 306 , loss: 0.008983437903225422\n",
      "iteration 307 , loss: 0.008578277193009853\n",
      "iteration 308 , loss: 0.00819087028503418\n",
      "iteration 309 , loss: 0.00782080926001072\n",
      "iteration 310 , loss: 0.007467349991202354\n",
      "iteration 311 , loss: 0.007129843346774578\n",
      "iteration 312 , loss: 0.0068069747649133205\n",
      "iteration 313 , loss: 0.0064986394718289375\n",
      "iteration 314 , loss: 0.006204093806445599\n",
      "iteration 315 , loss: 0.0059227836318314075\n",
      "iteration 316 , loss: 0.005654128268361092\n",
      "iteration 317 , loss: 0.005397437140345573\n",
      "iteration 318 , loss: 0.005152407102286816\n",
      "iteration 319 , loss: 0.004918181337416172\n",
      "iteration 320 , loss: 0.004694455768913031\n",
      "iteration 321 , loss: 0.004480968229472637\n",
      "iteration 322 , loss: 0.0042768819257617\n",
      "iteration 323 , loss: 0.004082089755684137\n",
      "iteration 324 , loss: 0.0038961207028478384\n",
      "iteration 325 , loss: 0.003718499792739749\n",
      "iteration 326 , loss: 0.0035489494912326336\n",
      "iteration 327 , loss: 0.0033869792241603136\n",
      "iteration 328 , loss: 0.0032323510386049747\n",
      "iteration 329 , loss: 0.0030847566667944193\n",
      "iteration 330 , loss: 0.0029438980855047703\n",
      "iteration 331 , loss: 0.002809305442497134\n",
      "iteration 332 , loss: 0.002680898876860738\n",
      "iteration 333 , loss: 0.0025583149399608374\n",
      "iteration 334 , loss: 0.002441327553242445\n",
      "iteration 335 , loss: 0.0023296037688851357\n",
      "iteration 336 , loss: 0.0022230015601962805\n",
      "iteration 337 , loss: 0.002121306722983718\n",
      "iteration 338 , loss: 0.0020241630263626575\n",
      "iteration 339 , loss: 0.0019314342644065619\n",
      "iteration 340 , loss: 0.001843001926317811\n",
      "iteration 341 , loss: 0.0017585603054612875\n",
      "iteration 342 , loss: 0.001677973079495132\n",
      "iteration 343 , loss: 0.001601066323928535\n",
      "iteration 344 , loss: 0.0015276869526132941\n",
      "iteration 345 , loss: 0.001457621343433857\n",
      "iteration 346 , loss: 0.0013907933607697487\n",
      "iteration 347 , loss: 0.0013270076597109437\n",
      "iteration 348 , loss: 0.0012661376968026161\n",
      "iteration 349 , loss: 0.0012080559972673655\n",
      "iteration 350 , loss: 0.0011526335729286075\n",
      "iteration 351 , loss: 0.0010997508652508259\n",
      "iteration 352 , loss: 0.001049307524226606\n",
      "iteration 353 , loss: 0.001001246739178896\n",
      "iteration 354 , loss: 0.0009552212432026863\n",
      "iteration 355 , loss: 0.0009113885462284088\n",
      "iteration 356 , loss: 0.0008695833384990692\n",
      "iteration 357 , loss: 0.0008296906016767025\n",
      "iteration 358 , loss: 0.0007916095783002675\n",
      "iteration 359 , loss: 0.0007552924798801541\n",
      "iteration 360 , loss: 0.0007206291193142533\n",
      "iteration 361 , loss: 0.0006875651306472719\n",
      "iteration 362 , loss: 0.0006560304318554699\n",
      "iteration 363 , loss: 0.0006259545916691422\n",
      "iteration 364 , loss: 0.0005971971550025046\n",
      "iteration 365 , loss: 0.0005698118475265801\n",
      "iteration 366 , loss: 0.000543669331818819\n",
      "iteration 367 , loss: 0.0005187258939258754\n",
      "iteration 368 , loss: 0.0004949350841343403\n",
      "iteration 369 , loss: 0.0004722317971754819\n",
      "iteration 370 , loss: 0.0004505789838731289\n",
      "iteration 371 , loss: 0.00042990679503418505\n",
      "iteration 372 , loss: 0.00041017227340489626\n",
      "iteration 373 , loss: 0.00039137169369496405\n",
      "iteration 374 , loss: 0.00037341128336265683\n",
      "iteration 375 , loss: 0.00035628516343422234\n",
      "iteration 376 , loss: 0.00033993966644629836\n",
      "iteration 377 , loss: 0.00032435054890811443\n",
      "iteration 378 , loss: 0.00030947220511734486\n",
      "iteration 379 , loss: 0.0002952737268060446\n",
      "iteration 380 , loss: 0.0002817253116518259\n",
      "iteration 381 , loss: 0.00026880609220825136\n",
      "iteration 382 , loss: 0.0002564816677477211\n",
      "iteration 383 , loss: 0.0002447151346132159\n",
      "iteration 384 , loss: 0.0002334853052161634\n",
      "iteration 385 , loss: 0.00022277426614891738\n",
      "iteration 386 , loss: 0.00021255506726447493\n",
      "iteration 387 , loss: 0.0002028082963079214\n",
      "iteration 388 , loss: 0.00019349955255165696\n",
      "iteration 389 , loss: 0.00018462329171597958\n",
      "iteration 390 , loss: 0.00017615138494875282\n",
      "iteration 391 , loss: 0.00016806708299554884\n",
      "iteration 392 , loss: 0.00016036067972891033\n",
      "iteration 393 , loss: 0.00015300436643883586\n",
      "iteration 394 , loss: 0.0001460633793612942\n",
      "iteration 395 , loss: 0.00013944055535830557\n",
      "iteration 396 , loss: 0.0001331203820882365\n",
      "iteration 397 , loss: 0.0001270959182875231\n",
      "iteration 398 , loss: 0.00012134634016547352\n",
      "iteration 399 , loss: 0.00011586371692828834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 , loss: 0.00011063663987442851\n",
      "iteration 401 , loss: 0.00010564574040472507\n",
      "iteration 402 , loss: 0.00010088491399073973\n",
      "iteration 403 , loss: 9.634345769882202e-05\n",
      "iteration 404 , loss: 9.201249486068264e-05\n",
      "iteration 405 , loss: 8.787601836957037e-05\n",
      "iteration 406 , loss: 8.393125608563423e-05\n",
      "iteration 407 , loss: 8.016546780709177e-05\n",
      "iteration 408 , loss: 7.65705481171608e-05\n",
      "iteration 409 , loss: 7.313646347029135e-05\n",
      "iteration 410 , loss: 6.986078369664028e-05\n",
      "iteration 411 , loss: 6.673353345831856e-05\n",
      "iteration 412 , loss: 6.374895019689575e-05\n",
      "iteration 413 , loss: 6.089794260333292e-05\n",
      "iteration 414 , loss: 5.81725180381909e-05\n",
      "iteration 415 , loss: 5.55708866158966e-05\n",
      "iteration 416 , loss: 5.3087038395460695e-05\n",
      "iteration 417 , loss: 5.071537088952027e-05\n",
      "iteration 418 , loss: 4.84485317429062e-05\n",
      "iteration 419 , loss: 4.6283799747470766e-05\n",
      "iteration 420 , loss: 4.421397534315474e-05\n",
      "iteration 421 , loss: 4.223949144943617e-05\n",
      "iteration 422 , loss: 4.035314850625582e-05\n",
      "iteration 423 , loss: 3.8547830627067015e-05\n",
      "iteration 424 , loss: 3.6826040741289034e-05\n",
      "iteration 425 , loss: 3.51799390045926e-05\n",
      "iteration 426 , loss: 3.360671689733863e-05\n",
      "iteration 427 , loss: 3.210428258171305e-05\n",
      "iteration 428 , loss: 3.066953649977222e-05\n",
      "iteration 429 , loss: 2.9298396839294583e-05\n",
      "iteration 430 , loss: 2.798701825668104e-05\n",
      "iteration 431 , loss: 2.6735890060081147e-05\n",
      "iteration 432 , loss: 2.5538825866533443e-05\n",
      "iteration 433 , loss: 2.4396356820943765e-05\n",
      "iteration 434 , loss: 2.330521419935394e-05\n",
      "iteration 435 , loss: 2.2259602701524273e-05\n",
      "iteration 436 , loss: 2.1262387235765345e-05\n",
      "iteration 437 , loss: 2.031014628300909e-05\n",
      "iteration 438 , loss: 1.940114816534333e-05\n",
      "iteration 439 , loss: 1.8528615328250453e-05\n",
      "iteration 440 , loss: 1.7697017028694972e-05\n",
      "iteration 441 , loss: 1.690268618403934e-05\n",
      "iteration 442 , loss: 1.614324537513312e-05\n",
      "iteration 443 , loss: 1.5418750990647823e-05\n",
      "iteration 444 , loss: 1.4726353583682794e-05\n",
      "iteration 445 , loss: 1.4062241461942904e-05\n",
      "iteration 446 , loss: 1.3429658793029375e-05\n",
      "iteration 447 , loss: 1.2824467376049142e-05\n",
      "iteration 448 , loss: 1.2247375707374886e-05\n",
      "iteration 449 , loss: 1.1696008186845575e-05\n",
      "iteration 450 , loss: 1.116767180064926e-05\n",
      "iteration 451 , loss: 1.0663888133422006e-05\n",
      "iteration 452 , loss: 1.0181742254644632e-05\n",
      "iteration 453 , loss: 9.722764843900222e-06\n",
      "iteration 454 , loss: 9.281098755309358e-06\n",
      "iteration 455 , loss: 8.862066351866815e-06\n",
      "iteration 456 , loss: 8.459913260594476e-06\n",
      "iteration 457 , loss: 8.076859558059368e-06\n",
      "iteration 458 , loss: 7.709065357630607e-06\n",
      "iteration 459 , loss: 7.3596020229160786e-06\n",
      "iteration 460 , loss: 7.025067588983802e-06\n",
      "iteration 461 , loss: 6.706392468913691e-06\n",
      "iteration 462 , loss: 6.400150141416816e-06\n",
      "iteration 463 , loss: 6.108813067839947e-06\n",
      "iteration 464 , loss: 5.830182999488898e-06\n",
      "iteration 465 , loss: 5.563884769799188e-06\n",
      "iteration 466 , loss: 5.309543666953687e-06\n",
      "iteration 467 , loss: 5.066373887530062e-06\n",
      "iteration 468 , loss: 4.834348601434613e-06\n",
      "iteration 469 , loss: 4.612649263435742e-06\n",
      "iteration 470 , loss: 4.400238140078727e-06\n",
      "iteration 471 , loss: 4.198984697723063e-06\n",
      "iteration 472 , loss: 4.00545286538545e-06\n",
      "iteration 473 , loss: 3.821096925094025e-06\n",
      "iteration 474 , loss: 3.645307515398599e-06\n",
      "iteration 475 , loss: 3.4774720916175283e-06\n",
      "iteration 476 , loss: 3.3165567856485723e-06\n",
      "iteration 477 , loss: 3.1623078484699363e-06\n",
      "iteration 478 , loss: 3.0161961603880627e-06\n",
      "iteration 479 , loss: 2.8760459827026352e-06\n",
      "iteration 480 , loss: 2.7434195999376243e-06\n",
      "iteration 481 , loss: 2.6156221792916767e-06\n",
      "iteration 482 , loss: 2.4935980036389083e-06\n",
      "iteration 483 , loss: 2.3774914552632254e-06\n",
      "iteration 484 , loss: 2.266442152176751e-06\n",
      "iteration 485 , loss: 2.160677013307577e-06\n",
      "iteration 486 , loss: 2.0592674445651937e-06\n",
      "iteration 487 , loss: 1.9631538634712342e-06\n",
      "iteration 488 , loss: 1.871237486739119e-06\n",
      "iteration 489 , loss: 1.7832790035754442e-06\n",
      "iteration 490 , loss: 1.699653239484178e-06\n",
      "iteration 491 , loss: 1.6194022691706778e-06\n",
      "iteration 492 , loss: 1.543282337479468e-06\n",
      "iteration 493 , loss: 1.4703366559842834e-06\n",
      "iteration 494 , loss: 1.400515429850202e-06\n",
      "iteration 495 , loss: 1.3341900739760604e-06\n",
      "iteration 496 , loss: 1.2710349892586237e-06\n",
      "iteration 497 , loss: 1.2111482874388457e-06\n",
      "iteration 498 , loss: 1.1535028079379117e-06\n",
      "iteration 499 , loss: 1.098086954698374e-06\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N是数据样本数，D_in是输入层维度\n",
    "# H是隐藏层维度，D_out是输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机输入和输出数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 使用 nn 定义模型与损失函数\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 使用 optim 包定义一个优化器，它将为我们更新权重\n",
    "# 此处使用 Adam\n",
    "# optim 包包含许多其他优化算法\n",
    "# Adam 构造函数的首个参数告诉优化器哪个 Tensor 应该被更新\n",
    "learning_rate = 1e-4\n",
    "iteration_count = 500  # 迭代次数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(iteration_count):\n",
    "    # 前向传播：将 x 传递给 model 以计算预测值 y\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 计算并打印损失\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(\"iteration\", t, \",\", \"loss:\", loss.item())  # t是循环变量\n",
    "\n",
    "    # 在计算反向传播前清零梯度\n",
    "    # 在反向传播之前，使用优化器对象将要更新的变量的所有梯度归零（这是模型的可学习权重）\n",
    "    # 这是因为默认情况下，只要调用.backward()，梯度就会累积在缓冲区中（即不会被覆盖）\n",
    "    # 有关更多详细信息，请查看 torch.autograd.backward 的文档\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 反向传播：计算损失关于所有可学习参数的梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 调用优化器的 step 函数，更新参数\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: 自定义 nn 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时，您需要指定**比现有模块序列更复杂**的模型; 对于这些情况，您可以通过子类化 nn.Module 定义自己的模块，并定义一个接收输入 Tensor 的 forward，并使用其他模块或 Tensor 上的其他 autograd 操作生成输出Tensor。\n",
    "\n",
    "在这个例子中，我们将我们的双层网络实现为自定义 Module 子类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 650.4168090820312\n",
      "1 604.8271484375\n",
      "2 565.3889770507812\n",
      "3 530.7918701171875\n",
      "4 499.9839782714844\n",
      "5 472.0627136230469\n",
      "6 446.8885192871094\n",
      "7 423.9190673828125\n",
      "8 402.59405517578125\n",
      "9 382.6856994628906\n",
      "10 363.968017578125\n",
      "11 346.40966796875\n",
      "12 329.8550720214844\n",
      "13 314.1585998535156\n",
      "14 299.1746520996094\n",
      "15 284.8345642089844\n",
      "16 271.18603515625\n",
      "17 258.0657043457031\n",
      "18 245.41969299316406\n",
      "19 233.26780700683594\n",
      "20 221.6000213623047\n",
      "21 210.37281799316406\n",
      "22 199.6201934814453\n",
      "23 189.3350830078125\n",
      "24 179.4701385498047\n",
      "25 170.0068359375\n",
      "26 160.9403839111328\n",
      "27 152.27197265625\n",
      "28 143.979736328125\n",
      "29 136.0373992919922\n",
      "30 128.4136505126953\n",
      "31 121.13982391357422\n",
      "32 114.2225341796875\n",
      "33 107.66429901123047\n",
      "34 101.43965911865234\n",
      "35 95.55449676513672\n",
      "36 89.99089813232422\n",
      "37 84.69075775146484\n",
      "38 79.68389129638672\n",
      "39 74.96304321289062\n",
      "40 70.51262664794922\n",
      "41 66.32493591308594\n",
      "42 62.37954330444336\n",
      "43 58.6663703918457\n",
      "44 55.167930603027344\n",
      "45 51.85204315185547\n",
      "46 48.73835372924805\n",
      "47 45.81789779663086\n",
      "48 43.07676315307617\n",
      "49 40.51050567626953\n",
      "50 38.103641510009766\n",
      "51 35.84551239013672\n",
      "52 33.72714614868164\n",
      "53 31.739810943603516\n",
      "54 29.87888526916504\n",
      "55 28.133481979370117\n",
      "56 26.494110107421875\n",
      "57 24.955106735229492\n",
      "58 23.510272979736328\n",
      "59 22.15471649169922\n",
      "60 20.88254165649414\n",
      "61 19.689104080200195\n",
      "62 18.567806243896484\n",
      "63 17.515913009643555\n",
      "64 16.526243209838867\n",
      "65 15.59669017791748\n",
      "66 14.722648620605469\n",
      "67 13.901010513305664\n",
      "68 13.128752708435059\n",
      "69 12.40227222442627\n",
      "70 11.718873023986816\n",
      "71 11.077043533325195\n",
      "72 10.473163604736328\n",
      "73 9.904953002929688\n",
      "74 9.37026309967041\n",
      "75 8.86728286743164\n",
      "76 8.39417839050293\n",
      "77 7.948423862457275\n",
      "78 7.528041839599609\n",
      "79 7.132325172424316\n",
      "80 6.7597575187683105\n",
      "81 6.408631324768066\n",
      "82 6.077478885650635\n",
      "83 5.765405178070068\n",
      "84 5.470718860626221\n",
      "85 5.192356586456299\n",
      "86 4.929925918579102\n",
      "87 4.681695938110352\n",
      "88 4.446671962738037\n",
      "89 4.224484443664551\n",
      "90 4.014431953430176\n",
      "91 3.8162152767181396\n",
      "92 3.628434658050537\n",
      "93 3.4507596492767334\n",
      "94 3.2825229167938232\n",
      "95 3.1233482360839844\n",
      "96 2.9726078510284424\n",
      "97 2.8299753665924072\n",
      "98 2.694620132446289\n",
      "99 2.5666120052337646\n",
      "100 2.445138692855835\n",
      "101 2.329890727996826\n",
      "102 2.220414876937866\n",
      "103 2.116405487060547\n",
      "104 2.0176961421966553\n",
      "105 1.924080491065979\n",
      "106 1.8352649211883545\n",
      "107 1.7510355710983276\n",
      "108 1.671013355255127\n",
      "109 1.59492027759552\n",
      "110 1.5226356983184814\n",
      "111 1.4539012908935547\n",
      "112 1.3885929584503174\n",
      "113 1.3264538049697876\n",
      "114 1.2674380540847778\n",
      "115 1.2112135887145996\n",
      "116 1.1577903032302856\n",
      "117 1.106956958770752\n",
      "118 1.058576226234436\n",
      "119 1.012497067451477\n",
      "120 0.9686092734336853\n",
      "121 0.9267550110816956\n",
      "122 0.8869354724884033\n",
      "123 0.8489871025085449\n",
      "124 0.8127830028533936\n",
      "125 0.7782686352729797\n",
      "126 0.7453024387359619\n",
      "127 0.7138729691505432\n",
      "128 0.6839433908462524\n",
      "129 0.6553502678871155\n",
      "130 0.6280455589294434\n",
      "131 0.6018532514572144\n",
      "132 0.5768840909004211\n",
      "133 0.5530387759208679\n",
      "134 0.5302873849868774\n",
      "135 0.5085374116897583\n",
      "136 0.48760339617729187\n",
      "137 0.4676033854484558\n",
      "138 0.4485117197036743\n",
      "139 0.4302576184272766\n",
      "140 0.41279709339141846\n",
      "141 0.3960704207420349\n",
      "142 0.38008445501327515\n",
      "143 0.36480721831321716\n",
      "144 0.3501855731010437\n",
      "145 0.33620649576187134\n",
      "146 0.3228153884410858\n",
      "147 0.30999016761779785\n",
      "148 0.29771023988723755\n",
      "149 0.2859514057636261\n",
      "150 0.2746926546096802\n",
      "151 0.26390787959098816\n",
      "152 0.2535741329193115\n",
      "153 0.2436777651309967\n",
      "154 0.23420409858226776\n",
      "155 0.22511669993400574\n",
      "156 0.2164023220539093\n",
      "157 0.20804663002490997\n",
      "158 0.200037881731987\n",
      "159 0.192357137799263\n",
      "160 0.18499122560024261\n",
      "161 0.17792119085788727\n",
      "162 0.1711397022008896\n",
      "163 0.164640873670578\n",
      "164 0.15839676558971405\n",
      "165 0.152414008975029\n",
      "166 0.14666366577148438\n",
      "167 0.1411426067352295\n",
      "168 0.13583789765834808\n",
      "169 0.13074582815170288\n",
      "170 0.1258576661348343\n",
      "171 0.12116305530071259\n",
      "172 0.11665309965610504\n",
      "173 0.11232176423072815\n",
      "174 0.10816021263599396\n",
      "175 0.10416029393672943\n",
      "176 0.10031906515359879\n",
      "177 0.09662602096796036\n",
      "178 0.09307835251092911\n",
      "179 0.08966497331857681\n",
      "180 0.08638564497232437\n",
      "181 0.08323156833648682\n",
      "182 0.08019936084747314\n",
      "183 0.07728367298841476\n",
      "184 0.0744798555970192\n",
      "185 0.07178159058094025\n",
      "186 0.06918548047542572\n",
      "187 0.06668908894062042\n",
      "188 0.06429008394479752\n",
      "189 0.06197983771562576\n",
      "190 0.059757281094789505\n",
      "191 0.05761910229921341\n",
      "192 0.05556017532944679\n",
      "193 0.05357848107814789\n",
      "194 0.05167108401656151\n",
      "195 0.049833908677101135\n",
      "196 0.04806607961654663\n",
      "197 0.0463627390563488\n",
      "198 0.04472329095005989\n",
      "199 0.043145276606082916\n",
      "200 0.04162485525012016\n",
      "201 0.040160223841667175\n",
      "202 0.03874955326318741\n",
      "203 0.0373908095061779\n",
      "204 0.03608211874961853\n",
      "205 0.03482072800397873\n",
      "206 0.03360496088862419\n",
      "207 0.03243400156497955\n",
      "208 0.03130563348531723\n",
      "209 0.030218834057450294\n",
      "210 0.02917143516242504\n",
      "211 0.02816070057451725\n",
      "212 0.027186822146177292\n",
      "213 0.026248576119542122\n",
      "214 0.02534342184662819\n",
      "215 0.024470670148730278\n",
      "216 0.023629117757081985\n",
      "217 0.022818122059106827\n",
      "218 0.022036036476492882\n",
      "219 0.021284140646457672\n",
      "220 0.020560462027788162\n",
      "221 0.019862741231918335\n",
      "222 0.01918942853808403\n",
      "223 0.01853977143764496\n",
      "224 0.017912881448864937\n",
      "225 0.017308082431554794\n",
      "226 0.016724521294236183\n",
      "227 0.016161654144525528\n",
      "228 0.01561830285936594\n",
      "229 0.015094425529241562\n",
      "230 0.014588517136871815\n",
      "231 0.014100019820034504\n",
      "232 0.013628857210278511\n",
      "233 0.013173925690352917\n",
      "234 0.012734797783195972\n",
      "235 0.012310664169490337\n",
      "236 0.011901204474270344\n",
      "237 0.01150577049702406\n",
      "238 0.011124096810817719\n",
      "239 0.010755899362266064\n",
      "240 0.010400391183793545\n",
      "241 0.010056786239147186\n",
      "242 0.009724817238748074\n",
      "243 0.009404078125953674\n",
      "244 0.009094453416764736\n",
      "245 0.008795244619250298\n",
      "246 0.00850628037005663\n",
      "247 0.008227217942476273\n",
      "248 0.00795763824135065\n",
      "249 0.0076976679265499115\n",
      "250 0.007446056231856346\n",
      "251 0.007203043904155493\n",
      "252 0.006968063302338123\n",
      "253 0.006740890443325043\n",
      "254 0.006521453615278006\n",
      "255 0.006309409625828266\n",
      "256 0.00610469002276659\n",
      "257 0.005906680598855019\n",
      "258 0.005715416744351387\n",
      "259 0.005530435126274824\n",
      "260 0.005351635627448559\n",
      "261 0.005178782157599926\n",
      "262 0.0050116004422307014\n",
      "263 0.004850062076002359\n",
      "264 0.004693915601819754\n",
      "265 0.004542924463748932\n",
      "266 0.0043968926183879375\n",
      "267 0.0042558703571558\n",
      "268 0.004119379445910454\n",
      "269 0.003987384960055351\n",
      "270 0.0038598424289375544\n",
      "271 0.0037364154122769833\n",
      "272 0.003617063397541642\n",
      "273 0.003501678816974163\n",
      "274 0.003389984369277954\n",
      "275 0.003282013116404414\n",
      "276 0.0031776626128703356\n",
      "277 0.003076698863878846\n",
      "278 0.0029789721593260765\n",
      "279 0.0028844522312283516\n",
      "280 0.0027930077631026506\n",
      "281 0.0027045332826673985\n",
      "282 0.0026189053896814585\n",
      "283 0.0025361073203384876\n",
      "284 0.002455980284139514\n",
      "285 0.002378524513915181\n",
      "286 0.0023035332560539246\n",
      "287 0.0022309389896690845\n",
      "288 0.0021607086528092623\n",
      "289 0.00209274934604764\n",
      "290 0.0020269749220460653\n",
      "291 0.001963340910151601\n",
      "292 0.0019017908489331603\n",
      "293 0.001842256635427475\n",
      "294 0.0017845404800027609\n",
      "295 0.0017286931397393346\n",
      "296 0.0016746657202020288\n",
      "297 0.0016223170096054673\n",
      "298 0.0015716678462922573\n",
      "299 0.001522676320746541\n",
      "300 0.0014752259012311697\n",
      "301 0.0014292882988229394\n",
      "302 0.001384840114042163\n",
      "303 0.0013417616719380021\n",
      "304 0.0013000696199014783\n",
      "305 0.0012597128516063094\n",
      "306 0.0012206420069560409\n",
      "307 0.001182806445285678\n",
      "308 0.0011461736867204309\n",
      "309 0.0011106727179139853\n",
      "310 0.0010763123864308\n",
      "311 0.0010430451948195696\n",
      "312 0.001010823529213667\n",
      "313 0.0009796192171052098\n",
      "314 0.0009493937250226736\n",
      "315 0.0009201274369843304\n",
      "316 0.0008918083040043712\n",
      "317 0.0008643656619824469\n",
      "318 0.0008378030033782125\n",
      "319 0.0008120817947201431\n",
      "320 0.0007871536072343588\n",
      "321 0.0007629928877577186\n",
      "322 0.0007396080181933939\n",
      "323 0.0007169540040194988\n",
      "324 0.0006949997623451054\n",
      "325 0.0006737338844686747\n",
      "326 0.0006531331455335021\n",
      "327 0.0006331739714369178\n",
      "328 0.000613842741586268\n",
      "329 0.0005951166967861354\n",
      "330 0.0005769652198068798\n",
      "331 0.0005593908135779202\n",
      "332 0.0005423484253697097\n",
      "333 0.000525851733982563\n",
      "334 0.0005098513211123645\n",
      "335 0.000494360807351768\n",
      "336 0.00047935015754774213\n",
      "337 0.00046481151366606355\n",
      "338 0.00045069781481288373\n",
      "339 0.0004370456445030868\n",
      "340 0.0004238129185978323\n",
      "341 0.00041099669761024415\n",
      "342 0.00039856915827840567\n",
      "343 0.00038652055081911385\n",
      "344 0.0003748357994481921\n",
      "345 0.0003635202010627836\n",
      "346 0.0003525562642607838\n",
      "347 0.00034191450686194\n",
      "348 0.0003316179208923131\n",
      "349 0.0003216278273612261\n",
      "350 0.00031194250914268196\n",
      "351 0.00030255821184255183\n",
      "352 0.00029346204246394336\n",
      "353 0.0002846436982508749\n",
      "354 0.00027609337121248245\n",
      "355 0.0002678035234566778\n",
      "356 0.0002597843995317817\n",
      "357 0.00025199359515681863\n",
      "358 0.0002444425190333277\n",
      "359 0.00023711829271633178\n",
      "360 0.00023002794478088617\n",
      "361 0.00022314874513540417\n",
      "362 0.00021647826361004263\n",
      "363 0.00021001789718866348\n",
      "364 0.00020374689484015107\n",
      "365 0.00019766905461438\n",
      "366 0.00019177583453711122\n",
      "367 0.0001860590127762407\n",
      "368 0.0001805150677682832\n",
      "369 0.0001751412928570062\n",
      "370 0.00016993253666441888\n",
      "371 0.0001648776524234563\n",
      "372 0.00015997307491488755\n",
      "373 0.00015521992463618517\n",
      "374 0.00015061485464684665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 0.000146143909660168\n",
      "376 0.00014181379810906947\n",
      "377 0.00013760817819274962\n",
      "378 0.0001335300476057455\n",
      "379 0.00012957108265254647\n",
      "380 0.00012573768617585301\n",
      "381 0.00012201550271129236\n",
      "382 0.00011841287050629035\n",
      "383 0.00011491759505588561\n",
      "384 0.00011152285878779367\n",
      "385 0.00010822912736330181\n",
      "386 0.00010503189696464688\n",
      "387 0.00010193906200584024\n",
      "388 9.892875823425129e-05\n",
      "389 9.601411147741601e-05\n",
      "390 9.318890079157427e-05\n",
      "391 9.044264152180403e-05\n",
      "392 8.778471237746999e-05\n",
      "393 8.51990407682024e-05\n",
      "394 8.269827958429232e-05\n",
      "395 8.02657232270576e-05\n",
      "396 7.791052485117689e-05\n",
      "397 7.562558312201872e-05\n",
      "398 7.340908632613719e-05\n",
      "399 7.125573029043153e-05\n",
      "400 6.916530401213095e-05\n",
      "401 6.713885522913188e-05\n",
      "402 6.517479778267443e-05\n",
      "403 6.326658331090584e-05\n",
      "404 6.141555786598474e-05\n",
      "405 5.961869828752242e-05\n",
      "406 5.78762155782897e-05\n",
      "407 5.618750583380461e-05\n",
      "408 5.4547541367355734e-05\n",
      "409 5.295498704072088e-05\n",
      "410 5.1408293074928224e-05\n",
      "411 4.991134483134374e-05\n",
      "412 4.845573494094424e-05\n",
      "413 4.7045799874467775e-05\n",
      "414 4.5674561988562346e-05\n",
      "415 4.434474976733327e-05\n",
      "416 4.305239417590201e-05\n",
      "417 4.180130054010078e-05\n",
      "418 4.0588398405816406e-05\n",
      "419 3.940912938560359e-05\n",
      "420 3.826367174042389e-05\n",
      "421 3.7154237361392006e-05\n",
      "422 3.6074685340281576e-05\n",
      "423 3.502807521726936e-05\n",
      "424 3.4014203265542164e-05\n",
      "425 3.3029544283635914e-05\n",
      "426 3.207243207725696e-05\n",
      "427 3.114362698397599e-05\n",
      "428 3.0242541470215656e-05\n",
      "429 2.9367447496042587e-05\n",
      "430 2.851980389095843e-05\n",
      "431 2.7695567041519098e-05\n",
      "432 2.689595021365676e-05\n",
      "433 2.6116993467439897e-05\n",
      "434 2.536478132242337e-05\n",
      "435 2.4631523046991788e-05\n",
      "436 2.391935959167313e-05\n",
      "437 2.323046646779403e-05\n",
      "438 2.2562308004125953e-05\n",
      "439 2.1912541342317127e-05\n",
      "440 2.127962579834275e-05\n",
      "441 2.0667630451498553e-05\n",
      "442 2.007270450121723e-05\n",
      "443 1.9496546883601695e-05\n",
      "444 1.893625631055329e-05\n",
      "445 1.839240940171294e-05\n",
      "446 1.7863940229290165e-05\n",
      "447 1.7350519556202926e-05\n",
      "448 1.6853155102580786e-05\n",
      "449 1.6369986042263918e-05\n",
      "450 1.590009378560353e-05\n",
      "451 1.544374208606314e-05\n",
      "452 1.5002993677626364e-05\n",
      "453 1.4572843610949349e-05\n",
      "454 1.4154462405713275e-05\n",
      "455 1.374733074044343e-05\n",
      "456 1.3355148439586628e-05\n",
      "457 1.2971718206244987e-05\n",
      "458 1.2602735296241008e-05\n",
      "459 1.2242960110597778e-05\n",
      "460 1.1893258488271385e-05\n",
      "461 1.155339487013407e-05\n",
      "462 1.1223274668736849e-05\n",
      "463 1.0902976100624073e-05\n",
      "464 1.0592701073619537e-05\n",
      "465 1.0290892532793805e-05\n",
      "466 9.996615517593455e-06\n",
      "467 9.712003702588845e-06\n",
      "468 9.435949323233217e-06\n",
      "469 9.166671588900499e-06\n",
      "470 8.906530638341792e-06\n",
      "471 8.652893484395463e-06\n",
      "472 8.406609595112968e-06\n",
      "473 8.16834744910011e-06\n",
      "474 7.935484063636977e-06\n",
      "475 7.710084901191294e-06\n",
      "476 7.4912691161443945e-06\n",
      "477 7.27841188563616e-06\n",
      "478 7.0710793806938455e-06\n",
      "479 6.871519417472882e-06\n",
      "480 6.67647964291973e-06\n",
      "481 6.487290193035733e-06\n",
      "482 6.3034640334080905e-06\n",
      "483 6.1249947975738905e-06\n",
      "484 5.9510557548492216e-06\n",
      "485 5.78311210119864e-06\n",
      "486 5.619767762254924e-06\n",
      "487 5.460563443193678e-06\n",
      "488 5.305757440510206e-06\n",
      "489 5.155573489901144e-06\n",
      "490 5.010438144381624e-06\n",
      "491 4.869134500040673e-06\n",
      "492 4.730684395326534e-06\n",
      "493 4.597650331561454e-06\n",
      "494 4.4673620323010255e-06\n",
      "495 4.341343810665421e-06\n",
      "496 4.218936737743206e-06\n",
      "497 4.1003067963174544e-06\n",
      "498 3.985295734310057e-06\n",
      "499 3.8728603612980805e-06\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        构造函数中，实例化两个 nn.Linear 模块并将其指定为成员变量\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向函数中，接受一个输入数据 Tensor 且必须返回一个输出数据 Tensor\n",
    "        可以使用构造函数中定义的模块或者 Tensor 的任意运算。\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N是数据样本数，D_in是输入层维度\n",
    "# H是隐藏层维度，D_out是输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机输入和输出数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 构造模型\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# 构造损失函数和一个优化器\n",
    "# SGD 构造器中的 model.parameters() 包含两层 nn.linear 模块中的可学习参数\n",
    "# 它们是模型的成员\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(500):\n",
    "    # 前向传播\n",
    "    y_pred=model(x)\n",
    "    \n",
    "    # 计算并打印损失\n",
    "    loss = criterion(y_pred,y)\n",
    "    print(t,loss.item())\n",
    "    \n",
    "    # 清零梯度，执行反向传播，更新权重\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: 控制流与权重共享"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为动态图和权重共享的一个例子，我们现在实现一个非常奇怪的模型：一个完全连接的 ReLU 网络，在每个前向传播中选择 1 到 4 之间的随机数并使用那么多隐藏层，重复使用相同的权重多次以计算最里面的隐藏层。\n",
    "\n",
    "对于这个模型，我们可以使用普通的 Python 流控制来实现循环，并且我们可以通过在定义前向传播时多次重复使用相同的模块来实现最内层之间的权重共享。\n",
    "\n",
    "我们可以轻松地将此模型实现为 Module 子类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 684.6495971679688\n",
      "1 646.7960205078125\n",
      "2 646.8902587890625\n",
      "3 641.616455078125\n",
      "4 642.1076049804688\n",
      "5 640.3995361328125\n",
      "6 639.8314208984375\n",
      "7 629.090576171875\n",
      "8 636.523193359375\n",
      "9 439.0577697753906\n",
      "10 601.0468139648438\n",
      "11 618.2501831054688\n",
      "12 584.3655395507812\n",
      "13 610.7720336914062\n",
      "14 630.1234741210938\n",
      "15 635.0167236328125\n",
      "16 532.8248901367188\n",
      "17 300.41424560546875\n",
      "18 497.7484130859375\n",
      "19 573.45263671875\n",
      "20 626.7001342773438\n",
      "21 549.343017578125\n",
      "22 616.338134765625\n",
      "23 515.1063232421875\n",
      "24 565.800048828125\n",
      "25 471.1210021972656\n",
      "26 339.4145202636719\n",
      "27 319.05706787109375\n",
      "28 400.5921630859375\n",
      "29 502.4747619628906\n",
      "30 259.2665100097656\n",
      "31 461.2328186035156\n",
      "32 203.297119140625\n",
      "33 422.7790832519531\n",
      "34 369.6379699707031\n",
      "35 341.1153869628906\n",
      "36 149.56039428710938\n",
      "37 257.345458984375\n",
      "38 164.8937225341797\n",
      "39 276.5129699707031\n",
      "40 147.45355224609375\n",
      "41 203.12112426757812\n",
      "42 251.89442443847656\n",
      "43 264.73565673828125\n",
      "44 108.53075408935547\n",
      "45 99.23666381835938\n",
      "46 117.24641418457031\n",
      "47 898.188232421875\n",
      "48 478.1507263183594\n",
      "49 422.154052734375\n",
      "50 337.14361572265625\n",
      "51 536.2012939453125\n",
      "52 492.8011169433594\n",
      "53 276.46417236328125\n",
      "54 218.48806762695312\n",
      "55 505.46875\n",
      "56 554.7118530273438\n",
      "57 408.92535400390625\n",
      "58 261.819580078125\n",
      "59 504.3977355957031\n",
      "60 424.3017578125\n",
      "61 232.69566345214844\n",
      "62 403.232177734375\n",
      "63 241.90573120117188\n",
      "64 186.39837646484375\n",
      "65 398.5433044433594\n",
      "66 122.50701904296875\n",
      "67 92.75161743164062\n",
      "68 65.86492919921875\n",
      "69 174.10806274414062\n",
      "70 396.1259460449219\n",
      "71 322.09075927734375\n",
      "72 149.12054443359375\n",
      "73 100.98406219482422\n",
      "74 318.783447265625\n",
      "75 91.05162811279297\n",
      "76 301.7431945800781\n",
      "77 59.716224670410156\n",
      "78 205.35775756835938\n",
      "79 192.72113037109375\n",
      "80 227.1573944091797\n",
      "81 241.24508666992188\n",
      "82 97.60427856445312\n",
      "83 219.7368621826172\n",
      "84 187.69552612304688\n",
      "85 112.64247131347656\n",
      "86 168.8266143798828\n",
      "87 162.59217834472656\n",
      "88 77.26898956298828\n",
      "89 150.5562286376953\n",
      "90 115.44381713867188\n",
      "91 109.99828338623047\n",
      "92 133.11683654785156\n",
      "93 64.57888793945312\n",
      "94 118.07749938964844\n",
      "95 112.60994720458984\n",
      "96 105.64747619628906\n",
      "97 119.66796875\n",
      "98 87.9889907836914\n",
      "99 102.81056213378906\n",
      "100 62.99761962890625\n",
      "101 49.307621002197266\n",
      "102 42.15074920654297\n",
      "103 53.3831901550293\n",
      "104 112.07300567626953\n",
      "105 55.072715759277344\n",
      "106 65.88170623779297\n",
      "107 66.81294250488281\n",
      "108 84.42073822021484\n",
      "109 34.18314743041992\n",
      "110 59.10118865966797\n",
      "111 37.81814956665039\n",
      "112 77.03663635253906\n",
      "113 43.67881774902344\n",
      "114 23.92986297607422\n",
      "115 34.64341354370117\n",
      "116 104.52961730957031\n",
      "117 16.45851707458496\n",
      "118 51.76104736328125\n",
      "119 61.34153366088867\n",
      "120 68.27568817138672\n",
      "121 12.39726448059082\n",
      "122 66.94709777832031\n",
      "123 65.38311767578125\n",
      "124 12.128576278686523\n",
      "125 58.3957633972168\n",
      "126 8.293720245361328\n",
      "127 7.3600287437438965\n",
      "128 36.542205810546875\n",
      "129 51.06413650512695\n",
      "130 26.647979736328125\n",
      "131 23.25434684753418\n",
      "132 25.146902084350586\n",
      "133 9.075724601745605\n",
      "134 25.941978454589844\n",
      "135 29.53536605834961\n",
      "136 23.8834171295166\n",
      "137 14.73952865600586\n",
      "138 45.94315719604492\n",
      "139 10.93717098236084\n",
      "140 21.606760025024414\n",
      "141 15.719179153442383\n",
      "142 19.098451614379883\n",
      "143 16.772544860839844\n",
      "144 48.45017623901367\n",
      "145 27.243053436279297\n",
      "146 25.224580764770508\n",
      "147 42.24288558959961\n",
      "148 18.16538429260254\n",
      "149 19.93665885925293\n",
      "150 14.67502498626709\n",
      "151 15.066248893737793\n",
      "152 11.566946029663086\n",
      "153 9.751118659973145\n",
      "154 7.3121442794799805\n",
      "155 24.18317222595215\n",
      "156 36.39370346069336\n",
      "157 13.480756759643555\n",
      "158 24.74942398071289\n",
      "159 58.51570510864258\n",
      "160 35.01393508911133\n",
      "161 21.759098052978516\n",
      "162 10.744932174682617\n",
      "163 8.403565406799316\n",
      "164 61.40284729003906\n",
      "165 74.99633026123047\n",
      "166 4.750476360321045\n",
      "167 34.734619140625\n",
      "168 8.83627986907959\n",
      "169 67.18659210205078\n",
      "170 30.204347610473633\n",
      "171 101.94903564453125\n",
      "172 13.029303550720215\n",
      "173 30.689014434814453\n",
      "174 72.84186553955078\n",
      "175 39.301414489746094\n",
      "176 39.88991165161133\n",
      "177 51.92513656616211\n",
      "178 24.021730422973633\n",
      "179 54.94972610473633\n",
      "180 36.227909088134766\n",
      "181 41.63230895996094\n",
      "182 38.95256042480469\n",
      "183 7.691965579986572\n",
      "184 42.110252380371094\n",
      "185 15.923294067382812\n",
      "186 16.194400787353516\n",
      "187 18.980575561523438\n",
      "188 14.88241195678711\n",
      "189 15.469584465026855\n",
      "190 37.26857376098633\n",
      "191 11.933497428894043\n",
      "192 19.122394561767578\n",
      "193 17.51219367980957\n",
      "194 16.194860458374023\n",
      "195 14.324907302856445\n",
      "196 6.707727909088135\n",
      "197 9.103903770446777\n",
      "198 18.8018856048584\n",
      "199 12.704729080200195\n",
      "200 10.423043251037598\n",
      "201 7.202271938323975\n",
      "202 9.842041969299316\n",
      "203 25.235321044921875\n",
      "204 14.576920509338379\n",
      "205 5.896846294403076\n",
      "206 6.7120890617370605\n",
      "207 44.65678787231445\n",
      "208 10.351364135742188\n",
      "209 8.343009948730469\n",
      "210 8.885643005371094\n",
      "211 22.973604202270508\n",
      "212 16.672178268432617\n",
      "213 9.031219482421875\n",
      "214 8.603358268737793\n",
      "215 12.716256141662598\n",
      "216 12.118678092956543\n",
      "217 8.036861419677734\n",
      "218 23.121139526367188\n",
      "219 7.510288238525391\n",
      "220 10.764086723327637\n",
      "221 15.983733177185059\n",
      "222 10.444666862487793\n",
      "223 6.846123695373535\n",
      "224 4.068643093109131\n",
      "225 3.9112584590911865\n",
      "226 7.040822505950928\n",
      "227 6.029892921447754\n",
      "228 12.493228912353516\n",
      "229 4.350229263305664\n",
      "230 11.956382751464844\n",
      "231 3.078795909881592\n",
      "232 5.577266216278076\n",
      "233 5.769247531890869\n",
      "234 2.7654523849487305\n",
      "235 5.033411026000977\n",
      "236 2.953610420227051\n",
      "237 8.014450073242188\n",
      "238 2.2957327365875244\n",
      "239 2.6741151809692383\n",
      "240 4.7558746337890625\n",
      "241 3.146145820617676\n",
      "242 4.66237735748291\n",
      "243 2.4107322692871094\n",
      "244 2.5570015907287598\n",
      "245 1.7299953699111938\n",
      "246 1.660431146621704\n",
      "247 11.499820709228516\n",
      "248 2.9526474475860596\n",
      "249 1.481246829032898\n",
      "250 3.8301687240600586\n",
      "251 3.4090306758880615\n",
      "252 2.820859432220459\n",
      "253 6.7243523597717285\n",
      "254 3.827214002609253\n",
      "255 3.4737675189971924\n",
      "256 5.964620113372803\n",
      "257 3.521988868713379\n",
      "258 2.5116519927978516\n",
      "259 3.1173148155212402\n",
      "260 1.6200966835021973\n",
      "261 2.6532163619995117\n",
      "262 1.4157148599624634\n",
      "263 3.2295889854431152\n",
      "264 2.9806337356567383\n",
      "265 2.542226791381836\n",
      "266 2.327726125717163\n",
      "267 2.4181571006774902\n",
      "268 2.1931889057159424\n",
      "269 1.9653366804122925\n",
      "270 1.5072414875030518\n",
      "271 1.3580933809280396\n",
      "272 1.0966302156448364\n",
      "273 3.2459359169006348\n",
      "274 2.2825307846069336\n",
      "275 1.9495837688446045\n",
      "276 0.7924900650978088\n",
      "277 2.1073906421661377\n",
      "278 0.8821408748626709\n",
      "279 2.370039224624634\n",
      "280 1.815316915512085\n",
      "281 1.241889476776123\n",
      "282 5.798352241516113\n",
      "283 2.0761024951934814\n",
      "284 1.2163734436035156\n",
      "285 1.2993781566619873\n",
      "286 5.62080717086792\n",
      "287 1.0834068059921265\n",
      "288 4.897267818450928\n",
      "289 2.5532991886138916\n",
      "290 1.4054325819015503\n",
      "291 2.244905710220337\n",
      "292 2.083902597427368\n",
      "293 1.8913240432739258\n",
      "294 3.281045436859131\n",
      "295 0.7641288042068481\n",
      "296 1.9681220054626465\n",
      "297 1.6119388341903687\n",
      "298 5.2329559326171875\n",
      "299 4.025341033935547\n",
      "300 1.2170790433883667\n",
      "301 2.387211799621582\n",
      "302 3.0492002964019775\n",
      "303 0.7906386256217957\n",
      "304 1.825971245765686\n",
      "305 6.78585958480835\n",
      "306 0.7186102867126465\n",
      "307 1.885955810546875\n",
      "308 1.3848131895065308\n",
      "309 15.023611068725586\n",
      "310 3.722168445587158\n",
      "311 9.08521842956543\n",
      "312 0.9322106242179871\n",
      "313 4.5386528968811035\n",
      "314 2.2113890647888184\n",
      "315 1.539585828781128\n",
      "316 3.20890212059021\n",
      "317 2.453604221343994\n",
      "318 2.0478715896606445\n",
      "319 1.2598239183425903\n",
      "320 8.672576904296875\n",
      "321 3.466264009475708\n",
      "322 6.785843849182129\n",
      "323 2.288400173187256\n",
      "324 1.0719947814941406\n",
      "325 1.7892184257507324\n",
      "326 1.2329579591751099\n",
      "327 2.4485678672790527\n",
      "328 7.872277736663818\n",
      "329 1.8137588500976562\n",
      "330 2.5773231983184814\n",
      "331 5.378092288970947\n",
      "332 4.855022430419922\n",
      "333 1.8870424032211304\n",
      "334 2.048449993133545\n",
      "335 0.8181295394897461\n",
      "336 7.148390293121338\n",
      "337 5.196225166320801\n",
      "338 8.793905258178711\n",
      "339 4.346786975860596\n",
      "340 14.040413856506348\n",
      "341 2.5409038066864014\n",
      "342 1.689804196357727\n",
      "343 0.6890614032745361\n",
      "344 2.443366765975952\n",
      "345 7.043782711029053\n",
      "346 5.9703288078308105\n",
      "347 11.822185516357422\n",
      "348 0.8233256340026855\n",
      "349 9.265624046325684\n",
      "350 86.82949829101562\n",
      "351 7.389893531799316\n",
      "352 2.831421136856079\n",
      "353 52.71243667602539\n",
      "354 20.7808837890625\n",
      "355 113.67022705078125\n",
      "356 4.210801601409912\n",
      "357 43.41035079956055\n",
      "358 26.165035247802734\n",
      "359 61.7510986328125\n",
      "360 54.963417053222656\n",
      "361 18.614892959594727\n",
      "362 30.851364135742188\n",
      "363 5.733281135559082\n",
      "364 33.30226135253906\n",
      "365 17.64535903930664\n",
      "366 9.910835266113281\n",
      "367 9.504849433898926\n",
      "368 6.821645259857178\n",
      "369 18.31903839111328\n",
      "370 5.305855751037598\n",
      "371 3.825026512145996\n",
      "372 23.722007751464844\n",
      "373 21.92464256286621\n",
      "374 11.876693725585938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 25.092817306518555\n",
      "376 20.329994201660156\n",
      "377 13.308563232421875\n",
      "378 12.667623519897461\n",
      "379 5.371011257171631\n",
      "380 6.465823650360107\n",
      "381 10.244417190551758\n",
      "382 15.709258079528809\n",
      "383 20.36738395690918\n",
      "384 7.933638572692871\n",
      "385 3.0845744609832764\n",
      "386 19.895034790039062\n",
      "387 6.562630653381348\n",
      "388 5.080763816833496\n",
      "389 3.2361795902252197\n",
      "390 14.162188529968262\n",
      "391 4.230726718902588\n",
      "392 4.042773723602295\n",
      "393 4.330113410949707\n",
      "394 3.5533559322357178\n",
      "395 7.274179935455322\n",
      "396 3.703739643096924\n",
      "397 3.9915847778320312\n",
      "398 3.303746461868286\n",
      "399 3.1808230876922607\n",
      "400 7.0486273765563965\n",
      "401 11.859415054321289\n",
      "402 4.366374492645264\n",
      "403 5.8661603927612305\n",
      "404 9.36649227142334\n",
      "405 13.153993606567383\n",
      "406 2.571829080581665\n",
      "407 1.5467166900634766\n",
      "408 3.5508220195770264\n",
      "409 17.692949295043945\n",
      "410 9.411348342895508\n",
      "411 1.8874576091766357\n",
      "412 4.455809593200684\n",
      "413 7.347081184387207\n",
      "414 2.688293933868408\n",
      "415 2.2896595001220703\n",
      "416 21.945087432861328\n",
      "417 4.9614105224609375\n",
      "418 5.222107887268066\n",
      "419 9.541421890258789\n",
      "420 9.731449127197266\n",
      "421 7.113768577575684\n",
      "422 3.5000271797180176\n",
      "423 3.3543221950531006\n",
      "424 3.183685779571533\n",
      "425 6.898646831512451\n",
      "426 7.516732692718506\n",
      "427 4.074975490570068\n",
      "428 2.172647714614868\n",
      "429 2.49224591255188\n",
      "430 3.9893476963043213\n",
      "431 4.012999057769775\n",
      "432 6.21856164932251\n",
      "433 1.3911339044570923\n",
      "434 5.447403430938721\n",
      "435 1.6928044557571411\n",
      "436 1.9848649501800537\n",
      "437 1.2884228229522705\n",
      "438 2.6645357608795166\n",
      "439 2.210188388824463\n",
      "440 1.7623776197433472\n",
      "441 3.1369125843048096\n",
      "442 3.1027278900146484\n",
      "443 2.4827208518981934\n",
      "444 2.107896327972412\n",
      "445 2.057224750518799\n",
      "446 3.4424593448638916\n",
      "447 1.4864264726638794\n",
      "448 1.1736726760864258\n",
      "449 2.1710128784179688\n",
      "450 2.2187178134918213\n",
      "451 1.8041073083877563\n",
      "452 1.0589121580123901\n",
      "453 1.5428657531738281\n",
      "454 1.2453776597976685\n",
      "455 1.0343183279037476\n",
      "456 1.4806272983551025\n",
      "457 0.7411181926727295\n",
      "458 2.094834089279175\n",
      "459 1.8520747423171997\n",
      "460 1.7599788904190063\n",
      "461 1.5756374597549438\n",
      "462 1.6028759479522705\n",
      "463 1.2970128059387207\n",
      "464 1.453505516052246\n",
      "465 0.7944726943969727\n",
      "466 1.2972393035888672\n",
      "467 0.6623944044113159\n",
      "468 1.615436315536499\n",
      "469 1.436888337135315\n",
      "470 0.5453993082046509\n",
      "471 0.9767468571662903\n",
      "472 0.8987816572189331\n",
      "473 0.8690647482872009\n",
      "474 0.8455765247344971\n",
      "475 0.9348751902580261\n",
      "476 0.9087129235267639\n",
      "477 0.6468914151191711\n",
      "478 2.4775140285491943\n",
      "479 2.1367273330688477\n",
      "480 1.9555946588516235\n",
      "481 0.4673740565776825\n",
      "482 0.956260621547699\n",
      "483 0.8606041073799133\n",
      "484 0.37603190541267395\n",
      "485 1.7237004041671753\n",
      "486 0.8402723670005798\n",
      "487 1.230905294418335\n",
      "488 1.3046616315841675\n",
      "489 0.6721556186676025\n",
      "490 1.0060473680496216\n",
      "491 1.1496973037719727\n",
      "492 1.7488125562667847\n",
      "493 1.1302950382232666\n",
      "494 1.0026986598968506\n",
      "495 0.8590317368507385\n",
      "496 0.7570025324821472\n",
      "497 0.7660679221153259\n",
      "498 0.6475145220756531\n",
      "499 1.5529123544692993\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        构造 nn.Linear 的三个实例\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"   \n",
    "        对于模型的前向传播，我们随机选择 0,1,2或3，并多次重用 middle_linear 模块来计算隐藏层表示\n",
    "        由于每个前向传播都构建了一个动态计算图，因此在定义模型的前向传播时，我们可以使用普通的 Python 控制流操作符，如循环或条件语句\n",
    "        在这里，我们还看到在定义计算图时多次重复使用相同的模块是完全安全的\n",
    "        这是对 Lua Torch 的一项重大改进，它的每个模块只能使用一次\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 4)):  # 0 ~ 3\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N是数据样本数，D_in是输入层维度\n",
    "# H是隐藏层维度，D_out是输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机输入和输出数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 创建随机输入和输出数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 构造模型\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# 构造损失函数和一个优化器\n",
    "# 使用 SGD 直接训练这个奇怪的模型有些困难，所以我们加上动量\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "for t in range(500):\n",
    "    # 前向传播\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 计算并打印损失\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # 清零梯度，执行反向传播，更新权重\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
